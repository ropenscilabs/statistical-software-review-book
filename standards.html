<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Standards: Version 0.0.1 | rOpenSci Statistical Software Peer Review</title>
<meta name="author" content="Mark Padgham and Noam Ross">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.9002/transition.js"></script><script src="libs/bs3compat-0.2.5.9002/tabs.js"></script><script src="libs/bs3compat-0.2.5.9002/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script><link href="libs/vis-4.20.1/vis.css" rel="stylesheet">
<script src="libs/vis-4.20.1/vis.min.js"></script><script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script><script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">rOpenSci Statistical Software Peer Review</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Welcome</a></li>
<li class="book-part">Introduction</li>
<li><a class="" href="overview.html"><span class="header-section-number">2</span> Overview of the Project and of this Book</a></li>
<li class="book-part">Package Development, Submission, and Review</li>
<li><a class="" href="pkgdev.html"><span class="header-section-number">3</span> Guide for Authors</a></li>
<li><a class="" href="pkgsubmission.html"><span class="header-section-number">4</span> Guide for Editors</a></li>
<li><a class="" href="pkgreview.html"><span class="header-section-number">5</span> Guide for Reviewers</a></li>
<li class="book-part">Standards</li>
<li><a class="active" href="standards.html"><span class="header-section-number">6</span> Standards: Version 0.0.1</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="appendices.html"><span class="header-section-number">A</span> Appendices</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="standards" class="section level1">
<h1>
<span class="header-section-number">6</span> Standards: Version 0.0.1<a class="anchor" aria-label="anchor" href="#standards"><i class="fas fa-link"></i></a>
</h1>
<p>This Chapter serves as the reference for rOpenSci’s standards for statistical
software. Software accepted for peer-review must fit one or more of our
categories, and thus all packages must comply with the <em>General Standards</em>
listed in the first of the following sections, along with one or more of the
category-specific sets of standards listed in the subsequent sections.</p>
<p>Examples of application of these standards may be viewed as separate
<a href="https://hackmd.io"><code>hackmd.io</code> files</a> by clicking on the following links:</p>
<ul>
<li><a href="https://hackmd.io/zVWTAl9ZQeCcj_bvMGcmMQ">Application of Bayesian and Monte Carlo Standards</a></li>
<li><a href="https://hackmd.io/VZ-wgQtZRV2pb-wFZNDM5g">Application of Regression and Supervised Learning Standards</a></li>
<li><a href="https://hackmd.io/iOZD_oCpT86zoY5z4memaQ">Application of Dimensionality Reduction, Clustering, and Unsupervised Learning Standards</a></li>
<li><a href="https://hackmd.io/K8F1RIhdQeuZFqMnzdqNVw">Application of Exploratory Data Analysis Standards</a></li>
<li><a href="https://hackmd.io/Ix1YwD8YTWGuzdiXsVQadA">Application of Machine Learning Software Standards</a></li>
</ul>
<p>Each of those files compares both general and category-specific standards
against selected R packages within those categories. These comparisons are
intended for illustrative purposes only, and are in no way intended to
represent evaluations of the software. They are presented in the hope of
demonstrating how the standards presented here may be applied to software, and
what the results of such application may look like.</p>
<!-- Edit the .Rmd not the .md file -->
<div id="general-standards" class="section level2">
<h2>
<span class="header-section-number">6.1</span> General Standards for Statistical Software<a class="anchor" aria-label="anchor" href="#general-standards"><i class="fas fa-link"></i></a>
</h2>
<p>These general standards, and all category-specific standards that follow, are
intended to serve as <em>recommendations</em> for best practices. Note in particular
that many standards are written using the word “<em>should</em>” in explicit
acknowledgement that adhering to such standards may not always be possible. All
standards phrased in these terms are intended to be interpreted as applicable
under such conditions as “<em>Where possible</em>”, or “<em>Where applicable</em>”.
Developers are requested to note any standards which they deem not applicable
to their software via the <a href="https://ropensci-review-tools.github.io/srr/"><code>srr</code>
package</a>, as described in
<a href="pkgdev.html#pkgdev">Chapter 3</a>.</p>
<details><summary>
These standards refer to <b>Data Types</b> as the fundamental types defined by the
<a href="https://cran.r-project.org/doc/manuals/R-lang.html">R language</a>
itself. Information on these types can be seen by clicking here.
</summary><p>
</p>
<p>The <a href="https://cran.r-project.org/doc/manuals/R-lang.html">R language</a> defines
the following data types:</p>
<ul>
<li>Logical</li>
<li>Integer</li>
<li>Continuous (<code>class = "numeric"</code> / <code>typeof = "double"</code>)</li>
<li>Complex</li>
<li>String / character</li>
</ul>
<p>The base R system also includes what are considered here to be direct
extensions of fundamental types to include:</p>
<ul>
<li>Factor</li>
<li>Ordered Factor</li>
<li>Date/Time</li>
</ul>
<p>The continuous type has a <code>typeof</code> of “double” because that represents the
storage mode in the C representation of such objects, while the <code>class</code> as
defined within R is referred to as “numeric”. While <code>typeof</code> is not the same as
<code>class</code>, with reference to continuous variables, “numeric” may be considered
identical to “double” throughout.</p>
<p>The term “character” is interpreted here to refer to a vector each element of
which is an individual “character” object. The term “string” does not relate to
any official R nomenclature, but is used here to refer for convenience to
a character vector of length one; in other words, a “string” is the sole
element of a single-length “character” vector.</p>
<hr></details><p><br></p>
<div id="documentation" class="section level3">
<h3>
<span class="header-section-number">6.1.1</span> Documentation<a class="anchor" aria-label="anchor" href="#documentation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="G1_0"><strong>G1.0</strong></span> <em>Statistical Software should list at least one primary
reference from published academic literature.</em>
</li>
</ul>
<p>We consider that statistical software submitted under our system will either
(i) implement or extend prior methods, in which case the <em>primary reference</em>
will be to the most relevant published version(s) of prior methods; or (ii) be
an implementation of some new method. In the second case, it will be expected
that the software will eventually form the basis of an academic publication.
Until that time, the most suitable reference for equivalent algorithms or
implementations should be provided.</p>
<ul>
<li>
<span id="G1_1"><strong>G1.1</strong></span> <em>Statistical Software should document whether the
algorithm(s) it implements are:</em>
<ul>
<li>
<em>The first implementation of a novel algorithm</em>; or</li>
<li>
<em>The first implementation within <strong>R</strong> of an algorithm which has
previously been implemented in other languages or contexts</em>; or</li>
<li>
<em>An improvement on other implementations of similar algorithms in <strong>R</strong></em>.</li>
</ul>
</li>
</ul>
<p>The second and third options additionally require references to comparable
algorithms or implementations to be documented somewhere within the software,
including references to all known implementations in other computer languages.
(A common location for such is a statement of “<em>Prior Art</em>” or similar at the
end of the main <code>README</code> document.)</p>
<ul>
<li>
<span id="G1_2"><strong>G1.2</strong></span> <em>Statistical Software should include a</em> Life Cycle
Statement <em>describing current and anticipated future states of development.</em>
</li>
</ul>
<p>We encourage these to placed within a repository’s <a href="https://devguide.ropensci.org/collaboration.html#contributing-guide"><code>CONTRIBUTING.md</code>
file</a>, as
in <a href="https://github.com/ecohealthalliance/fasterize/blob/master/CONTRIBUTING.md">this
example</a>.
A simple <em>Life Cycle Statement</em> may be formed by selecting one of the following
four statements.</p>
<pre><code>This package is

    - In a stable state of development, with minimal subsequent development
      envisioned.
    - In a stable state of development, with active subsequent development
      primarily in response to user feedback.
    - In a stable state of development, with some degree of active subsequent
      development as envisioned by the primary authors.
    - In an initially stable state of development, with a great deal of active
      subsequent development envisioned.</code></pre>
<div id="statistical-terminology" class="section level4">
<h4>
<span class="header-section-number">6.1.1.1</span> Statistical Terminology<a class="anchor" aria-label="anchor" href="#statistical-terminology"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="G1_3"><strong>G1.3</strong></span> <em>All statistical terminology should be clarified and
unambiguously defined.</em>
</li>
</ul>
<p>Developers should not presume anywhere in the documentation of software that
specific statistical terminology may be “generally understood”, and therefore
not need explicit clarification. Even terms which many may consider
sufficiently generic as to not require such clarification, such as “null
hypotheses” or “confidence intervals”, will generally need explicit
clarification. For example, both the estimation and interpretation of
confidence intervals are dependent on distributional properties and associated
assumptions. Any particular implementation of procedures to estimate or report
on confidence intervals will accordingly reflect assumptions on distributional
properties (among other aspects), both the nature and implications of which
must be explicitly clarified.</p>
</div>
<div id="function-level-documentation" class="section level4">
<h4>
<span class="header-section-number">6.1.1.2</span> Function-level Documentation<a class="anchor" aria-label="anchor" href="#function-level-documentation"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="G1_4"><strong>G1.4</strong></span> <em>Software should use
<a href="https://roxygen2.r-lib.org/"><code>roxygen2</code></a> to document all functions.</em>
<ul>
<li>
<span id="G1_4a"><strong>G1.4a</strong></span> <em>All internal (non-exported) functions should also be
documented in standard <a href="https://roxygen2.r-lib.org/"><code>roxygen2</code></a> format,
along with a final <code>@noRd</code> tag to suppress automatic generation of <code>.Rd</code>
files.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="supplementary-documentation" class="section level4">
<h4>
<span class="header-section-number">6.1.1.3</span> Supplementary Documentation<a class="anchor" aria-label="anchor" href="#supplementary-documentation"><i class="fas fa-link"></i></a>
</h4>
<p>The following standards describe several forms of what might be considered
“Supplementary Material”. While there are many places within an R package where
such material may be included, common locations include vignettes, or in
additional directories (such as <code>data-raw</code>) listed in <code>.Rbuildignore</code> to
prevent inclusion within installed packages.</p>
<p>Where software supports a publication, all claims made in the publication with
regard to software performance (for example, claims of algorithmic scaling or
efficiency; or claims of accuracy), the following standard applies:</p>
<ul>
<li>
<span id="G1_5"><strong>G1.5</strong></span> <em>Software should include all code necessary to reproduce
results which form the basis of performance claims made in associated
publications.</em>
</li>
</ul>
<p>Where claims regarding aspects of software performance are made with respect to
other extant R packages, the following standard applies:</p>
<ul>
<li>
<span id="G1_6"><strong>G1.6</strong></span> <em>Software should include code necessary to compare
performance claims with alternative implementations in other R packages.</em>
</li>
</ul>
</div>
</div>
<div id="input-structures" class="section level3">
<h3>
<span class="header-section-number">6.1.2</span> Input Structures<a class="anchor" aria-label="anchor" href="#input-structures"><i class="fas fa-link"></i></a>
</h3>
<p>This section considers general standards for <em>Input Structures</em>. These
standards may often effectively be addressed through implementing class
structures, although this is not a general requirement. Developers are
nevertheless encouraged to examine the guide to <a href="https://vctrs.r-lib.org/articles/s3-vector.html#casting-and-coercion">S3
vectors</a>
in the <a href="https://vctrs.r-lib.org"><code>vctrs</code> package</a> as an example of the kind of
assurances and validation checks that are possible with regard to input data.
Systems like those demonstrated in that vignette provide a very effective way
to ensure that software remains robust to diverse and unexpected classes and
types of input data. Packages such
<a href="https://mllg.github.io/checkmate/index.html"><code>checkmate</code></a> enable direct and
simple ways to check and assert input structures.</p>
<div id="uni-variate-vector-input" class="section level4">
<h4>
<span class="header-section-number">6.1.2.1</span> Uni-variate (Vector) Input<a class="anchor" aria-label="anchor" href="#uni-variate-vector-input"><i class="fas fa-link"></i></a>
</h4>
<p>It is important to note for univariate data that single values in R are vectors
with a length of one, and that <code>1</code> is of exactly the same <em>data type</em> as <code>1:n</code>.
Given this, inputs expected to be univariate should:</p>
<ul>
<li>
<span id="G2_0"><strong>G2.0</strong></span> <em>Implement assertions on lengths of inputs, particularly
through asserting that inputs expected to be single- or multi-valued are
indeed so.</em>
<ul>
<li>
<span id="G2_0a"><strong>G2.0a</strong></span> Provide explicit secondary documentation of any
expectations on lengths of inputs</li>
</ul>
</li>
<li>
<span id="G2_1"><strong>G2.1</strong></span> <em>Implement assertions on types of inputs (see the initial
point on nomenclature above).</em>
<ul>
<li>
<span id="G2_1a"><strong>G2.1a</strong></span> <em>Provide explicit secondary documentation of
expectations on data types of all vector inputs.</em>
</li>
</ul>
</li>
<li>
<span id="G2_2"><strong>G2.2</strong></span> <em>Appropriately prohibit or restrict submission of
multivariate input to parameters expected to be univariate.</em>
</li>
<li>
<span id="G2_3"><strong>G2.3</strong></span> <em>For univariate character input:</em>
<ul>
<li>
<span id="G2_3a"><strong>G2.3a</strong></span> <em>Use <code><a href="https://rdrr.io/r/base/match.arg.html">match.arg()</a></code> or equivalent where applicable to
only permit expected values.</em>
</li>
<li>
<span id="G2_3b"><strong>G2.3b</strong></span> <em>Either: use <code><a href="https://rdrr.io/r/base/chartr.html">tolower()</a></code> or equivalent to ensure
input of character parameters is not case dependent; or explicitly
document that parameters are strictly case-sensitive.</em>
</li>
</ul>
</li>
<li>
<span id="G2_4"><strong>G2.4</strong></span> <em>Provide appropriate mechanisms to convert between
different data types, potentially including:</em>
<ul>
<li>
<span id="G2_4a"><strong>G2.4a</strong></span> <em>explicit conversion to <code>integer</code> via <code><a href="https://rdrr.io/r/base/integer.html">as.integer()</a></code></em>
</li>
<li>
<span id="G2_4b"><strong>G2.4b</strong></span> <em>explicit conversion to continuous via
<code><a href="https://rdrr.io/r/base/numeric.html">as.numeric()</a></code></em>
</li>
<li>
<span id="G2_4c"><strong>G2.4c</strong></span> <em>explicit conversion to character via
<code><a href="https://rdrr.io/r/base/character.html">as.character()</a></code> (and not <code>paste</code> or <code>paste0</code>)</em>
</li>
<li>
<span id="G2_4d"><strong>G2.4d</strong></span> <em>explicit conversion to factor via <code><a href="https://rdrr.io/r/base/factor.html">as.factor()</a></code></em>
</li>
<li>
<span id="G2_4e"><strong>G2.4e</strong></span> <em>explicit conversion from factor via <code>as...()</code>
functions</em>
</li>
</ul>
</li>
<li>
<span id="G2_5"><strong>G2.5</strong></span> <em>Where inputs are expected to be of <code>factor</code> type,
secondary documentation should explicitly state whether these should be
<code>ordered</code> or not, and those inputs should provide appropriate error or other
routines to ensure inputs follow these expectations.</em>
</li>
</ul>
<p>A few packages implement R versions of “static type” forms common in other
languages, whereby the type of a variable must be explicitly specified prior to
assignment. Use of such approaches is encouraged, including but not restricted
to approaches documented in packages such as
<a href="https://vctrs.r-lib.org"><code>vctrs</code></a>, or the experimental package
<a href="https://github.com/moodymudskipper/typed"><code>typed</code></a>. One additional standard
for vector input is:</p>
<ul>
<li>
<span id="G2_6"><strong>G2.6</strong></span> <em>Software which accepts one-dimensional input should ensure
values are appropriately pre-processed regardless of class structures.</em>
</li>
</ul>
<p>The <a href="https://github.com/r-quantities/units/"><code>units</code> package</a> provides a good
example, in creating objects that may be treated as vectors, yet which have
a class structure that does not inherit from the <code>vector</code> class. Using these
objects as input often causes software to fail. The <code>storage.mode</code> of the
underlying objects may nevertheless be examined, and the objects transformed or
processed accordingly to ensure such inputs do not lead to errors.</p>
</div>
<div id="tabular-input" class="section level4">
<h4>
<span class="header-section-number">6.1.2.2</span> Tabular Input<a class="anchor" aria-label="anchor" href="#tabular-input"><i class="fas fa-link"></i></a>
</h4>
<p>This sub-section concerns input in “tabular data” forms, meaning the base
R forms <code>array</code>, <code>matrix</code>, and <code>data.frame</code>, and other forms and classes
derived from these. Tabular data generally have two dimensions, although may
have more (such as for <code>array</code> objects). There is a primary distinction within
R itself between <code>array</code> or <code>matrix</code> representations, and <code>data.frame</code> and
associated representations. The former are restricted to storing data of
a single uniform type (for example, all <code>integer</code> or all <code>character</code> values),
whereas <code>data.frame</code> as associated representations (generally) store each
column as a list item, allowing different columns to hold values of different
types. Further noting that a <code>matrix</code> may, <a href="https://developer.r-project.org/Blog/public/2019/11/09/when-you-think-class.-think-again/index.html">as of R version
4.0</a>,
be considered as a strictly two-dimensional array, tabular inputs for the
purposes of these standards are considered to imply data represented in one or
more of the following forms:</p>
<ul>
<li>
<code>matrix</code> form when referring to specifically two-dimensional data of one
uniform type</li>
<li>
<code>array</code> form as a more general expression, or when referring to data that are
not necessarily or strictly two-dimensional</li>
<li><code>data.frame</code></li>
<li>Extensions such as
<ul>
<li><a href="https://tibble.tidyverse.org"><code>tibble</code></a></li>
<li><a href="https://rdatatable.gitlab.io/data.table"><code>data.table</code></a></li>
<li>domain-specific classes such as
<a href="https://tsibble.tidyverts.org"><code>tsibble</code></a> for time series, or
<a href="https://r-spatial.github.io/sf/"><code>sf</code></a> for spatial data.</li>
</ul>
</li>
</ul>
<p>Both <code>matrix</code> and <code>array</code> forms are actually stored as vectors with a single
<code>storage.mode</code>, and so all of the preceding standards <strong>G2.0</strong>–<strong>G2.5</strong> apply.
The other rectangular forms are not stored as vectors, and do not necessarily
have a single <code>storage.mode</code> for all columns. These forms are referred to
throughout these standards as “<code>data.frame</code>-type tabular forms”, which may be
assumed to refer to data represented in either the <code><a href="https://rdrr.io/r/base/data.frame.html">base::data.frame</a></code> format,
and/or any of the classes listed in the final of the above points.</p>
<p>General Standards applicable to software which is intended to accept any one or
more of these <code>data.frame</code>-type tabular inputs are then that:</p>
<ul>
<li>
<span id="G2_7"><strong>G2.7</strong></span> <em>Software should accept as input as many of the above
standard tabular forms as possible, including extension to domain-specific
forms.</em>
</li>
</ul>
<p>Software need not necessarily test abilities to accept different types of
inputs, because that may require adding packages to the <code>Suggests</code> field of
a package for that purpose alone. Nevertheless, software which somehow uses
(through <code>Depends</code> or <code>Suggests</code>) any packages for representing tabular data
should confirm in tests the ability to accept these types of input.</p>
<ul>
<li>
<span id="G2_8"><strong>G2.8</strong></span> <em>Software should provide appropriate conversion or dispatch
routines as part of initial pre-processing to ensure that all other
sub-functions of a package receive inputs of a single defined class or type.</em>
</li>
<li>
<span id="G2_9"><strong>G2.9</strong></span> <em>Software should issue diagnostic messages for type
conversion in which information is lost (such as conversion of variables from
factor to character; standardisation of variable names; or removal of
meta-data such as those associated with
<a href="https://r-spatial.github.io/sf/"><code>sf</code>-format</a> data) or added (such as
insertion of variable or column names where none were provided).</em>
</li>
</ul>
<p>Note, for example, that an <code>array</code> may have column names which start with
numeric values, but that a <code>data.frame</code> may not.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span> <span class="op">(</span><span class="fl">1</span>, dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>, dimnames <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="st">"1"</span>, <span class="st">"2"</span><span class="op">)</span><span class="op">)</span> <span class="co"># okay</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span></code></pre></div>
<pre><code>##   2
## 1 1</code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span></code></pre></div>
<pre><code>##   X2
## 1  1</code></pre>
<p>If <code>array</code> or <code>matrix</code> class objects are accepted as input, then <strong>G2.8</strong>
implies that routines should be implemented to check for such conversion of
column names.</p>
<p>The next standard concerns the following inconsistencies between three common
tabular classes in regard the column extraction operator, <code>[</code>.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">iris</span> <span class="co"># data.frame from the datasets package</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="co">#&gt; [1] "data.frame"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span>
<span class="co">#&gt; [1] "numeric"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span>, drop <span class="op">=</span> <span class="cn">TRUE</span><span class="op">]</span><span class="op">)</span> <span class="co"># default</span>
<span class="co">#&gt; [1] "numeric"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span>, drop <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span><span class="op">)</span>
<span class="co">#&gt; [1] "data.frame"</span>

<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html">tibble</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span>
<span class="co">#&gt; [1] "tbl_df"     "tbl"        "data.frame"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span>, drop <span class="op">=</span> <span class="cn">TRUE</span><span class="op">]</span><span class="op">)</span>
<span class="co">#&gt; [1] "numeric"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span>, drop <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span><span class="op">)</span> <span class="co"># default</span>
<span class="co">#&gt; [1] "tbl_df"     "tbl"        "data.frame"</span>

<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">data.table</span><span class="fu">::</span><span class="fu">data.table</span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span><span class="op">]</span><span class="op">)</span>
<span class="co">#&gt; [1] "data.table" "data.frame"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span>, drop <span class="op">=</span> <span class="cn">TRUE</span><span class="op">]</span><span class="op">)</span> <span class="co"># no effect</span>
<span class="co">#&gt; [1] "data.table" "data.frame"</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span> <span class="op">[</span>, <span class="fl">1</span>, drop <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span><span class="op">)</span> <span class="co"># default</span>
<span class="co">#&gt; [1] "data.table" "data.frame"</span></code></pre></div>
<ul>
<li>Extracting a single column from a <code>data.frame</code> returns a <code>vector</code> by default,
and a <code>data.frame</code> if <code>drop = FALSE</code>.</li>
<li>Extracting a single column from a <code>tibble</code> returns a single-column <code>tibble</code>
by default, and a <code>vector</code> if <code>drop = TRUE</code>.</li>
<li>Extracting a single column from a <code>data.table</code> always returns a <code>data.table</code>,
and the <code>drop</code> argument has no effect.</li>
</ul>
<p>Given such inconsistencies,</p>
<ul>
<li>
<span id="G2_10"><strong>G2.10</strong></span> <em>Software should ensure that extraction or filtering of
single columns from tabular inputs should not presume any particular default
behaviour, and should ensure all column-extraction operations behave
consistently regardless of the class of tabular data used as input.</em>
</li>
</ul>
<p>Adherence to the above standard <strong>G2.8</strong> will ensure that any implicitly or
explicitly assumed default behaviour will yield consistent results regardless
of input classes.</p>
<p><strong>Columns of tabular inputs</strong></p>
<p>The follow standards apply to <code>data.frame</code>-like tabular objects (including all
derived and otherwise compatible classes), and so do not apply to <code>matrix</code> or
<code>array</code> objects.</p>
<ul>
<li>
<span id="G2_11"><strong>G2.11</strong></span> <em>Software should ensure that <code>data.frame</code>-like tabular
objects which have columns which do not themselves have standard class
attributes (typically, <code>vector</code>) are appropriately processed, and do not
error without reason. This behaviour should be tested. Again, columns created
by the <a href="https://github.com/r-quantities/units/"><code>units</code> package</a> provide
a good test case.</em>
</li>
<li>
<span id="G2_12"><strong>G2.12</strong></span> <em>Software should ensure that <code>data.frame</code>-like tabular
objects which have list columns should ensure that those columns are
appropriately pre-processed either through being removed, converted to
equivalent vector columns where appropriate, or some other appropriate
treatment such as an informative error. This behaviour should be tested.</em>
</li>
</ul>
</div>
<div id="missing-or-undefined-values" class="section level4">
<h4>
<span class="header-section-number">6.1.2.3</span> Missing or Undefined Values<a class="anchor" aria-label="anchor" href="#missing-or-undefined-values"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="G2_13"><strong>G2.13</strong></span> <em>Statistical Software should implement appropriate checks
for missing data as part of initial pre-processing prior to passing data to
analytic algorithms.</em>
</li>
<li>
<span id="G2_14"><strong>G2.14</strong></span> <em>Where possible, all functions should provide options for
users to specify how to handle missing (<code>NA</code>) data, with options minimally
including:</em>
<ul>
<li>
<span id="G2_14a"><strong>G2.14a</strong></span> <em>error on missing data</em>
</li>
<li>
<span id="G2_14b"><strong>G2.14b</strong></span> <em>ignore missing data with default warnings or
messages issued</em>
</li>
<li>
<span id="G2_14c"><strong>G2.14c</strong></span> <em>replace missing data with appropriately imputed
values</em>
</li>
</ul>
</li>
<li>
<span id="G2_15"><strong>G2.15</strong></span> <em>Functions should never assume non-missingness, and
should never pass data with potential missing values to any base routines
with default <code>na.rm = FALSE</code>-type parameters (such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/mean.html"><code>mean()</code></a>,
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sd.html"><code>sd()</code></a> or
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html"><code>cor()</code></a>).</em>
</li>
<li>
<span id="G2_16"><strong>G2.16</strong></span> <em>All functions should also provide options to handle
undefined values (e.g., <code>NaN</code>, <code>Inf</code> and <code>-Inf</code>), including potentially
ignoring or removing such values.</em>
</li>
</ul>
</div>
</div>
<div id="algorithms" class="section level3">
<h3>
<span class="header-section-number">6.1.3</span> Algorithms<a class="anchor" aria-label="anchor" href="#algorithms"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="G3_0"><strong>G3.0</strong></span> <em>Statistical software should never compare floating point
numbers for equality. All numeric equality comparisons should either ensure
that they are made between integers, or use appropriate tolerances for
approximate equality.</em>
</li>
</ul>
<p>This standard applies to all computer languages included in any package. In R,
values can be affirmed to be integers through <code><a href="https://rdrr.io/r/base/integer.html">is.integer()</a></code>, or asserting that
the <code><a href="https://rdrr.io/r/base/mode.html">storage.mode()</a></code> of an object is “integer”. One way to compare numeric
values with tolerance is with the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/all.equal.html"><code>all.equal()</code>
function</a>,
which accepts an additional <code>tolerance</code> parameter with a default for <code>numeric</code>
comparison of <code>sqrt(.Machine$double.eps)</code>, which is typically around e(-8–10).
In other languages, including C and C++, comparisons of floating point numbers
are commonly implemented by conditions such as <code>if (abs(a - b) &lt; tol)</code>, where
<code>tol</code> specifies the tolerance for equality.</p>
<p>Importantly, R functions such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/duplicated.html"><code>duplicated()</code></a>
and
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/unique.html"><code>unique()</code></a>
rely on equality comparisons, and this standard extends to require that
software should not apply any functions which themselves rely on equality
comparisons to floating point numbers.</p>
<ul>
<li>
<span id="G3_1"><strong>G3.1</strong></span> <em>Statistical software which relies on covariance
calculations should enable users to choose between different algorithms for
calculating covariances, and should not rely solely on covariances from the
<code><a href="https://rdrr.io/r/stats/cor.html">stats::cov</a></code> function.</em>
<ul>
<li>
<span id="G3_1a"><strong>G3.1a</strong></span> <em>The ability to use arbitrarily specified covariance
methods should be documented (typically in examples or vignettes).</em>
</li>
</ul>
</li>
</ul>
<p>Estimates of covariance can be very sensitive to outliers, and a variety of
methods have been developed for “robust” estimates of covariance, implemented
in such packages as <a href="https://cran.r-project.org/package=rms"><code>rms</code></a>,
<a href="https://cran.r-project.org/package=robust"><code>robust</code></a>, and
<a href="https://cran.r-project.org/package=sandwich"><code>sandwich</code></a>. Adhering to this
standard merely requires an ability for a user to specify a particular
covariance function, such as through an additional parameter. The <code><a href="https://rdrr.io/r/stats/cor.html">stats::cov</a></code>
function can be used as a default, and additional packages such as the three
listed here need not necessarily be listed as <code>Imports</code> to a package.</p>
</div>
<div id="output-structures" class="section level3">
<h3>
<span class="header-section-number">6.1.4</span> Output Structures<a class="anchor" aria-label="anchor" href="#output-structures"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="G4_0"><strong>G4.0</strong></span> <em>Statistical Software which enables outputs to be written
to local files should parse parameters specifying file names to ensure
appropriate file suffices are automatically generated where not provided.</em>
</li>
</ul>
</div>
<div id="testing" class="section level3">
<h3>
<span class="header-section-number">6.1.5</span> Testing<a class="anchor" aria-label="anchor" href="#testing"><i class="fas fa-link"></i></a>
</h3>
<p>All packages should follow rOpenSci standards on
<a href="https://devguide.ropensci.org/building.html#testing">testing</a> and <a href="https://devguide.ropensci.org/ci.html">continuous
integration</a>, including aiming for high
test coverage. Extant R packages which may be useful for testing include
<a href="https://testthat.r-lib.org"><code>testthat</code></a>,
<a href="https://github.com/markvanderloo/tinytest"><code>tinytest</code></a>,
<a href="https://github.com/mikldk/roxytest"><code>roxytest</code></a>, and
<a href="https://github.com/LudvigOlsen/xpectr"><code>xpectr</code></a>.</p>
<div id="test-data-sets" class="section level4">
<h4>
<span class="header-section-number">6.1.5.1</span> Test Data Sets<a class="anchor" aria-label="anchor" href="#test-data-sets"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="G5_0"><strong>G5.0</strong></span> <em>Where applicable or practicable, tests should use standard
data sets with known properties (for example, the <a href="https://www.itl.nist.gov/div898/strd/">NIST Standard Reference
Datasets</a>, or data sets provided by
other widely-used R packages).</em>
</li>
<li>
<span id="G5_1"><strong>G5.1</strong></span> <em>Data sets created within, and used to test, a package
should be exported (or otherwise made generally available) so that users can
confirm tests and run examples.</em>
</li>
</ul>
</div>
<div id="responses-to-unexpected-input" class="section level4">
<h4>
<span class="header-section-number">6.1.5.2</span> Responses to Unexpected Input<a class="anchor" aria-label="anchor" href="#responses-to-unexpected-input"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="G5_2"><strong>G5.2</strong></span> <em>Appropriate error and warning behaviour of all functions
should be explicitly demonstrated through tests. In particular,</em>
<ul>
<li>
<span id="G5_2a"><strong>G5.2a</strong></span> <em>Every message produced within R code by <code><a href="https://rdrr.io/r/base/stop.html">stop()</a></code>,
<code><a href="https://rdrr.io/r/base/warning.html">warning()</a></code>, <code><a href="https://rdrr.io/r/base/message.html">message()</a></code>, or equivalent should be unique</em>
</li>
<li>
<span id="G5_2b"><strong>G5.2b</strong></span> <em>Explicit tests should demonstrate conditions which
trigger every one of those messages, and should compare the result with
expected values.</em>
</li>
</ul>
</li>
<li>
<span id="G5_3"><strong>G5.3</strong></span> <em>For functions which are expected to return objects
containing no missing (<code>NA</code>) or undefined (<code>NaN</code>, <code>Inf</code>) values, the absence
of any such values in return objects should be explicitly tested.</em>
</li>
</ul>
</div>
<div id="algorithm-tests" class="section level4">
<h4>
<span class="header-section-number">6.1.5.3</span> Algorithm Tests<a class="anchor" aria-label="anchor" href="#algorithm-tests"><i class="fas fa-link"></i></a>
</h4>
<p>For testing <em>statistical algorithms</em>, tests should include tests of the
following types:</p>
<ul>
<li>
<span id="G5_4"><strong>G5.4</strong></span> <strong>Correctness tests</strong> <em>to test that statistical algorithms
produce expected results to some fixed test data sets (potentially through
comparisons using binding frameworks such as
<a href="https://github.com/lbraglia/RStata">RStata</a>).</em>
<ul>
<li>
<span id="G5_4a"><strong>G5.4a</strong></span> <em>For new methods, it can be difficult to separate out
correctness of the method from the correctness of the implementation, as
there may not be reference for comparison. In this case, testing may be
implemented against simple, trivial cases or against multiple
implementations such as an initial R implementation compared with results
from a C/C++ implementation.</em>
</li>
<li>
<span id="G5_4b"><strong>G5.4b</strong></span> <em>For new implementations of existing methods,
correctness tests should include tests against previous implementations.
Such testing may explicitly call those implementations in testing,
preferably from fixed-versions of other software, or use stored outputs
from those where that is not possible.</em>
</li>
<li>
<span id="G5_4c"><strong>G5.4c</strong></span> <em>Where applicable, stored values may be drawn from
published paper outputs when applicable and where code from original
implementations is not available</em>
</li>
</ul>
</li>
<li>
<span id="G5_5"><strong>G5.5</strong></span> <em>Correctness tests should be run with a fixed random seed</em>
</li>
<li>
<span id="G5_6"><strong>G5.6</strong></span> <strong>Parameter recovery tests</strong> <em>to test that the
implementation produce expected results given data with known properties.
For instance, a linear regression algorithm should return expected
coefficient values for a simulated data set generated from a linear model.</em>
<ul>
<li>
<span id="G5_6a"><strong>G5.6a</strong></span> <em>Parameter recovery tests should generally be
expected to succeed within a defined tolerance rather than recovering
exact values.</em>
</li>
<li>
<span id="G5_6b"><strong>G5.6b</strong></span> <em>Parameter recovery tests should be run with multiple
random seeds when either data simulation or the algorithm contains
a random component. (When long-running, such tests may be part of an
extended, rather than regular, test suite; see G4.10-4.12, below).</em>
</li>
</ul>
</li>
<li>
<span id="G5_7"><strong>G5.7</strong></span> <strong>Algorithm performance tests</strong> <em>to test that
implementation performs as expected as properties of data change. For
instance, a test may show that parameters approach correct estimates within
tolerance as data size increases, or that convergence times decrease for
higher convergence thresholds.</em>
</li>
<li>
<span id="G5_8"><strong>G5.8</strong></span> <strong>Edge condition tests</strong> <em>to test that these conditions
produce expected behaviour such as clear warnings or errors when confronted
with data with extreme properties including but not limited to:</em>
<ul>
<li>
<span id="G5_8a"><strong>G5.8a</strong></span> <em>Zero-length data</em>
</li>
<li>
<span id="G5_8b"><strong>G5.8b</strong></span> <em>Data of unsupported types (e.g., character or
complex numbers in for functions designed only for numeric data)</em>
</li>
<li>
<span id="G5_8c"><strong>G5.8c</strong></span> <em>Data with all-<code>NA</code> fields or columns or all
identical fields or columns</em>
</li>
<li>
<span id="G5_8d"><strong>G5.8d</strong></span> <em>Data outside the scope of the algorithm (for
example, data with more fields (columns) than observations (rows) for
some regression algorithms)</em>
</li>
</ul>
</li>
<li>
<span id="G5_9"><strong>G5.9</strong></span> <strong>Noise susceptibility tests</strong> <em>Packages should test for
expected stochastic behaviour, such as through the following conditions:</em>
<ul>
<li>
<span id="G5_9a"><strong>G5.9a</strong></span> <em>Adding trivial noise (for example, at the scale of
<code>.Machine$double.eps</code>) to data does not meaningfully change results</em>
</li>
<li>
<span id="G5_9c"><strong>G5.9b</strong></span> <em>Running under different random seeds or initial
conditions does not meaningfully change results</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="extended-tests" class="section level4">
<h4>
<span class="header-section-number">6.1.5.4</span> Extended tests<a class="anchor" aria-label="anchor" href="#extended-tests"><i class="fas fa-link"></i></a>
</h4>
<p>Thorough testing of statistical software may require tests on large data sets,
tests with many permutations, or other conditions leading to long-running
tests. In such cases it may be neither possible nor advisable to execute tests
continuously, or with every code change. Software should nevertheless test any
and all conditions regardless of how long tests may take, and in doing so
should adhere to the following standards:</p>
<ul>
<li>
<span id="G5_10"><strong>G5.10</strong></span> <em>Extended tests should included and run under a common
framework with other tests but be switched on by flags such as as
a <code>&lt;MYPKG&gt;_EXTENDED_TESTS=1</code> environment variable.</em>
</li>
<li>
<span id="G5_11"><strong>G5.11</strong></span> <em>Where extended tests require large data sets or other
assets, these should be provided for downloading and fetched as part of the
testing workflow.</em>
<ul>
<li>
<span id="G5_11a"><strong>G5.11a</strong></span> <em>When any downloads of additional data necessary
for extended tests fail, the tests themselves should not fail, rather be
skipped and implicitly succeed with an appropriate diagnostic message.</em>
</li>
</ul>
</li>
<li>
<span id="G5_12"><strong>G5.12</strong></span> <em>Any conditions necessary to run extended tests such as
platform requirements, memory, expected runtime, and artefacts produced that
may need manual inspection, should be described in developer documentation
such as a <code>CONTRIBUTING.md</code> or <code>tests/README.md</code> file.</em>
</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="standards-bayesian" class="section level2">
<h2>
<span class="header-section-number">6.2</span> Bayesian and Monte Carlo Software<a class="anchor" aria-label="anchor" href="#standards-bayesian"><i class="fas fa-link"></i></a>
</h2>
<p>Bayesian and Monte Carlo software centres on quantitative estimation of
components of <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Baye’s theorem</a>,
particularly on estimation or application of prior and/or posterior probability
distributions. The procedures implemented to estimate the properties of such
distributions are commonly based on random sampling procedures, hence referred
to as “<em>Monte Carlo</em>” routines in reference to the random yet quantifiable
nature of casino games. The scope of this category also includes algorithms
which focus on sampling routines only, such as Markov-Chain Monte Carlo (MCMC)
procedures, independent of application in Bayesian analyses.</p>
<p>The term “model” is understood with reference here to Bayesian software to
refer to an encoded description of how parameters specifying aspects of one or
more prior distributions are transformed into (properties of) one or more
posterior distributions.</p>
<p>Some examples of Bayesian and Monte Carlo software include:</p>
<ol style="list-style-type: decimal">
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.01541"><code>bayestestR</code>
package</a> which “provides
tools to describe … posterior distributions”</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.01143"><code>ArviZ</code> package</a>
python package for exploratory analyses of Bayesian models, particularly
posterior distributions.</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.00216"><code>GammaGompertzCR</code>
package</a>, which features
explicit diagnostics of MCMC convergence statistics.</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.00425"><code>BayesianNetwork</code>
package</a>, which is in
many ways a wrapper package primarily serving a <code>shiny</code> app, and is also
accordingly a package in the EDA category.</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.01427"><code>fmcmc</code> package</a>,
which is a “classic” MCMC package which directly provides its own
implementation, and generates its own convergence statistics.</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.00739"><code>rsimsum</code> package</a>
which “summarise[s] results from Monte Carlo simulation studies”.
Many of the statistics generated by this package are useful for assessing
and comparing Bayesian and Monte Carlo software in general. (See also the
<a href="https://joss.theoj.org/papers/10.21105/joss.00640"><code>MCMCvis</code> package</a>, with
more of a focus on visualisation.)</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.00061"><code>walkr</code> package</a> for
“MCMC Sampling from Non-Negative Convex Polytopes”. This package is also
indicative of the difficulties of deriving generally applicable assessments
of software in this category, because MCMC <em>sampling</em> relies on
fundamentally different inputs and outputs than many other MCMC routines.</li>
</ol>
<p>Click on the following link to view a demonstration <a href="https://hackmd.io/zVWTAl9ZQeCcj_bvMGcmMQ">Application of Bayesian
and Monte Carlo Standards</a>.</p>
<p>Bayesian and Monte Carlo Software (hereafter referred to for simplicity as
“Bayesian Software”) is presumed to perform one or more of the following steps:</p>
<ol style="list-style-type: decimal">
<li>Document how to specify inputs including:
<ul>
<li>1.1 Data</li>
<li>1.2 Parameters determining prior distributions</li>
<li>1.3 Parameters determining the computational processes</li>
</ul>
</li>
<li>Accept and validate all of forms of input</li>
<li>Apply data transformation and pre-processing steps</li>
<li>Apply one or more analytic algorithms, generally sampling algorithms used to
generate estimates of posterior distributions</li>
<li>Return the result of that algorithmic application</li>
<li>Offer additional functionality such as printing or summarising return results</li>
</ol>
<p>This chapter details standards for each of these steps, each prefixed with “BS”.</p>
<div id="documentation-of-inputs" class="section level3">
<h3>
<span class="header-section-number">6.2.1</span> Documentation of Inputs<a class="anchor" aria-label="anchor" href="#documentation-of-inputs"><i class="fas fa-link"></i></a>
</h3>
<p>Prior to actual standards for documentation of inputs, we note one
terminological standard for Bayesian software which uses the term
“hyperparameter”:</p>
<ul>
<li>
<span id="BS1_0"><strong>BS1.0</strong></span> <em>Bayesian software which uses the term “hyperparameter”
should explicitly clarify the meaning of that term in the context of that
software.</em>
</li>
</ul>
<p>This standard reflects the dual facts that this term is frequently used in
Bayesian software, yet has no unambiguous definition or interpretation. The
term “hyperparameter” is also used in other statistical contexts in ways that
are often distinctly different from its common use in Bayesian analyses. Examples of the kinds of clarifications required to adhere to this standard include,</p>
<blockquote>
<p>Hyperparameters refer here to parameters determining the form of prior
distributions that conditionally depend on other parameters.</p>
</blockquote>
<p>Such a clarification would then require further explicit distinction between
“parameters” and “hyperparameters”. The remainder of these standards does not
refer to “hyperparameters”, rather attempts to make explicit distinctions
between different kinds of parameters, such as distributional or algorithmic
control parameters. Beyond this standard, Bayesian Software should provide the
following documentation of how to specify inputs:</p>
<ul>
<li>
<span id="BS1_1"><strong>BS1.1</strong></span> <em>Descriptions of how to enter data, both in textual form
and via code examples. Both of these should consider the simplest cases of
single objects representing independent and dependent data, and potentially
more complicated cases of multiple independent data inputs.</em>
</li>
<li>
<span id="BS1_2"><strong>BS1.2</strong></span> <em>Description of how to specify prior distributions, both
in textual form
describing the general principles of specifying prior distributions, along
with more applied descriptions and examples, within:</em>
<ul>
<li>
<span id="BS1_2a"><strong>B31.2a</strong></span> <em>The main package <code>README</code>, either as textual
description or example code</em> <span id="BS1_2b"><strong>B31.2b</strong></span> <em>At least one package
vignette, both as general and applied textual descriptions, and example
code</em> <span id="BS1_2c"><strong>B31.2c</strong></span> <em>Function-level documentation, preferably
with code included in examples</em> <span id="BS1_3"><strong>BS1.3</strong></span> <em>Description of all
parameters which control the computational process (typically those
determining aspects such as numbers and lengths of sampling processes,
seeds used to start them, thinning parameters determining post-hoc
sampling from simulated values, and convergence criteria). In
particular:</em>
</li>
<li>
<span id="BS1_3a"><strong>BS1.3a</strong></span> <em>Bayesian Software should document, both in text
and examples, how to use the output of previous simulations as starting
points of subsequent simulations.</em> <span id="BS1_3b"><strong>BS1.3b</strong></span> <em>Where
applicable, Bayesian software should document, both in text and examples,
how to use different sampling algorithms for a given model.</em>
<span id="BS1_4"><strong>BS1.4</strong></span> <em>For Bayesian Software which implements or otherwise
enables convergence checkers, documentation should explicitly describe
and provide examples of use with and without convergence checkers.</em>
</li>
</ul>
</li>
<li>
<span id="BS1_5"><strong>BS1.5</strong></span> <em>For Bayesian Software which implements or otherwise
enables multiple
convergence checkers, differences between these should be explicitly tested.</em>
</li>
</ul>
</div>
<div id="input-data-structures-and-validation" class="section level3">
<h3>
<span class="header-section-number">6.2.2</span> Input Data Structures and Validation<a class="anchor" aria-label="anchor" href="#input-data-structures-and-validation"><i class="fas fa-link"></i></a>
</h3>
<p>This section contains standards primarily intended to ensure that input data,
including model specifications, are validated prior to passing through to the
main computational algorithms.</p>
<div id="input-data" class="section level4">
<h4>
<span class="header-section-number">6.2.2.1</span> Input Data<a class="anchor" aria-label="anchor" href="#input-data"><i class="fas fa-link"></i></a>
</h4>
<p>Bayesian Software is commonly designed to accept generic one- or
two-dimensional forms such as vector, matrix, or <code>data.frame</code> objects, for
which the following standard applies.</p>
<ul>
<li>
<span id="BS2_1"><strong>BS2.1</strong></span> <em>Bayesian Software should implement pre-processing
routines to ensure all input data is dimensionally commensurate, for example
by ensuring commensurate lengths of vectors or numbers of rows of tabular
inputs.</em>
<ul>
<li>
<span id="BS2_1a"><strong>BS2.1a</strong></span> <em>The effects of such routines should be tested.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="prior-distributions-model-specifications-and-distributional-parameters" class="section level4">
<h4>
<span class="header-section-number">6.2.2.2</span> Prior Distributions, Model Specifications, and Distributional Parameters<a class="anchor" aria-label="anchor" href="#prior-distributions-model-specifications-and-distributional-parameters"><i class="fas fa-link"></i></a>
</h4>
<p>The second set of standards in this section concern specification of prior
distributions, model structures, or other equivalent ways of specifying
hypothesised relationships among input data structures. R already has a diverse
range of Bayesian Software with distinct approaches to this task, commonly
either through specifying a model as a character vector representing an R
function, or an external file either as R code, or encoded according to some
alternative system (such as for <a href="https://mc-stan.org/rstan/"><code>rstan</code></a>).</p>
<p>Bayesian Software should:</p>
<ul>
<li>
<span id="BS2_2"><strong>BS2.2</strong></span> <em>Ensure that all appropriate validation and
pre-processing of distributional parameters are implemented as distinct
pre-processing steps prior to submitting to analytic routines, and especially
prior to submitting to multiple parallel computational chains.</em>
</li>
<li>
<span id="BS2_3"><strong>BS2.3</strong></span> <em>Ensure that lengths of vectors of distributional
parameters are
checked, with no excess values silently discarded (unless such output is
explicitly suppressed, as detailed below).</em>
</li>
<li>
<span id="BS2_4"><strong>BS2.4</strong></span> <em>Ensure that lengths of vectors of distributional
parameters are
commensurate with expected model input (see example immediately below)</em>
</li>
<li>
<span id="BS2_5"><strong>BS2.5</strong></span> <em>Where possible, implement pre-processing checks to
validate
appropriateness of numeric values submitted for distributional parameters;
for example, by ensuring that distributional parameters defining second-order
moments such as distributional variance or shape parameters, or any
parameters which are logarithmically transformed, are non-negative.</em>
</li>
</ul>
<p>The following example demonstrates how standards like the above (BS2.4-2.5)
might be addressed. Consider the following function which defines a
log-likelihood estimator for a linear regression, controlled via a vector of
three distributional parameters, <code>p</code>:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ll</span> <span class="op">&lt;-</span> <span class="kw">function</span> <span class="op">(</span><span class="va">x</span>, <span class="va">y</span>, <span class="va">p</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">x</span> <span class="op">*</span> <span class="va">p</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, sd <span class="op">=</span> <span class="va">p</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<p>Pre-processing stages should be used to determine:</p>
<ol style="list-style-type: decimal">
<li>That the dimensions of the input data, <code>x</code> and <code>y</code>, are commensurate (BS2.1);
non-commensurate inputs should error by default.</li>
<li>The length of the vector <code>p</code> (BS2.3)</li>
</ol>
<p>The latter task is not necessarily straightforward, because the definition of
the function, <code>ll()</code>, will itself generally be part of the input to an actual
Bayesian Software function. This functional input thus needs to be examined to
determine expected lengths of hyperparameter vectors. The following code
illustrates one way to achieve this, relying on utilities for parsing function
calls in R, primarily through the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/utils/html/getParseData.html"><code>getParseData</code></a>
function from the <code>utils</code> package. The parse data for a function can be
extracted with the following line:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/getParseData.html">getParseData</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/parse.html">parse</a></span> <span class="op">(</span>text <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html">deparse</a></span> <span class="op">(</span><span class="va">ll</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>The object <code>x</code> is a <code>data.frame</code> of every R token (such as an expression,
symbol, or operator) parsed from the function <code>ll</code>. The following section
illustrates how this data can be used to determine the expected lengths of
vector inputs to the function, <code>ll()</code>.</p>
<details><summary>
click to see details
</summary><p>
</p>
<p>Input arguments used to define parameter vectors in any R software are accessed
through R’s standard vector access syntax of <code>vec[i]</code>, for some element <code>i</code> of
a vector <code>vec</code>. The parse data for such begins with the <code>SYMBOL</code> of <code>vec</code>, the
<code>[</code>, a <code>NUM_CONST</code> for the value of <code>i</code>, and a closing <code>]</code>. The following code
can be used to extract elements of the parse data which match this pattern, and
ultimately to extract the various values of <code>i</code> used to access members of
<code>vec</code>.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">vector_length</span> <span class="op">&lt;-</span> <span class="kw">function</span> <span class="op">(</span><span class="va">x</span>, <span class="va">i</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">xn</span> <span class="op">&lt;-</span> <span class="va">x</span> <span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span> <span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">token</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span> <span class="op">(</span><span class="st">"SYMBOL"</span>, <span class="st">"NUM_CONST"</span>, <span class="st">"'['"</span>, <span class="st">"']'"</span><span class="op">)</span><span class="op">)</span>, <span class="op">]</span>
    <span class="co"># split resultant data.frame at first "SYMBOL" entry</span>
    <span class="va">xn</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/split.html">split</a></span> <span class="op">(</span><span class="va">xn</span>, <span class="fu"><a href="https://rdrr.io/r/base/cumsum.html">cumsum</a></span> <span class="op">(</span><span class="va">xn</span><span class="op">$</span><span class="va">token</span> <span class="op">==</span> <span class="st">"SYMBOL"</span><span class="op">)</span><span class="op">)</span>
    <span class="co"># reduce to only those matching the above pattern</span>
    <span class="va">xn</span> <span class="op">&lt;-</span> <span class="va">xn</span> <span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">vapply</a></span> <span class="op">(</span><span class="va">xn</span>, <span class="kw">function</span> <span class="op">(</span><span class="va">j</span><span class="op">)</span>
                             <span class="va">j</span><span class="op">$</span><span class="va">text</span> <span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">==</span> <span class="va">i</span> <span class="op">&amp;</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span> <span class="op">(</span><span class="va">j</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">3</span>,
                             <span class="fu"><a href="https://rdrr.io/r/base/logical.html">logical</a></span> <span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">]</span>
    <span class="va">ret</span> <span class="op">&lt;-</span> <span class="cn">NA_integer_</span> <span class="co"># default return value</span>
    <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span> <span class="op">(</span><span class="va">xn</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span>
        <span class="co"># get all values of NUM_CONST as integers</span>
        <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">vapply</a></span> <span class="op">(</span><span class="va">xn</span>, <span class="kw">function</span> <span class="op">(</span><span class="va">j</span><span class="op">)</span>
                         <span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span> <span class="op">(</span><span class="va">j</span><span class="op">$</span><span class="va">text</span> <span class="op">[</span><span class="va">j</span><span class="op">$</span><span class="va">token</span> <span class="op">==</span> <span class="st">"NUM_CONST"</span><span class="op">]</span> <span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>,
                         <span class="fu"><a href="https://rdrr.io/r/base/integer.html">integer</a></span> <span class="op">(</span><span class="fl">1</span><span class="op">)</span>, USE.NAMES <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>
        <span class="co"># and return max of these</span>
        <span class="va">ret</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span> <span class="op">(</span><span class="va">n</span><span class="op">)</span>
    <span class="op">}</span>
    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span> <span class="op">(</span><span class="va">ret</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>That function can then be used to determine the length of any inputs which are
used as hyperparameter vectors:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ll</span> <span class="op">&lt;-</span> <span class="kw">function</span> <span class="op">(</span><span class="va">p</span>, <span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span> <span class="op">(</span><span class="va">y</span> <span class="op">-</span> <span class="op">(</span><span class="va">p</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">x</span> <span class="op">*</span> <span class="va">p</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span>, sd <span class="op">=</span> <span class="va">p</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/parse.html">parse</a></span> <span class="op">(</span>text <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/deparse.html">deparse</a></span> <span class="op">(</span><span class="va">ll</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">utils</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/utils/getParseData.html">getParseData</a></span> <span class="op">(</span><span class="va">p</span><span class="op">)</span>

<span class="co"># extract the names of the parameters:</span>
<span class="va">params</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html">unique</a></span> <span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">text</span> <span class="op">[</span><span class="va">x</span><span class="op">$</span><span class="va">token</span> <span class="op">==</span> <span class="st">"SYMBOL"</span><span class="op">]</span><span class="op">)</span>
<span class="va">lens</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">vapply</a></span> <span class="op">(</span><span class="va">params</span>, <span class="kw">function</span> <span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="fu">vector_length</span> <span class="op">(</span><span class="va">x</span>, <span class="va">i</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/integer.html">integer</a></span> <span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span>
<span class="va">lens</span>
<span class="co">#&gt;  y  p  x </span>
<span class="co">#&gt; NA  3 NA</span></code></pre></div>
<p>And the vector <code>p</code> is used as a hyperparameter vector containing three
parameters. Any initial value vectors can then be examined to ensure that they
have this same length.</p>
<hr></details><p><br></p>
<p>Not all Bayesian Software is designed to accept model inputs expressed as
R code. The <a href="https://github.com/stan-dev/rstan"><code>rstan</code> package</a>, for example,
implements its own model specification language, and only allows distributional
parameters to be named, and not addressed by index. While this largely avoids
problems of mismatched lengths of parameter vectors, the software (at v2.21.1)
does not ensure the existence of named parameters prior to starting the
computational chains. This ultimately results in each chain generating an error
when a model specification refers to a non-existent or undefined
distributional parameter. Such controls should be part of a single
pre-processing stage, and so should only generate a single error.</p>
</div>
<div id="computational-parameters" class="section level4">
<h4>
<span class="header-section-number">6.2.2.3</span> Computational Parameters<a class="anchor" aria-label="anchor" href="#computational-parameters"><i class="fas fa-link"></i></a>
</h4>
<p>Computational parameters are considered here distinct from distributional
parameters, and commonly passed to Bayesian functions to directly control
computational processes. They typically include parameters controlling lengths
of runs, lengths of burn-in periods, numbers of parallel computations, other
parameters controlling how samples are to be generated, or convergence
criteria. All Computational Parameters should be checked for general “sanity”
prior to calling primary computational algorithms. The standards for such
sanity checks include that Bayesian Software should:</p>
<ul>
<li>
<span id="BS2_6"><strong>BS2.6</strong></span> <em>Check that values for computational parameters lie
within plausible ranges.</em>
</li>
</ul>
<p>While admittedly not always possible to define, plausible ranges may be as
simple as ensuring values are greater than zero. Where possible, checks should
nevertheless ensure appropriate responses to extremely large values, for
example by issuing diagnostic messages about likely long computational times.
The following two sub-sections consider particular cases of computational
parameters.</p>
</div>
<div id="parameters-controlling-start-values" class="section level4">
<h4>
<span class="header-section-number">6.2.2.4</span> Parameters Controlling Start Values<a class="anchor" aria-label="anchor" href="#parameters-controlling-start-values"><i class="fas fa-link"></i></a>
</h4>
<p>Bayesian software generally relies on sequential random sampling procedures,
with each sequence uniquely determined by (among other aspects) the value at
which it is started. Given that, Bayesian software should:</p>
<ul>
<li>
<span id="BS2_7"><strong>BS2.7</strong></span> <em>Enable starting values to be explicitly controlled via
one or more input parameters, including multiple values for software which
implements or enables multiple computational “chains.”</em>
</li>
<li>
<span id="BS2_8"><strong>BS2.8</strong></span> <em>Enable results of previous runs to be used as starting
points for
subsequent runs.</em>
</li>
</ul>
<p>Bayesian Software which implements or enables multiple computational chains
should:</p>
<ul>
<li>
<span id="BS2_9"><strong>BS2.9</strong></span> <em>Ensure each chain is started with a different seed by
default.</em>
</li>
<li>
<span id="BS2_10"><strong>BS2.10</strong></span> <em>Issue diagnostic messages when identical seeds are
passed to distinct
computational chains.</em>
</li>
<li>
<span id="BS2_11"><strong>BS2.11</strong></span> <em>Software which accepts starting values as a vector
should provide
the parameter with a plural name: for example, “starting_values” and not
“starting_value”.</em>
</li>
</ul>
<p>To avoid potential confusion between separate parameters to control random
seeds and starting values, we recommended a single “starting values” rather
than “seeds” argument, with appropriate translation of these parameters into
seeds where necessary.</p>
</div>
<div id="output-verbosity" class="section level4">
<h4>
<span class="header-section-number">6.2.2.5</span> Output Verbosity<a class="anchor" aria-label="anchor" href="#output-verbosity"><i class="fas fa-link"></i></a>
</h4>
<p>All Bayesian Software should implement computational parameters to control
output verbosity. Bayesian computations are often time-consuming, and often
performed as batch computations. The following standards should be adhered to
in regard to output verbosity:</p>
<ul>
<li>
<span id="BS2_12"><strong>BS2.12</strong></span> <em>Bayesian Software should implement at least one
parameter controlling the verbosity of output, defaulting to verbose output
of all appropriate messages, warnings, errors, and progress indicators.</em>
</li>
<li>
<span id="BS2_13"><strong>BS2.13</strong></span> <em>Bayesian Software should enable suppression of
messages and progress
indicators, while retaining verbosity of warnings and errors. This should be
tested.</em>
</li>
<li>
<span id="BS2_14"><strong>BS2.14</strong></span> <em>Bayesian Software should enable suppression of
warnings where
appropriate. This should be tested.</em>
</li>
<li>
<span id="BS2_15"><strong>BS2.15</strong></span> <em>Bayesian Software should explicitly enable errors to
be caught, and
appropriately processed either through conversion to warnings, or otherwise
captured in return values. This should be tested.</em>
</li>
</ul>
</div>
</div>
<div id="pre-processing-and-data-transformation" class="section level3">
<h3>
<span class="header-section-number">6.2.3</span> Pre-processing and Data Transformation<a class="anchor" aria-label="anchor" href="#pre-processing-and-data-transformation"><i class="fas fa-link"></i></a>
</h3>
<div id="missing-values" class="section level4">
<h4>
<span class="header-section-number">6.2.3.1</span> Missing Values<a class="anchor" aria-label="anchor" href="#missing-values"><i class="fas fa-link"></i></a>
</h4>
<p>In additional to the <em>General Standards</em> for missing values
(<strong>G2.13</strong>–<strong>2.16</strong>), and in particular <strong>G2.13</strong>, Bayesian Software should:</p>
<ul>
<li>
<span id="BS3_0"><strong>BS3.0</strong></span> <em>Explicitly document assumptions made in regard to
missing values; for example that data is assumed to contain no missing (<code>NA</code>,
<code>Inf</code>) values, and that such values, or entire rows including any such
values, will be automatically removed from input data.</em>
</li>
</ul>
</div>
<div id="perfect-collinearity" class="section level4">
<h4>
<span class="header-section-number">6.2.3.2</span> Perfect Collinearity<a class="anchor" aria-label="anchor" href="#perfect-collinearity"><i class="fas fa-link"></i></a>
</h4>
<p>Where appropriate, Bayesian Software should:</p>
<ul>
<li>
<span id="BS3_1"><strong>BS3.1</strong></span> <em>Implement pre-processing routines to diagnose perfect
collinearity, and provide appropriate diagnostic messages or warnings</em>
</li>
<li>
<span id="BS3_2"><strong>BS3.2</strong></span> <em>Provide distinct routines for processing perfectly
collinear data, potentially bypassing sampling algorithms</em>
</li>
</ul>
<p>An appropriate test for <strong>BS3.2</strong> would confirm that <code><a href="https://rdrr.io/r/base/system.time.html">system.time()</a></code> or
equivalent timing expressions for perfectly collinear data should be <em>less</em>
than equivalent routines called with non-collinear data. Alternatively, a test
could ensure that perfectly collinear data passed to a function with a stopping
criteria generated no results, while specifying a fixed number of iterations
may generate results.</p>
</div>
</div>
<div id="analytic-algorithms" class="section level3">
<h3>
<span class="header-section-number">6.2.4</span> Analytic Algorithms<a class="anchor" aria-label="anchor" href="#analytic-algorithms"><i class="fas fa-link"></i></a>
</h3>
<p>As mentioned, analytic algorithms for Bayesian Software are commonly algorithms
to simulate posterior distributions, and to draw samples from those
simulations. Numerous extant R packages implement and offer sampling
algorithms, and not all Bayesian Software will internally implement sampling
algorithms. The following standards apply to packages which do implement
internal sampling algorithms:</p>
<ul>
<li>
<span id="BS4_0"><strong>BS4.0</strong></span> <em>Packages should document sampling algorithms (generally
via literary citation, or reference to other software)</em>
</li>
<li>
<span id="BS4_1"><strong>BS4.1</strong></span> <em>Packages should provide explicit comparisons with
external samplers
which demonstrate intended advantage of implementation (generally via tests,
vignettes, or both).</em>
</li>
</ul>
<p>Regardless of whether or not Bayesian Software implements internal sampling
algorithms, it should:</p>
<ul>
<li>
<strong>BS4.2</strong> <em>Implement at least one means to validate posterior estimates.</em>
</li>
</ul>
<p>An example of posterior validation is the <a href="https://arxiv.org/abs/1804.06788">Simulation Based
Calibration</a> approach implemented in the
<a href="https://mc-stan.org/rstan"><code>rstan</code></a> function
<a href="https://mc-stan.org/rstan/reference/sbc.html"><code>sbc</code></a>). (Note also that the
<a href="https://cran.r-project.org/package=BayesValidate"><code>BayesValidate</code> package</a> has
not been updated for almost 15 years, so should not be directly used, although
ideas from that package may be adapted for validation purposes.) Beyond this,
where possible or applicable, Bayesian Software should:</p>
<ul>
<li>
<span id="BS4_3"><strong>BS4.3</strong></span> <em>Implement or otherwise offer at least one type of
convergence checker, and provide a documented reference for that
implementation.</em>
</li>
<li>
<span id="BS4_4"><strong>BS4.4</strong></span> <em>Enable computations to be stopped on convergence
(although not
necessarily by default).</em>
</li>
<li>
<span id="BS4_5"><strong>BS4.5</strong></span> <em>Ensure that appropriate mechanisms are provided for
models which do not
converge.</em>
</li>
</ul>
<p>This is often achieved by having default behaviour to stop after specified
numbers of iterations regardless of convergence.</p>
<ul>
<li>
<span id="BS4_6"><strong>BS4.6</strong></span> <em>Implement tests to confirm that results with convergence
checker are statistically equivalent to results from equivalent fixed number
of samples without convergence checking.</em>
</li>
<li>
<span id="BS4_7"><strong>BS4.7</strong></span> <em>Where convergence checkers are themselves parametrised,
the effects of
such parameters should also be tested. For threshold parameters, for example,
lower values should result in longer sequence lengths.</em>
</li>
</ul>
</div>
<div id="return-values" class="section level3">
<h3>
<span class="header-section-number">6.2.5</span> Return Values<a class="anchor" aria-label="anchor" href="#return-values"><i class="fas fa-link"></i></a>
</h3>
<p>Unlike software in many other categories, Bayesian Software should generally
return several kinds of distinct data, both the raw data derived from
statistical algorithms, and associated metadata. Such distinct and generally
disparate forms of data will be generally best combined into a single object
through implementing a defined class structure, although other options are
possible, including (re-)using extant class structures (see the CRAN Task view
on <a href="https://cran.r-project.org/web/views/Bayesian.html">Bayesian Inference</a> for
reference to other packages and class systems). Regardless of the precise form
of return object, and whether or not defined class structures are used or
implemented, the following standards apply:</p>
<ul>
<li>
<span id="BS5_0"><strong>BS5.0</strong></span> <em>Return values should include starting value(s) or
seed(s), including values for each sequence where multiple sequences are
included</em>
</li>
<li>
<span id="BS5_1"><strong>BS5.1</strong></span> <em>Return values should include appropriate metadata on
types (or
classes) and dimensions of input data</em>
</li>
</ul>
<p>The latter standard may also include returning a unique hash computed from the
input data, to enable results to be uniquely associated with that input data.
With regard to the input function, or alternative means of specifying prior
distributions:</p>
<ul>
<li>
<span id="BS5_2"><strong>BS5.2</strong></span> <em>Bayesian Software should either return the input
function or prior distributional specification in the return object; or
enable direct access to such via additional functions which accept the return
object as single argument.</em>
</li>
</ul>
<p>Where convergence checkers are implemented or provided:</p>
<ul>
<li>
<span id="BS5_3"><strong>BS5.3</strong></span> <em>Bayesian Software should return convergence statistics
or equivalent</em>
</li>
<li>
<span id="BS5_4"><strong>BS5.4</strong></span> <em>Where multiple checkers are enabled, Bayesian Software
should return details of convergence checker used</em>
</li>
<li>
<span id="BS5_5"><strong>BS5.5</strong></span> <em>Appropriate diagnostic statistics to indicate absence of
convergence should either be returned or immediately able to be accessed.</em>
</li>
</ul>
</div>
<div id="additional-functionality" class="section level3">
<h3>
<span class="header-section-number">6.2.6</span> Additional Functionality<a class="anchor" aria-label="anchor" href="#additional-functionality"><i class="fas fa-link"></i></a>
</h3>
<p>With regard to additional methods implemented for, or dispatched on, return
objects:</p>
<ul>
<li>
<span id="BS6_0"><strong>BS6.0</strong></span> <em>Software should implement a default <code>print</code> method for
return objects</em>
</li>
<li>
<span id="BS6_1"><strong>BS6.1</strong></span> <em>Software should implement a default <code>plot</code> method for
return
objects</em>
</li>
<li>
<span id="BS6_2"><strong>BS6.2</strong></span> <em>Software should provide and document straightforward
abilities to
plot sequences of posterior samples, with burn-in periods clearly
distinguished</em>
</li>
<li>
<span id="BS6_3"><strong>BS6.3</strong></span> <em>Software should provide and document straightforward
abilities to plot posterior distributional estimates</em>
</li>
</ul>
<p>Beyond these points:</p>
<ul>
<li>
<span id="BS6_4"><strong>BS6.4</strong></span> <em>Software may provide <code>summary</code> methods for return objects</em>
</li>
<li>
<span id="BS6_5"><strong>BS6.5</strong></span> <em>Software may provide abilities to plot both sequences of
posterior samples and distributional estimates together in single graphic</em>
</li>
</ul>
</div>
<div id="tests" class="section level3">
<h3>
<span class="header-section-number">6.2.7</span> Tests<a class="anchor" aria-label="anchor" href="#tests"><i class="fas fa-link"></i></a>
</h3>
<div id="parameter-recovery-tests" class="section level4">
<h4>
<span class="header-section-number">6.2.7.1</span> Parameter Recovery Tests<a class="anchor" aria-label="anchor" href="#parameter-recovery-tests"><i class="fas fa-link"></i></a>
</h4>
<p>Bayesian software should implement the following parameter recovery tests:</p>
<ul>
<li>
<span id="BS7_0"><strong>BS7.0</strong></span> <em>Software should demonstrate and confirm recovery of
parametric estimates of a prior distribution</em>
</li>
<li>
<span id="BS7_1"><strong>BS7.1</strong></span> <em>Software should demonstrate and confirm recovery of a
prior
distribution in the absence of any additional data or information</em>
</li>
<li>
<span id="BS7_2"><strong>BS7.2</strong></span> <em>Software should demonstrate and confirm recovery of a
expected posterior distribution given a specified prior and some input data</em>
</li>
</ul>
</div>
<div id="algorithmic-scaling-tests" class="section level4">
<h4>
<span class="header-section-number">6.2.7.2</span> Algorithmic Scaling Tests<a class="anchor" aria-label="anchor" href="#algorithmic-scaling-tests"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="BS7_3"><strong>BS7.3</strong></span> <em>Bayesian software should include tests which demonstrate
and confirm the scaling of algorithmic efficiency with sizes of input data.</em>
</li>
</ul>
<p>An example of adhering to this standard would be documentation or tests which
demonstrate or confirm that computation times increase approximately
logarithmically with increasing sizes of input data.</p>
</div>
<div id="scaling-of-input-to-output-data" class="section level4">
<h4>
<span class="header-section-number">6.2.7.3</span> Scaling of Input to Output Data<a class="anchor" aria-label="anchor" href="#scaling-of-input-to-output-data"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="BS7_4"><strong>BS7.4</strong></span> <em>Bayesian software should implement tests which confirm
that predicted or fitted values are on (approximately) the same scale as
input values.</em>
<ul>
<li>
<span id="BS7_4a"><strong>BS7.4a</strong></span> <em>The implications of any assumptions on scales on
input objects should be explicitly tested in this context; for example
that the scales of inputs which do not have means of zero will not be
able to be recovered.</em>
</li>
</ul>
</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="exploratory-data-analysis" class="section level2">
<h2>
<span class="header-section-number">6.3</span> Exploratory Data Analysis<a class="anchor" aria-label="anchor" href="#exploratory-data-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>Exploration is a part of all data analyses, and Exploratory Data Analysis (EDA)
is not something that is entered into and exited from at some point prior to
“real” analysis. Exploratory Analyses are also not strictly limited to <em>Data</em>,
but may extend to exploration of <em>Models</em> of those data. The category could
thus equally be termed, “<em>Exploratory Data and Model Analysis</em>”, yet we opt to
utilise the standard acronym of EDA in this document.</p>
<p>EDA is nevertheless somewhat different to many other categories included within
rOpenSci’s program for peer-reviewing statistical software. Primary differences include:</p>
<ul>
<li>EDA software often has a strong focus upon visualization, which is a category
which we have otherwise explicitly excluded from the scope of the project at
the present stage.</li>
<li>The assessment of EDA software requires addressing more general questions
than software in most other categories, notably including the important
question of intended audience(s).</li>
</ul>
<p>Examples of EDA software include:</p>
<ol style="list-style-type: decimal">
<li>A package rejected by rOpenSci as out-of-scope,
<a href="https://github.com/ddsjoberg/gtsummary"><code>gtsummary</code></a>, which provides,
“Presentation-ready data summary and analytic result tables.” Other
examples include:</li>
<li>The <a href="https://github.com/daya6489/SmartEDA"><code>smartEDA</code> package</a> (with
accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.01509">JOSS
paper</a>) “for automated
exploratory data analysis”. The package, “automatically selects the
variables and performs the related descriptive statistics. Moreover, it also
analyzes the information value, the weight of evidence, custom tables,
summary statistics, and performs graphical techniques for both numeric and
categorical variables.” This package is potentially as much a workflow
package as it is a statistical reporting package, and illustrates the
ambiguity between these two categories.</li>
<li>The <a href="https://github.com/ShanaScogin/modeLLtest"><code>modeLLtest</code> package</a> (with
accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.01542">JOSS
paper</a>) is “An R Package
for Unbiased Model Comparison using Cross Validation.” Its main
functionality allows different statistical models to be compared, likely
implying that this represents a kind of meta package.</li>
<li>The <a href="https://github.com/easystats/insight"><code>insight</code> package</a> (with
accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.01412">JOSS paper</a>
provides “a unified interface to access information from model objects in
R,” with a strong focus on unified and consistent reporting of statistical
results.</li>
<li>The <a href="https://github.com/arviz-devs/arviz"><code>arviz</code> software for python</a> (with
accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.01143">JOSS paper</a>
provides “a unified library for exploratory analysis of Bayesian models in
Python.”</li>
<li>The <a href="https://github.com/sumbose/iRF"><code>iRF</code> package</a> (with accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.01077">JOSS
paper</a> enables
“extracting interactions from random forests”, yet also focusses primarily
on enabling interpretation of random forests through reporting on
interaction terms.</li>
</ol>
<p>Click on the following link to view a demonstration <a href="https://hackmd.io/K8F1RIhdQeuZFqMnzdqNVw">Application of Exploratory
Data Analysis Standards</a>.</p>
<p>Reflecting these considerations, the following standards are somewhat
differently structured than equivalent standards developed to date for other
categories, particularly through being more qualitative and abstract. In
particular, while documentation is an important component of standards for all
categories, clear and instructive documentation is of paramount importance for
EDA Software, and so warrants its own sub-section within this document.</p>
<div id="documentation-standards" class="section level3">
<h3>
<span class="header-section-number">6.3.1</span> Documentation Standards<a class="anchor" aria-label="anchor" href="#documentation-standards"><i class="fas fa-link"></i></a>
</h3>
<p>The following refer to <em>Primary Documentation</em>, implying in main package
<code>README</code> or vignette(s), and <em>Secondary Documentation</em>, implying function-level
documentation.</p>
<p>The <em>Primary Documentation</em> (<code>README</code> and/or vignette(s)) of EDA software
should:</p>
<ul>
<li>
<span id="EA1_0"><strong>EA1.0</strong></span> <em>Identify one or more target audiences for whom the
software is intended</em>
</li>
<li>
<span id="EA1_1"><strong>EA1.1</strong></span> <em>Identify the kinds of data the software is capable of
analysing (see </em>Kinds of Data* below).*</li>
<li>
<span id="EA1_2"><strong>EA1.2</strong></span> <em>Identify the kinds of questions the software is intended
to help explore.</em>
</li>
</ul>
<p>Important distinctions between kinds of questions include whether they are
inferential, predictive, associative, causal, or representative of other modes
of statistical enquiry. The <em>Secondary Documentation</em> (within individual
functions) of EDA software should:</p>
<ul>
<li>
<span id="EA1_3"><strong>EA1.3</strong></span> <em>Identify the kinds of data each function is intended to
accept as input</em>
</li>
</ul>
</div>
<div id="input-data-1" class="section level3">
<h3>
<span class="header-section-number">6.3.2</span> Input Data<a class="anchor" aria-label="anchor" href="#input-data-1"><i class="fas fa-link"></i></a>
</h3>
<p>A further primary difference of EDA software from that of our other categories
is that input data for statistical software may be generally presumed of one or
more specific types, whereas EDA software often accepts data of more general
and varied types. EDA software should aim to accept and appropriately transform
as many diverse kinds of input data as possible, through addressing the
following standards, considered in terms of the two cases of input data in uni-
and multi-variate form. All of the general standards for kinds of input (G2.0 -
G2.12) apply to input data for EDA Software.</p>
<div id="index-columns" class="section level4">
<h4>
<span class="header-section-number">6.3.2.1</span> Index Columns<a class="anchor" aria-label="anchor" href="#index-columns"><i class="fas fa-link"></i></a>
</h4>
<p>The following standards refer to an <em>index column</em>, which is understood to
imply an explicitly named or identified column which can be used to provide a
unique index index into any and all rows of that table. Index columns ensure
the universal applicability of standard table join operations, such as those
implemented via the <a href="https://dplyr.tidyverse.org"><code>dplyr</code> package</a>.</p>
<ul>
<li>
<span id="EA2_0"><strong>EA2.0</strong></span> <em>EDA Software which accepts standard tabular data and
implements or relies upon extensive table filter and join operations should
utilise an <strong>index column</strong> system</em>
</li>
<li>
<span id="EA2_1"><strong>EA2.1</strong></span> <em>All values in an index column must be unique, and this
uniqueness should be affirmed as a pre-processing step for all input data.</em>
</li>
<li>
<span id="EA2_2"><strong>EA2.2</strong></span> <em>Index columns should be explicitly identified, either:</em>
<ul>
<li>
<span id="EA2_2a"><strong>EA2.2a</strong></span> <em>by using an appropriate class system, or</em>
</li>
<li>
<span id="EA2_2b"><strong>EA2.2b</strong></span> <em>through setting an <code>attribute</code> on a table, <code>x</code>, of
<code>attr(x, "index") &lt;- &lt;index_col_name&gt;</code>.</em>
</li>
</ul>
</li>
</ul>
<p>For EDA software which either implements custom classes or explicitly sets
attributes specifying index columns, these attributes should be used as the
basis of all table join operations, and in particular:</p>
<ul>
<li>
<span id="EA2_3"><strong>EA2.3</strong></span> <em>Table join operations should not be based on any assumed
variable or column names</em>
</li>
</ul>
</div>
<div id="multi-tabular-input" class="section level4">
<h4>
<span class="header-section-number">6.3.2.2</span> Multi-tabular input<a class="anchor" aria-label="anchor" href="#multi-tabular-input"><i class="fas fa-link"></i></a>
</h4>
<p>EDA software designed to accept multi-tabular input should:</p>
<ul>
<li>
<span id="EA2_4"><strong>EA2.4</strong></span> <em>Use and demand an explicit class system for such input
(for example, via the <a href="https://github.com/krlmlr/dm"><code>DM</code> package</a>).</em>
</li>
<li>
<span id="EA2_5"><strong>EA2.5</strong></span> <em>Ensure all individual tables follow the above standards
for Index Columns</em>
</li>
</ul>
</div>
<div id="classes-and-sub-classes" class="section level4">
<h4>
<span class="header-section-number">6.3.2.3</span> Classes and Sub-Classes<a class="anchor" aria-label="anchor" href="#classes-and-sub-classes"><i class="fas fa-link"></i></a>
</h4>
<p><em>Classes</em> are understood here to be the classes define single input objects,
while <em>Sub-Classes</em> refer to the class definitions of components of input
objects (for example, of columns of an input <code>data.frame</code>). EDA software which
is intended to receive input in general vector formats (see <em>Uni-variate Input</em>
section of <a href="standards.html#general-standards"><em>General Standards</em></a>) should ensure that it
complies with <strong>G2.</strong>, so that vector input is appropriately processed
regardless of input class. An additional standard for EDA software is that,</p>
<ul>
<li>
<span id="EA2_6"><strong>EA2.6</strong></span> <em>Routines should appropriately process vector data
regardless of additional attributes</em>
</li>
</ul>
<p>The following code illustrates some ways by which “metadata” defining classes
and additional attributes associated with a standard vector object may by
modified.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>
<span class="fu"><a href="https://rdrr.io/r/base/class.html">class</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"notvector"</span>
<span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span> <span class="op">(</span><span class="va">x</span>, <span class="st">"extra_attribute"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"another attribute"</span>
<span class="fu"><a href="https://rdrr.io/r/base/attr.html">attr</a></span> <span class="op">(</span><span class="va">x</span>, <span class="st">"vector attribute"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span> <span class="op">(</span><span class="fl">5</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/attributes.html">attributes</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="co">#&gt; $class</span>
<span class="co">#&gt; [1] "notvector"</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $extra_attribute</span>
<span class="co">#&gt; [1] "another attribute"</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; $`vector attribute`</span>
<span class="co">#&gt; [1] 0.03521663 0.49418081 0.60129563 0.75804346 0.16073301</span></code></pre></div>
<p>All statistical software should appropriately deal with such input
data, as exemplified by the <code><a href="https://rdrr.io/r/base/mode.html">storage.mode()</a></code>, <code><a href="https://rdrr.io/r/base/length.html">length()</a></code>, and <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code> functions
of the <code>base</code> package, which return the appropriate values regardless of
redefinition of class or additional attributes.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mode.html">storage.mode</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="co">#&gt; [1] "integer"</span>
<span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="co">#&gt; [1] 10</span>
<span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="co">#&gt; [1] 55</span>
<span class="fu"><a href="https://rdrr.io/r/base/mode.html">storage.mode</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] "integer"</span></code></pre></div>
<p>Tabular inputs in <code>data.frame</code> class may contain columns which are themselves
defined by custom classes, and which possess additional attributes. The ability
of software to accept such inputs is covered by the <em>Tabular Input</em> section of
the <a href="standards.html#general-standards"><em>General Standards</em></a>.</p>
</div>
</div>
<div id="analytic-algorithms-1" class="section level3">
<h3>
<span class="header-section-number">6.3.3</span> Analytic Algorithms<a class="anchor" aria-label="anchor" href="#analytic-algorithms-1"><i class="fas fa-link"></i></a>
</h3>
<p>EDA software will generally not directly implement what might be considered as
statistical algorithms in their own right. Where algorithms are implemented,
the following standards apply.</p>
<ul>
<li>
<span id="EA3_0"><strong>EA3.0</strong></span> <em>The algorithmic components of EDA Software should enable
automated extraction and/or reporting of statistics as some sufficiently
“meta” level (such as variable or model selection), for which previous or
reference implementations require manual intervention.</em>
</li>
<li>
<span id="EA3_1"><strong>EA3.1</strong></span> <em>EDA software should enable standardised comparison of
inputs, processes, models, or outputs which previous or reference
implementations otherwise only enable in some comparably unstandardised
form.</em>
</li>
</ul>
<p>Both of these standards also relate to the following standards for output
values, visualisation, and summary output.</p>
</div>
<div id="return-results-output-data" class="section level3">
<h3>
<span class="header-section-number">6.3.4</span> Return Results / Output Data<a class="anchor" aria-label="anchor" href="#return-results-output-data"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="EA4_0"><strong>EA4.0</strong></span> <em>EDA Software should ensure all return results have types
which are consistent with input types.</em>
</li>
</ul>
<p>Examples of such compliance include ensuring that <code>sum</code>, <code>min</code>, or <code>max</code> values
applied to <code>integer</code>-type vectors return <code>integer</code> values.</p>
<ul>
<li>
<span id="EA4_1"><strong>EA4.1</strong></span> <em>EDA Software should implement parameters to enable
explicit control of numeric precision</em>
</li>
<li>
<span id="EA4_2"><strong>EA4.2</strong></span> <em>The primary routines of EDA Software should return
objects for which default <code>print</code> and <code>plot</code> methods give sensible results.
Default <code>summary</code> methods may also be implemented.</em>
</li>
</ul>
</div>
<div id="visualization-and-summary-output" class="section level3">
<h3>
<span class="header-section-number">6.3.5</span> Visualization and Summary Output<a class="anchor" aria-label="anchor" href="#visualization-and-summary-output"><i class="fas fa-link"></i></a>
</h3>
<p>Visualization commonly represents one of the primary functions of EDA Software,
and thus visualization output is given greater consideration in this category
than in other categories in which visualization may nevertheless play an
important role. In particular, one component of this sub-category is <em>Summary
Output</em>, taken to refer to all forms of screen-based output beyond conventional
graphical output, including tabular and other text-based forms. Standards for
visualization itself are considered in the two primary sub-categories of static
and dynamic visualization, where the latter includes interactive visualization.</p>
<p>Prior to these individual sub-categories, we consider a few standards
applicable to visualization in general, whether static or dynamic.</p>
<ul>
<li>
<span id="EA5_0"><strong>EA5.0</strong></span> <em>Graphical presentation in EDA software should be as
accessible as possible or practicable. In particular, EDA software should
consider accessibility in terms of:</em>
<ul>
<li>
<span id="EA5_0a"><strong>EA5.0a</strong></span> <em>Typeface sizes, which should default to sizes
which explicitly enhance accessibility</em>
</li>
<li>
<span id="EA5_0b"><strong>EA5.0b</strong></span> <em>Default colour schemes, which should be carefully
constructed to ensure accessibility.</em>
</li>
</ul>
</li>
<li>
<span id="EA5_1"><strong>EA5.1</strong></span> <em>Any explicit specifications of typefaces which override
default values provided through other packages (including the <code>graphics</code>
package) should consider accessibility</em>
</li>
</ul>
<div id="summary-and-screen-based-output" class="section level4">
<h4>
<span class="header-section-number">6.3.5.1</span> Summary and Screen-based Output<a class="anchor" aria-label="anchor" href="#summary-and-screen-based-output"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="EA5_2"><strong>EA5.2</strong></span> <em>Screen-based output should never rely on default print
formatting of <code>numeric</code> types, rather should also use some version of
<code>round(., digits)</code>, <code>formatC</code>, <code>sprintf</code>, or similar functions for numeric
formatting according the parameter described in</em> <strong>EA4.1</strong>.</li>
<li>
<span id="EA5_3"><strong>EA5.3</strong></span> <em>Column-based summary statistics should always indicate
the <code>storage.mode</code>, <code>class</code>, or equivalent defining attribute of each
column.</em>
</li>
</ul>
<p>An example of compliance with the latter standard is the <code>print.tibble</code> method
of the <a href="https://tibble.tidyverse.org"><code>tibble</code> package</a>.</p>
</div>
<div id="general-standards-for-visualization-static-and-dynamic" class="section level4">
<h4>
<span class="header-section-number">6.3.5.2</span> General Standards for Visualization (Static and Dynamic)<a class="anchor" aria-label="anchor" href="#general-standards-for-visualization-static-and-dynamic"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="EA5_4"><strong>EA5.4</strong></span> <em>All visualisations should ensure values are rounded
sensibly (for example, via <code><a href="https://rdrr.io/r/base/pretty.html">pretty()</a></code> function).</em>
</li>
<li>
<span id="EA5_5"><strong>EA5.5</strong></span> <em>All visualisations should include units on all axes
where such are specified or otherwise obtainable from input data or other
routines.</em>
</li>
</ul>
</div>
<div id="dynamic-visualization" class="section level4">
<h4>
<span class="header-section-number">6.3.5.3</span> Dynamic Visualization<a class="anchor" aria-label="anchor" href="#dynamic-visualization"><i class="fas fa-link"></i></a>
</h4>
<p>Dynamic visualization routines are commonly implemented as interfaces to
<code>javascript</code> routines. Unless routines have been explicitly developed as an
internal part of an R package, standards shall not be considered to apply to
the code itself, rather only to decisions present as user-controlled parameters
exposed within the R environment. That said, one standard may nevertheless be
applied, which aims to maximise inter-operability between packages.</p>
<ul>
<li>
<span id="EA5_6"><strong>EA5.6</strong></span> <em>Any packages which internally bundle libraries used for
dynamic visualization and which are also bundled in other, pre-existing
R packages, should explain the necessity and advantage of re-bundling that
library.</em>
</li>
</ul>
</div>
</div>
<div id="testing-1" class="section level3">
<h3>
<span class="header-section-number">6.3.6</span> Testing<a class="anchor" aria-label="anchor" href="#testing-1"><i class="fas fa-link"></i></a>
</h3>
<div id="return-values-1" class="section level4">
<h4>
<span class="header-section-number">6.3.6.1</span> Return Values<a class="anchor" aria-label="anchor" href="#return-values-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="EA6_0"><strong>EA6.0</strong></span> <em>Return values from all functions should be tested,
including tests for the following characteristics:</em>
<ul>
<li>
<span id="EA6_0a"><strong>EA6.0a</strong></span> <em>Classes and types of objects</em>
</li>
<li>
<span id="EA6_0b"><strong>EA6.0b</strong></span> <em>Dimensions of tabular objects</em>
</li>
<li>
<span id="EA6_0c"><strong>EA6.0c</strong></span> <em>Column names (or equivalent) of tabular objects</em>
</li>
<li>
<span id="EA6_0d"><strong>EA6.0d</strong></span> <em>Classes or types of all columns contained within
<code>data.frame</code>-type tabular objects </em>
</li>
<li>
<span id="EA6_0e"><strong>EA6.0e</strong></span> <em>Values of single-valued objects; for <code>numeric</code>
values either using <code>testthat::expect_equal()</code> or equivalent with
a defined value for the <code>tolerance</code> parameter, or using <code>round(..., digits = x)</code> with some defined value of <code>x</code> prior to testing equality.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="graphical-output" class="section level4">
<h4>
<span class="header-section-number">6.3.6.2</span> Graphical Output<a class="anchor" aria-label="anchor" href="#graphical-output"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="EA6_1"><strong>EA6.1</strong></span> <em>The properties of graphical output from EDA software
should be explicitly tested, for example via the <a href="https://github.com/r-lib/vdiffr"><code>vdiffr</code>
package</a> or equivalent.</em>
</li>
</ul>
<p>Tests for graphical output are frequently only run as part of an extended test
suite.</p>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="ml-standards" class="section level2">
<h2>
<span class="header-section-number">6.4</span> Machine Learning Software<a class="anchor" aria-label="anchor" href="#ml-standards"><i class="fas fa-link"></i></a>
</h2>
<p>R has an extensive and diverse ecosystem of Machine Learning (ML) software
which is very well described in the corresponding <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN Task
View</a>. Unlike most
other categories of statistical software considered here, the primary
distinguishing feature of ML software is not (necessarily or directly)
algorithmic, rather pertains to a <em>workflow</em> typical of machine learning tasks.
In particular, we consider ML software to approach data analysis via the two
primary steps of:</p>
<ol style="list-style-type: decimal">
<li>Passing a set of <em>training</em> data to an algorithm in order to generate a
candidate mapping between that data and some form of pre-specified output
or response variable. Such mappings will be referred to here as “models”,
with a single analysis of a single set of training data generating one
model.</li>
<li>Passing a set of test data to the model(s) generated by the first step in
order to derive some measure of predictive accuracy for that model.</li>
</ol>
<p>A single ML task generally yields two distinct outputs:</p>
<ol style="list-style-type: decimal">
<li>The model derived in the first of the previous steps; and</li>
<li>Associated statistics of model performance, as evaluated within the context
of the test data used to assess that performance.</li>
</ol>
<p>Click on the following link to view a demonstration <a href="https://hackmd.io/Ix1YwD8YTWGuzdiXsVQadA">Application of Machine
Learning Software Standards</a>.</p>
<p><strong>A Machine Learning Workflow</strong></p>
<p>Given those initial considerations, we now attempt the difficult task of
envisioning a typical standard workflow for inherently diverse ML software. The
following workflow ought to be considered an “extensive” workflow, with shorter
versions, and correspondingly more restricted sets of standards, possible
dependent upon envisioned areas of application. For example, the workflow
presumes input data to be too large to be stored as a single entity in local
memory. Adaptation to situations in which all training data can be loaded into
memory may mean that some of the following workflow stages, and therefore
corresponding standards, may not apply.</p>
<p>Just as typical workflows are potentially very diverse, so are outputs of ML
software, which depend on areas of application and intended purpose of
software. The following refers to the “desired output” of ML software,
a phrase which is intentionally left non-specific, but which it intended to
connote any and all forms of “response variable” and other “pre-specified
outputs” such as categorical labels or validation data, along with outputs
which may not necessarily be able to be pre-specified in simple uni- or
multi-variate form, such as measures of distance between sets of training and
validation data.</p>
<p>Such “desired outputs” are presumed to be quantified in terms of a “loss” or
“cost” function (hereafter, simply “loss function”) quantifying some measure of
distance between a model estimate (resulting from applying the model to one or
more components of a training data set) and a pre-defined “valid” output
(during training), or a test data set (following training).</p>
<p>Given the foregoing considerations, we consider a typical ML workflow to
progress through (at least some of) the following steps:</p>
<ol style="list-style-type: decimal">
<li>
<strong><em>Input Data Specification</em></strong> Obtain a local copy of input data, often as
multiple <em>objects</em> (either on-disk or in memory) in some suitably structured
form such as in a series of sub-directories or accompanied by additional
data defining the structural properties of input objects. Regardless of
form, multiple objects are commonly given generic labels which distinguish
between <code>training</code> and <code>test</code> data, along with optional additional
categories and labels such as <code>validation</code> data used, for example, to
determine accuracy of models applied to training data yet prior to testing.</li>
<li>
<strong><em>Pre-Processing</em></strong> Define transformations of input data, including but not
restricted to, broadcasting dimensions (as defined below) and standardising
data ranges (typically to defined values of mean and standard deviation).</li>
<li>
<strong><em>Model and Algorithm Specification</em></strong> Specify the model and associated
processes which will be applied to map the input data on to the desired
output. This step minimally includes the following distinct stages
(generally in no particular order):
<ol style="list-style-type: lower-alpha">
<li>Specify the kind of model which will be applied to the training data. ML
software often allows the use of pre-trained models, in which case this
this step includes downloading or otherwise obtaining a pre-trained
model, along with specification of which aspects of those models are to
be modified through application to a particular set of training and
validation data.</li>
<li>Specify the kind of algorithm which will be used to explore the search
space (for example some kind of gradient descent algorithm), along with
parameters controlling how that algorithm will be applied (for example
a learning rate, as defined above).</li>
<li>Specify the kind of loss function will be used to quantify distance
between model estimates and desired output.</li>
</ol>
</li>
<li>
<strong><em>Model Training</em></strong> Apply the specified model to the training data to
generate a series of estimates from the specified loss function. This stage
may also include specifying parameters such as stopping or exit criteria,
and parameters controlling batch processing of input data. Moreover, this
stage may involve retaining some of the following additional data:
<ol style="list-style-type: lower-alpha">
<li>Potential “pre-processing” stages such as initial estimates of optimal
learning rates (see above).</li>
<li>Details of summaries of actual paths taken through the search space
towards convergence on local or global minimum.</li>
</ol>
</li>
<li>
<strong><em>Model Output and Performance</em></strong> Measure the performance of the trained
model when applied to the test data set, generally requiring the
specification of a metric of model performance or accuracy.</li>
</ol>
<p>Importantly, ML workflows may be partly iterative. This may in turn potentially
confound distinctions between training and test data, and accordingly confound
expectations commonly placed upon statistical analyses of statistical
independence of response variables. ML routines such as cross-validation
repeatedly (re-)partition data between training and test sets. Resultant models
can then not be considered to have been developed through application to any
single set of truly “independent” data. In the context of the standards that
follow, these considerations admit a potential lack of clarity in any notional
categorical distinction between training and test data, and between model
specification and training.</p>
<p>The preceding workflow mentioned a couple of concepts the interpretations of
which in the context of these standards may be seen by clicking on the
corresponding items below. Following that, we proceed to standards for ML
software, enumerated and developed with reference to the preceding workflow
steps. In order that the following standards initially adhere to the
enumeration of workflow steps given above, more general standards pertaining to
aspects such as documentation and testing are given following the initial five
“workflow” standards.</p>
<details><summary>
Click for a definition of <em>broadcasting</em>, referred to in Step 2, above.
</summary><p>
</p>
<p>The following definition comes from a vignette for the <a href="https://github.com/r-lib/rray"><code>rray</code>
package</a> named
<a href="https://rray.r-lib.org/articles/broadcasting.html"><em>Broadcasting</em></a>.</p>
<ul>
<li>
<strong><em>Broadcasting</em></strong> is, “repeating the dimensions of one object to match the
dimensions of another.”</li>
</ul>
<p>This concept runs counter to aspects of standards in other categories, which
often suggest that functions should error when passed input objects which do
not have commensurate dimensions. Broadcasting is a pre-processing step which
enables objects with incommensurate dimensions to be dimensionally reconciled.</p>
<p>The following demonstration is taken directly from the <a href="https://github.com/r-lib/rray"><code>rray</code>
package</a> (which is not currently on CRAN).</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span> <span class="op">(</span><span class="va">rray</span><span class="op">)</span>
<span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
<span class="va">b</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/array.html">array</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span><span class="op">)</span>, dim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="co"># rbind (a, b) # error!</span>
<span class="fu">rray_bind</span> <span class="op">(</span><span class="va">a</span>, <span class="va">b</span>, .axis <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>
<span class="co">#&gt;      [,1] [,2]</span>
<span class="co">#&gt; [1,]    1    1</span>
<span class="co">#&gt; [2,]    2    2</span>
<span class="co">#&gt; [3,]    3    4</span>
<span class="fu">rray_bind</span> <span class="op">(</span><span class="va">a</span>, <span class="va">b</span>, .axis <span class="op">=</span> <span class="fl">2</span><span class="op">)</span>
<span class="co">#&gt;      [,1] [,2] [,3]</span>
<span class="co">#&gt; [1,]    1    3    4</span>
<span class="co">#&gt; [2,]    2    3    4</span></code></pre></div>
<p>Broadcasting is commonly employed in ML software because it enables ML
operations to be implemented on objects with incommensurate dimensions.
One example is image analysis, in which training data may all be dimensionally
commensurate, yet test images may have different dimensions. Broadcasting
allows data to be submitted to ML routines regardless of potentially
incommensurate dimensions.</p>

</details><details><summary>
Click for a definition of <em>learning rate</em>, referred to in Step 5, above.
</summary><p>
</p>
<ul>
<li>
<strong><em>Learning Rate</em></strong> (generally) determines the step size used to search for
local optima as a fraction of the local gradient.</li>
</ul>
<p>This parameter is particularly important for training ML algorithms like
neural networks, the results of which can be very sensitive to
variations in learning rates. A useful overview of the importance of
learning rates, and a useful approach to automatically determining
appropriate values, is given in <a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html">this blog
post</a>.</p>

</details><p><br></p>
<p>Partly because of widespread and current relevance, the category of Machine
Learning software is one for which there have been other notable attempts to
develop standards. A particularly useful reference is the <a href="https://www.mlperf.org/">MLPerf
organization</a> which, among other activities, hosts
several <a href="https://github.com/mlperf">github repositories</a> providing reference
datasets and benchmark conditions for comparing performance aspects of ML
software. While such reference or benchmark standards are not explicitly
referred to in the current version of the following standards, we expect them
to be gradually adapted and incorporated as we start to apply and refine our
standards in application to software submitted to our review system.</p>
<div id="input-data-specification" class="section level3">
<h3>
<span class="header-section-number">6.4.1</span> Input Data Specification<a class="anchor" aria-label="anchor" href="#input-data-specification"><i class="fas fa-link"></i></a>
</h3>
<p>Many of the following standards refer to the labelling of input data as
“testing” or “training” data, along with potentially additional labels such as
“validation” data. In regard to such labelling, the following two standards apply,</p>
<ul>
<li>
<span id="ML1_0"><strong>ML1.0</strong></span> <em>Documentation should make a clear conceptual distinction
between training and test data (even where such may ultimately be confounded
as described above.)</em>
<ul>
<li>
<span id="ML1_0a"><strong>ML1.0a</strong></span> <em>Where these terms are ultimately eschewed, these
should nevertheless be used in initial documentation, along with clear
explanation of, and justification for, alternative terminology.</em>
</li>
</ul>
</li>
<li>
<span id="ML1_1"><strong>ML1.1</strong></span> <em>Absent clear justification for alternative design
decisions, input data should be expected to be labelled “test”, “training”,
and, where applicable, “validation” data.</em>
<ul>
<li>
<span id="ML1_1a"><strong>ML1.1a</strong></span> <em>The presence and use of these labels should be
explicitly confirmed via pre-processing steps (and tested in accordance
with <strong>ML7.0</strong>, below).</em>
</li>
<li>
<span id="ML1_1b"><strong>ML1.1b</strong></span> <em>Matches to expected labels should be
case-insensitive and based on partial matching such that, for example,
“Test”, “test”, or “testing” should all suffice.</em>
</li>
</ul>
</li>
</ul>
<p>The following three standards (<strong>ML1.2</strong>–<strong>ML1.4</strong>) represent three possible
design intentions for ML software. Only one of these three will generally be
applicable to any one piece of software, although it is nevertheless possible
that more than one of these standards may apply. The first of these three
standards applies to ML software which is intended to process, or capable of
processing, input data as a single (generally tabular) object.</p>
<ul>
<li>
<span id="ML1_2"><strong>ML1.2</strong></span> <em>Training and test data sets for ML software should be
able to be input as a single, generally tabular, data object, with the
training and test data distinguished either by</em>
<ul>
<li><em>A specified variable containing, for example, <code>TRUE</code>/<code>FALSE</code> or <code>0</code>/<code>1</code>
values, or which uses some other system such as missing (<code>NA</code>) values to
denote test data); and/or</em></li>
<li><em>An additional parameter designating case or row numbers, or labels of
test data.</em></li>
</ul>
</li>
</ul>
<p>The second of these three standards applies to ML software which is intended to
process, or capable of processing, input data represented as multiple objects
which exist in local memory.</p>
<ul>
<li>
<span id="ML1_3"><strong>ML1.3</strong></span> <em>Input data should be clearly partitioned between
training and test data (for example, through having each passed as a distinct
<code>list</code> item), or should enable an additional means of categorically
distinguishing training from test data (such as via an additional parameter
which provides explicit labels). Where applicable, distinction of validation
and any other data should also accord with this standard.</em>
</li>
</ul>
<p>The third of these three standards for data input applies to ML software for
which data are expected to be input as references to multiple external objects,
generally expected to be read from either local or remote connections.</p>
<ul>
<li>
<span id="ML1_4"><strong>ML1.4</strong></span> <em>Training and test data sets, along with other necessary
components such as validation data sets, should be stored in their own
distinctly labelled sub-directories (for distinct files), or according to an
explicit and distinct labelling scheme (for example, for database
connections). Labelling should in all cases adhere to <strong>ML1.1</strong>, above.</em>
</li>
</ul>
<p>The following standard applies to all ML software regardless of the
applicability or otherwise of the preceding three standards.</p>
<ul>
<li>
<span id="ML1_5"><strong>ML1.5</strong></span> <em>ML software should implement a single function which
summarises the contents of test and training (and other) data sets, minimally
including counts of numbers of cases, records, or files, and potentially
extending to tables or summaries of file or data types, sizes, and other
information (such as unique hashes for each component).</em>
</li>
</ul>
<div id="missing-values-1" class="section level4">
<h4>
<span class="header-section-number">6.4.1.1</span> Missing Values<a class="anchor" aria-label="anchor" href="#missing-values-1"><i class="fas fa-link"></i></a>
</h4>
<p>Missing data are handled differently by different ML routines, and it is also
difficult to suggest generally applicable standards for pre-processing missing
values in ML software. The <a href="standards.html#general-standards"><em>General Standards</em></a> for
missing values (<strong>G2.13</strong>–<strong>G2.16</strong>) do not apply to Machine Learning
software, in the place of which the following standards attempt to cover
a practical range of typical approaches and applications.</p>
<ul>
<li>
<span id="ML1_6"><strong>ML1.6</strong></span> <em>ML software which does not admit missing values, and
which expects no missing values, should implement explicit pre-processing
routines to identify whether data has any missing values, and should
generally error appropriately and informatively when passed data with missing
values. In addition, ML software which does not admit missing values should:</em>
<ul>
<li>
<span id="ML1_6a"><strong>ML1.6a</strong></span> <em>Explain why missing values are not admitted.</em>
</li>
<li>
<span id="ML1_6b"><strong>ML1.6b</strong></span> <em>Provide explicit examples (in function
documentation, vignettes, or both) for how missing values may be imputed,
rather than simply discarded.</em>
</li>
</ul>
</li>
<li>
<span id="ML1_7"><strong>ML1.7</strong></span> <em>ML software which admits missing values should clearly
document how such values are processed.</em>
<ul>
<li>
<span id="ML1_7a"><strong>ML1.7a</strong></span> <em>Where missing values are imputed, software should
offer multiple user-defined ways to impute missing data.</em>
</li>
<li>
<span id="ML1_7b"><strong>ML1.7b</strong></span> <em>Where missing values are imputed, the precise
imputation steps should also be explicitly documented, either in tests
(see <strong>ML7.2</strong> below), function documentation, or vignettes.</em>
</li>
</ul>
</li>
<li>
<span id="ML1_8"><strong>ML1.8</strong></span> <em>ML software should enable equal treatment of missing
values for both training and test data, with optional user ability to control
application to either one or both.</em>
</li>
</ul>
</div>
</div>
<div id="pre-processing" class="section level3">
<h3>
<span class="header-section-number">6.4.2</span> Pre-processing<a class="anchor" aria-label="anchor" href="#pre-processing"><i class="fas fa-link"></i></a>
</h3>
<p>As reflected in the workflow envisioned at the outset, ML software operates
somewhat differently to statistical software in many other categories. In
particular, ML software often requires explicit specification of a workflow,
including specification of input data (as per the standards of the preceding
sub-section), and of both transformations and statistical models to be applied
to those data. This section of standards refers exclusively to the
transformation of input data as a pre-processing step prior to any
specification of, or submission to, actual models.</p>
<ul>
<li>
<span id="ML2_0"><strong>ML2.0</strong></span> <em>A dedicated function should enable pre-processing steps
to be defined and parametrized.</em>
<ul>
<li>
<span id="ML2_0a"><strong>ML2.0a</strong></span> <em>That function should return an object which can be
directly submitted to a specified model (see section 3, below).</em>
</li>
<li>
<span id="ML2_0b"><strong>ML2.0b</strong></span> <em>Absent explicit justification otherwise, that
return object should have a defined class minimally intended to implement
a default <code>print</code> method which summarizes the input data set (as per
<strong>ML1.5</strong> above) and associated transformations (see the following
standard).</em>
</li>
</ul>
</li>
</ul>
<p>Standards for most other categories of statistical software suggest that
pre-processing routines should ensure that input data sets are commensurate,
for example, through having equal numbers of cases or rows. In contrast, ML
software is commonly intended to accept input data which can not be guaranteed
to be dimensionally commensurate, such as software intended to process
rectangular image files which may be of different sizes.</p>
<ul>
<li>
<span id="ML2_1"><strong>ML2.1</strong></span> <em>ML software which uses broadcasting to reconcile
dimensionally incommensurate input data should offer an ability to at least
optionally record transformations applied to each input file.</em>
</li>
</ul>
<p>Beyond broadcasting and dimensional transformations, the following standards
apply to the pre-processing stages of ML software.</p>
<ul>
<li>
<span id="ML2_2"><strong>ML2.2</strong></span> <em>ML software which requires or relies upon numeric
transformations of input data (such as change in mean values or variances)
should allow optimal explicit specification of target values, rather than
restricting transformations to default generic values only (such as
transformations to z-scores).</em>
<ul>
<li>
<span id="ML2_2a"><strong>ML2.2a</strong></span> <em>Where the parameters have default values, reasons
for those particular defaults should be explicitly described.</em>
</li>
<li>
<span id="ML2_2b"><strong>ML2.2b</strong></span> <em>Any extended documentation (such as vignettes)
which demonstrates the use of explicit values for numeric transformations
should explicitly describe why particular values are used.</em>
</li>
</ul>
</li>
</ul>
<p>For all transformations applied to input data, whether of dimension (<strong>ML2.1</strong>)
or scale (<strong>ML2.2</strong>),</p>
<ul>
<li>
<span id="ML2_3"><strong>ML2.3</strong></span> <em>The values associated with all transformations should be
recorded in the object returned by the function described in the preceding
standard (<strong>ML2.0</strong>).</em>
</li>
<li>
<span id="ML2_4"><strong>ML2.4</strong></span> <em>Default values of all transformations should be
explicitly documented, both in documentation of parameters where appropriate
(such as for numeric transformations), and in extended documentation such as
vignettes.</em>
</li>
<li>
<span id="ML2_5"><strong>ML2.5</strong></span> <em>ML software should provide options to bypass or
otherwise switch off all default transformations.</em>
</li>
<li>
<span id="ML2_6"><strong>ML2.6</strong></span> <em>Where transformations are implemented via distinct
functions, these should be exported to a package’s namespace so they can be
applied in other contexts.</em>
</li>
<li>
<span id="ML2_7"><strong>ML2.7</strong></span> <em>Where possible, documentation should be provided for how
transformations may be reversed. For example, documentation may demonstrate
how the values retained via <strong>ML2.3</strong>, above, can be used along with
transformations either exported via <strong>ML2.6</strong> or otherwise exemplified in
demonstration code to independently transform data, and then to reverse those
transformations.</em>
</li>
</ul>
</div>
<div id="model-and-algorithm-specification" class="section level3">
<h3>
<span class="header-section-number">6.4.3</span> Model and Algorithm Specification<a class="anchor" aria-label="anchor" href="#model-and-algorithm-specification"><i class="fas fa-link"></i></a>
</h3>
<p>A “model” in the context of ML software is understood to be a means of
specifying a mapping between input and output data, generally applied to
training and validation data. Model specification is the step of specifying
<em>how</em> such a mapping is to be constructed. The specification of <em>what</em> the
values of such a model actually are occurs through training the model, and is
described in the following sub-section. These standards also refer to <em>control
parameters</em> which specify how models are trained. These parameters commonly
include values specifying numbers of iterations, training rates, and parameters
controlling algorithmic processes such as re-sampling or cross-validation.</p>
<ul>
<li>
<span id="ML3_0"><strong>ML3.0</strong></span> <em>Model specification should be implemented as a distinct
stage subsequent to specification of pre-processing routines (see Section 2,
above) and prior to actual model fitting or training (see Section 4, below).
In particular,</em>
<ul>
<li>
<span id="ML3_0a"><strong>ML3.0a</strong></span> <em>A dedicated function should enable models to be
specified without actually fitting or training them, or if this (<strong>ML3</strong>)
and the following (<strong>ML4</strong>) stages are controlled by a single function,
that function should have a parameter enabling models to be specified yet
not fitted (for example, <code>nofit = FALSE</code>).</em>
</li>
<li>
<span id="ML3_0b"><strong>ML3.0b</strong></span> <em>That function should accept as input the objects
produced by the previous Input Data Specification stage, and defined
according to <strong>ML2.0</strong>, above.</em>
</li>
<li>
<span id="ML3_0c"><strong>ML3.0c</strong></span> <em>The function described above (<strong>ML3.0a</strong>) should
return an object which can be directly trained as described in the
following sub-section (<strong>ML4</strong>).</em>
</li>
<li>
<span id="ML3_0d"><strong>ML3.0d</strong></span> <em>That return object should have a defined class
minimally intended to implement a default <code>print</code> method which summarises
the model specification, including values of all relevant parameters.</em>
</li>
</ul>
</li>
<li>
<span id="ML3_1"><strong>ML3.1</strong></span> <em>ML software should allow the use of both untrained
models, specified through model parameters only, as well as pre-trained
models. Use of the latter commonly entails an ability to submit
a previously-trained model object to the function defined according to
<strong>ML3.0a</strong>, above.</em>
</li>
<li>
<span id="ML3_2"><strong>ML3.2</strong></span> <em>ML software should enable different models to be applied
to the object specifying data inputs and transformations (see sub-sections
1–2, above) without needing to re-define those preceding steps.</em>
</li>
</ul>
<p>A function fulfilling <strong>ML3.0–3.2</strong> might, for example, permit the following
arguments:</p>
<ol style="list-style-type: decimal">
<li>
<code>data</code>: Input data specification constructed according to <strong>ML1</strong>
</li>
<li>
<code>model</code>: An optional previously-trained model</li>
<li>
<code>control</code>: A list of parameters controlling how the model algorithm is to be
applied during the subsequent training phase (<strong>ML4</strong>).</li>
</ol>
<p>A function with the arguments defined above would fulfil the preceding three
standards, because the <code>data</code> stage would represent the output of <strong>ML1</strong>,
while the <code>model</code> stage would allow for different pre-trained models to be
submitted using the same data and associated specifications (<strong>ML3.1</strong>). The
provision of a separate <code>.data</code> argument would fulfil <strong>ML3.2</strong> by allowing one
or both <code>model</code> or <code>control</code> parameters to be re-defined while submitting the
same <code>data</code> object.</p>
<ul>
<li>
<span id="ML3_3"><strong>ML3.3</strong></span> <em>Where ML software implements its own distinct classes of
model objects, the properties and behaviours of those specific classes of
objects should be explicitly compared with objects produced by other ML
software. In particular, where possible, ML software should provide extended
documentation (as vignettes or equivalent) comparing model objects with those
from other ML software, noting both unique abilities and restrictions of any
implemented classes.</em>
</li>
<li>
<span id="ML3_4"><strong>ML3.4</strong></span> <em>Where training rates are used, ML software should
provide explicit documentation both in all functions which use training
rates, and in extended form such as vignettes, of the importance of, and/or
sensitivity to, different values of training rates. In particular,</em>
<ul>
<li>
<span id="ML3_4a"><strong>ML3.4a</strong></span> <em>Unless explicitly justified otherwise, ML software
should offer abilities to automatically determine appropriate or optimal
training rates, either as distinct pre-processing stages, or as implicit
stages of model training.</em>
</li>
<li>
<span id="ML3_4b"><strong>ML3.4b</strong></span> <em>ML software which provides default values for
training rates should clearly document anticipated restrictions of
validity of those default values; for example through clear suggestions
that user-determined and -specified values may generally be necessary or
preferable.</em>
</li>
</ul>
</li>
</ul>
<div id="control-parameters" class="section level4">
<h4>
<span class="header-section-number">6.4.3.1</span> Control Parameters<a class="anchor" aria-label="anchor" href="#control-parameters"><i class="fas fa-link"></i></a>
</h4>
<p>Control parameters are considered here to specify how a model is to be applied
to a set of training data. These are generally distinct from parameters
specifying the actual model (such as model architecture). While we recommend
that control parameters be submitted as items of a single named list, this is
neither a firm expectation nor an explicit part of the current standards.</p>
<ul>
<li>
<span id="ML3_5"><strong>ML3.5</strong></span> <em>Parameters controlling optimization algorithms should
minimally include:</em>
<ul>
<li>
<span id="ML3_5a"><strong>ML3.5a</strong></span> <em>Specification of the type of algorithm used to
explore the search space (commonly, for example, some kind of gradient
descent algorithm)</em>
</li>
<li>
<span id="ML3_5b"><strong>ML3.5b</strong></span> <em>The kind of loss function used to assess distance
between model estimates and desired output.</em>
</li>
</ul>
</li>
<li>
<span id="ML3_6"><strong>ML3.6</strong></span> <em>Unless explicitly justified otherwise (for example
because ML software under consideration is an implementation of one specific
algorithm), ML software should:</em>
<ul>
<li>
<span id="ML3_6a"><strong>ML3.6a</strong></span> <em>Implement or otherwise permit usage of multiple
ways of exploring search space</em>
</li>
<li>
<span id="ML3_6b"><strong>ML3.6b</strong></span> <em>Implement or otherwise permit usage of multiple
loss functions.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="cpu-and-gpu-processing" class="section level4">
<h4>
<span class="header-section-number">6.4.3.2</span> CPU and GPU processing<a class="anchor" aria-label="anchor" href="#cpu-and-gpu-processing"><i class="fas fa-link"></i></a>
</h4>
<p>ML software often involves manipulation of large numbers of rectangular arrays
for which graphics processing units (GPUs) are often more efficient than
central processing units (CPUs). ML software thus commonly offers options to
train models using either CPUs or GPUs. While these standards do not currently
suggest any particular design choice in this regard, we do note the following:</p>
<ul>
<li>
<span id="ML3_7"><strong>ML3.7</strong></span> <em>For ML software in which algorithms are coded in C++,
user-controlled use of either CPUs or GPUs (on NVIDIA processors at least)
should be implemented through direct use of
<a href="https://github.com/NVIDIA/libcudacxx"><code>libcudacxx</code></a>.</em>
</li>
</ul>
<p>This library can be “switched on” through activating a single C++ header file
to switch from CPU to GPU.</p>
</div>
</div>
<div id="model-training" class="section level3">
<h3>
<span class="header-section-number">6.4.4</span> Model Training<a class="anchor" aria-label="anchor" href="#model-training"><i class="fas fa-link"></i></a>
</h3>
<p>Model training is the stage of the ML workflow envisioned here in which the
actual computation is performed by applying a model specified according to
<strong>ML3</strong> to data specified according to <strong>ML1</strong> and <strong>ML2</strong>.</p>
<ul>
<li>
<span id="ML4_0"><strong>ML4.0</strong></span> <em>ML software should generally implement a unified
single-function interface to model training, able to receive as input a model
specified according to all preceding standards. In particular, models with
categorically different specifications, such as different model architectures
or optimization algorithms, should be able to be submitted to the same model
training function.</em>
</li>
<li>
<span id="ML4_1"><strong>ML4.1</strong></span> <em>ML software should at least optionally retain explicit
information on paths taken as an optimizer advances towards minimal loss.
Such information should minimally include:</em>
<ul>
<li>
<span id="ML4_1a"><strong>ML4.1a</strong></span> <em>Specification of all model-internal parameters, or
equivalent hashed representation.</em>
</li>
<li>
<span id="ML4_1b"><strong>ML4.1b</strong></span> <em>The value of the loss function at each point</em>
</li>
<li>
<span id="ML4_1c"><strong>ML4.1c</strong></span> <em>Information used to advance to next point, for
example quantification of local gradient.</em>
</li>
</ul>
</li>
<li>
<span id="ML4_2"><strong>ML4.2</strong></span> <em>The subsequent extraction of information retained
according to the preceding standard should be explicitly documented,
including through example code.</em>
</li>
</ul>
<div id="batch-processing" class="section level4">
<h4>
<span class="header-section-number">6.4.4.1</span> Batch Processing<a class="anchor" aria-label="anchor" href="#batch-processing"><i class="fas fa-link"></i></a>
</h4>
<p>The following standards apply to ML software which implements batch processing,
commonly to train models on data sets too large to be loaded in their entirety
into memory.</p>
<ul>
<li>
<span id="ML4_3"><strong>ML4.3</strong></span> <em>All parameters controlling batch processing and
associated terminology should be explicitly documented, and it should not,
for example, be presumed that users will understand the definition of “epoch”
as implemented in any particular ML software.</em>
</li>
</ul>
<p>According to that standard, it would for example be inappropriate to have
a parameter, <code>nepochs</code>, described as “Number of epochs used in model training”.
Rather, the definition and particular implementation of “epoch” must be
explicitly defined.</p>
<ul>
<li>
<span id="ML4_4"><strong>ML4.4</strong></span> <em>Explicit guidance should be provided on selection of
appropriate values for parameter controlling batch processing, for example,
on trade-offs between batch sizes and numbers of epochs (with both terms
provided as Control Parameters in accordance with the preceding standard,
<strong>ML3</strong>).</em>
</li>
<li>
<span id="ML4_5"><strong>ML4.5</strong></span> <em>ML software may optionally include a function to
estimate likely time to train a specified model, through estimating initial
timings from a small sample of the full batch.</em>
</li>
<li>
<span id="ML4_6"><strong>ML4.6</strong></span> <em>ML software should by default provide explicit
information on the progress of batch jobs (even where those jobs may be
implemented in parallel on GPUs). That information may be optionally
suppressed through additional parameters.</em>
</li>
</ul>
</div>
<div id="re-sampling" class="section level4">
<h4>
<span class="header-section-number">6.4.4.2</span> Re-sampling<a class="anchor" aria-label="anchor" href="#re-sampling"><i class="fas fa-link"></i></a>
</h4>
<p>As described at the outset, ML software does not always rely on pre-specified
and categorical distinctions between training and test data. For example,
models may be fit to what is effectively one single data set in which specified
cases or rows are used as training data, and the remainder as test data.
Re-sampling generally refers to the practice of re-defining categorical
distinctions between training and test data. One training run accordingly
connotes training a model on one particular set of training data and then
applying that model to the specified set of test data. Re-sampling starts that
process anew, through constructing an alternative categorical partition between
test and training data.</p>
<p>Even where test and training data are distinguished by more than a simple
data-internal category (such as a labelling column), for example,
by being stored in distinctly-named sub-directories, re-sampling may be
implemented by effectively shuffling data between training and test
sub-directories.</p>
<ul>
<li>
<span id="ML4_7"><strong>ML4.7</strong></span> <em>ML software should provide an ability to combine results
from multiple re-sampling iterations using a single parameter specifying
numbers of iterations.</em>
</li>
<li>
<span id="ML4_8"><strong>ML4.8</strong></span> <em>Absent any additional specification, re-sampling
algorithms should by default partition data according to proportions of
original test and training data.</em>
<ul>
<li>
<span id="ML4_8a"><strong>ML4.8a</strong></span> <em>Re-sampling routines of ML software should
nevertheless offer an ability to explicitly control or override such
default proportions of test and training data.</em>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="model-output-and-performance" class="section level3">
<h3>
<span class="header-section-number">6.4.5</span> Model Output and Performance<a class="anchor" aria-label="anchor" href="#model-output-and-performance"><i class="fas fa-link"></i></a>
</h3>
<p>Model output is considered here as a stage distinct from model performance.
Model output refers to the end result of model training (<strong>ML4</strong>), while model
performance involves the assessment of a trained model against a test data set.
The present section first describes standards for model output, which are
standards guiding the form of a model trained according to the preceding
standards (<strong>ML4</strong>). Model Performance is then considered as a separate stage.</p>
<div id="model-output" class="section level4">
<h4>
<span class="header-section-number">6.4.5.1</span> Model Output<a class="anchor" aria-label="anchor" href="#model-output"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="ML5_0"><strong>ML5.0</strong></span> <em>The result of applying the training processes described
above should be contained within a single model object returned by the
function defined according to <strong>ML4.0</strong>, above. Even where the output
reflects application to a test data set, the resultant object need not
include any information on model performance (see <strong>ML5.3</strong>–<strong>ML5.4</strong>,
below).</em>
<ul>
<li>
<span id="ML5_0a"><strong>ML5.0a</strong></span> <em>That object should either have its own class, or
extend some previously-defined class.</em>
</li>
<li>
<span id="ML5_0b"><strong>ML5.0b</strong></span> <em>That class should have a defined <code>print</code> method
which summarises important aspects of the model object, including but not
limited to summaries of input data and algorithmic control parameters.</em>
</li>
</ul>
</li>
<li>
<span id="ML5_1"><strong>ML5.1</strong></span> <em>As for the untrained model objects produced according to
the above standards, and in particular as a direct extension of <strong>ML3.3</strong>,
the properties and behaviours of trained models produced by ML software
should be explicitly compared with equivalent objects produced by other ML
software. (Such comparison will generally be done in terms of comparing model
performance, as described in the following standard <strong>ML5.3</strong>–<strong>ML5.4</strong>).</em>
</li>
<li>
<span id="ML5_2"><strong>ML5.2</strong></span> <em>The structure and functionality of objects representing
trained ML models should be thoroughly documented. In particular,</em>
<ul>
<li>
<span id="ML5_2a"><strong>ML5.2a</strong></span> <em>Either all functionality extending from the class
of model object should be explicitly documented, or a method for listing
or otherwise accessing all associated functionality explicitly documented
and demonstrated in example code.</em>
</li>
<li>
<span id="ML5_2b"><strong>ML5.2b</strong></span> <em>Documentation should include examples of how to
save and re-load trained model objects for their re-use in accordance
with <strong>ML3.1</strong>, above.</em>
</li>
<li>
<span id="ML5_2c"><strong>ML5.2c</strong></span> <em>Where general functions for saving or serializing
objects, such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/readRDS.html"><code>saveRDS</code></a>
are not appropriate for storing local copies of trained models, an
explicit function should be provided for that purpose, and should be
demonstrated with example code.</em>
</li>
</ul>
</li>
</ul>
<p>The <a href="https://r6.r-lib.org"><code>R6</code> system</a> for representing classes in R is an
example of a system with explicit functionality, all components of which are
accessible by a simple
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/ls.html"><code>ls()</code></a> call.
Adherence to <strong>ML5.2a</strong> would nevertheless
require explicit description of the ability of
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/ls.html"><code>ls()</code></a> to
supply a list of all functions associated with an object. The <a href="https://github.com/mlr-org/mlr3"><code>mlr</code>
package</a>, for example, uses <a href="https://r6.r-lib.org"><code>R6</code>
classes</a>, yet neither explicitly describes the use of
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/ls.html"><code>ls()</code></a> to
list all associated functions, nor explicitly lists those functions.</p>
</div>
<div id="model-performance" class="section level4">
<h4>
<span class="header-section-number">6.4.5.2</span> Model Performance<a class="anchor" aria-label="anchor" href="#model-performance"><i class="fas fa-link"></i></a>
</h4>
<p>Model performance refers to the quantitative assessment of a trained model when
applied to a set of test data.</p>
<ul>
<li>
<span id="ML5_3"><strong>ML5.3</strong></span> <em>Assessment of model performance should be implemented as
one or more functions distinct from model training.</em>
</li>
<li>
<span id="ML5_4"><strong>ML5.4</strong></span> <em>Model performance should be able to be assessed
according to a variety of metrics.</em>
<ul>
<li>
<span id="ML5_4a"><strong>ML5.4a</strong></span> <em>All model performance metrics represented by
functions internal to a package must be clearly and distinctly
documented.</em>
</li>
<li>
<span id="ML5_4b"><strong>ML5.4b</strong></span> <em>It should be possible to submit custom metrics to
a model assessment function, and the ability to do so should be clearly
documented including through example code.</em>
</li>
</ul>
</li>
</ul>
<p>The remaining sub-sections specify general standards beyond the preceding
workflow-specific ones.</p>
</div>
</div>
<div id="documentation-1" class="section level3">
<h3>
<span class="header-section-number">6.4.6</span> Documentation<a class="anchor" aria-label="anchor" href="#documentation-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="ML6_0"><strong>ML6.0</strong></span> <em>Descriptions of ML software should make explicit reference to
a workflow which separates training and testing stages, and which clearly
indicates a need for distinct training and test data sets.</em>
</li>
</ul>
<p>The following standard applies to packages which are intended or other able to
only encompass a restricted subset of the six primary workflow steps enumerated
at the outset. Envisioned here are packages explicitly intended to aid one
particular aspect of the general workflow envisioned here, such as
implementations of ML optimization functions, or specific loss measures.</p>
<ul>
<li>
<span id="ML6_1"><strong>ML6.1</strong></span> <em>ML software intentionally designed to address only
a restricted subset of the workflow described here should clearly document
how it can be embedded within a typical full ML workflow in the sense
considered here.</em>
<ul>
<li>
<span id="ML6_1a"><strong>ML6.1a</strong></span> <em>Such demonstrations should include and contrast
embedding within a full workflow using at least two other packages to
implement that workflow.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="testing-2" class="section level3">
<h3>
<span class="header-section-number">6.4.7</span> Testing<a class="anchor" aria-label="anchor" href="#testing-2"><i class="fas fa-link"></i></a>
</h3>
<div id="input-data-2" class="section level4">
<h4>
<span class="header-section-number">6.4.7.1</span> Input Data<a class="anchor" aria-label="anchor" href="#input-data-2"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="ML7_0"><strong>ML7.0</strong></span> <em>Test should explicitly confirm partial and
case-insensitive matching of “test”, “train”, and, where applicable,
“validation” data.</em>
</li>
<li>
<span id="ML7_1"><strong>ML7.1</strong></span> <em>Tests should demonstrate effects of different numeric
scaling of input data (see <strong>ML2.2</strong>).</em>
</li>
<li>
<span id="ML7_2"><strong>ML7.2</strong></span> <em>For software which imputes missing data, tests should
compare internal imputation with explicit code which directly implements
imputation steps (even where such imputation is a single-step implemented via
some external package). These tests serve as an explicit reference for how
imputation is performed.</em>
</li>
</ul>
</div>
<div id="model-classes" class="section level4">
<h4>
<span class="header-section-number">6.4.7.2</span> Model Classes<a class="anchor" aria-label="anchor" href="#model-classes"><i class="fas fa-link"></i></a>
</h4>
<p>The following standard applies to models in both untrained and trained forms,
considered to be the respective outputs of the preceding standards <strong>ML3</strong> and
<strong>ML4</strong>.</p>
<ul>
<li>
<span id="ML7_3"><strong>ML7.3</strong></span> <em>Where model objects are implemented as distinct classes,
tests should explicitly compare the functionality of these classes with
functionality of equivalent classes for ML model objects from other
packages.</em>
<ul>
<li>
<span id="ML7_3a"><strong>ML7.3a</strong></span> <em>These tests should explicitly identify
restrictions on the functionality of model objects in comparison with
those of other packages.</em>
</li>
<li>
<span id="ML7_3b"><strong>ML7.3b</strong></span> <em>These tests should explicitly identify functional
advantages and unique abilities of the model objects in comparison with
those of other packages.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="model-training-1" class="section level4">
<h4>
<span class="header-section-number">6.4.7.3</span> Model Training<a class="anchor" aria-label="anchor" href="#model-training-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="ML7_4"><strong>ML7.4</strong></span> <em>ML software should explicit document the effects of
different training rates, and in particular should demonstrate divergence
from optima with inappropriate training rates.</em>
</li>
<li>
<span id="ML7_5"><strong>ML7.5</strong></span> <em>ML software which implements routines to determine
optimal training rates (see <strong>ML3.4</strong>, above) should implement tests to
confirm the optimality of resultant values.</em>
</li>
<li>
<span id="ML7_6"><strong>ML7.6</strong></span> <em>ML software which implement independent training
“epochs” should demonstrate in tests the effects of lesser versus greater
numbers of epochs.</em>
</li>
<li>
<span id="ML7_7"><strong>ML7.7</strong></span> <em>ML software should explicitly test different
optimization algorithms, even where software is intended to implement one
specific algorithm.</em>
</li>
<li>
<span id="ML7_8"><strong>ML7.8</strong></span> <em>ML software should explicitly test different loss
functions, even where software is intended to implement one specific measure
of loss.</em>
</li>
<li>
<span id="ML7_9"><strong>ML7.9</strong></span> <em>Tests should explicitly compare all possible
combinations in categorical differences in model architecture, such as
different model architectures with same optimization algorithms, same model
architectures with different optimization algorithms, and differences in
both.</em>
<ul>
<li>
<span id="ML7_9a"><strong>ML7.9a</strong></span> <em>Such combinations will generally be formed from
multiple categorical factors, for which explicit use of functions such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/expand.grid.html"><code>expand.grid()</code></a>
is recommended.</em>
</li>
</ul>
</li>
</ul>
<p>The following example illustrates:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">architechture</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span> <span class="op">(</span><span class="st">"archA"</span>, <span class="st">"archB"</span><span class="op">)</span>
<span class="va">optimizers</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span> <span class="op">(</span><span class="st">"optA"</span>, <span class="st">"optB"</span>, <span class="st">"optC"</span><span class="op">)</span>
<span class="va">cost_fns</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span> <span class="op">(</span><span class="st">"costA"</span>, <span class="st">"costB"</span>, <span class="st">"costC"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span> <span class="op">(</span><span class="va">architechture</span>, <span class="va">optimizers</span>, <span class="va">cost_fns</span><span class="op">)</span></code></pre></div>
<pre><code>##     Var1 Var2  Var3
## 1  archA optA costA
## 2  archB optA costA
## 3  archA optB costA
## 4  archB optB costA
## 5  archA optC costA
## 6  archB optC costA
## 7  archA optA costB
## 8  archB optA costB
## 9  archA optB costB
## 10 archB optB costB
## 11 archA optC costB
## 12 archB optC costB
## 13 archA optA costC
## 14 archB optA costC
## 15 archA optB costC
## 16 archB optB costC
## 17 archA optC costC
## 18 archB optC costC</code></pre>
<p>All possible combinations of these categorical parameters could then be tested
by iterating over the rows of that output.</p>
<ul>
<li>
<span id="ML7_10"><strong>ML7.10</strong></span> <em>The successful extraction of information on paths
taken by optimizers (see <strong>ML5.1</strong>, above), should be tested, including
testing the general properties, but not necessarily actual values of, such
data.</em>
</li>
</ul>
</div>
<div id="model-performance-1" class="section level4">
<h4>
<span class="header-section-number">6.4.7.4</span> Model Performance<a class="anchor" aria-label="anchor" href="#model-performance-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="ML7_11"><strong>ML7.11</strong></span> <em>All performance metrics available for a given class of
trained model should be thoroughly tested and compared.</em>
<ul>
<li>
<span id="ML7_11a"><strong>ML7.11a</strong></span> <em>Tests which compare metrics should do so over
a range of inputs (generally implying differently trained models) to
demonstrate relative advantages and disadvantages of different metrics.</em>
</li>
</ul>
</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="standards-regression" class="section level2">
<h2>
<span class="header-section-number">6.5</span> Regression and Supervised Learning<a class="anchor" aria-label="anchor" href="#standards-regression"><i class="fas fa-link"></i></a>
</h2>
<p>This sub-section details standards for Regression and Supervised Learning
Software – referred to from here on for simplicity as “Regression Software”.
Regression Software implements algorithms which aim to construct or analyse one
or more mappings between two defined data sets (for example, a set of
“independent” data, <span class="math inline">\(X\)</span>, and a set of “dependent” data, <span class="math inline">\(Y\)</span>). In contrast, the
analogous category of Unsupervised Learning Software aims to construct or
analyse one or more mappings between a defined set of input or independent
data, and a second set of “output” data which are not necessarily known or
given prior to the analysis.</p>
<p>Common purposes of Regression Software are to fit models to estimate
relationships or to make predictions between specified inputs and outputs.
Regression Software includes tools with inferential or predictive foci,
Bayesian, frequentist, or probability-free Machine Learning (ML) approaches,
parametric or or non-parametric approaches, discrete outputs (such as in
classification tasks) or continuous outputs, and models and algorithms specific
to applications or data such as time series or spatial data. In many cases
other standards specific to these subcategories may apply.</p>
<p>Examples of the diversity of Regression and Unsupervised Learning software
include the following.</p>
<ol style="list-style-type: decimal">
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01761"><code>xrnet</code></a> to perform
“hierarchical regularized regression to incorporate external data”, where
“external data” in this case refers to structured meta-data as applied to
genomic features.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01434"><code>survPen</code></a> is, “an
R package for hazard and excess hazard modelling with multidimensional
penalized splines”</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01221"><code>areal</code></a> is, “an
R package for areal weighted interpolation”.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01287"><code>ChiRP</code></a> is a package
for “Chinese Restaurant Process mixtures for regression and clustering”,
which implements a class of non-parametric Bayesian Monte Carlo models.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.00722"><code>klrfome</code></a> is a package
for, “kernel logistic regression on focal mean embeddings,” with a specific
and exclusive application to the prediction of likely archaeological sites.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01038"><code>gravity</code></a> is a package
for “estimation methods for gravity models in R,” where “gravity models”
refers to models of spatial interactions between point locations based on
the properties of those locations.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.00967"><code>compboost</code></a> is an
example of an R package for gradient boosting, which is inherently
a regression-based technique, and so standards for regression software
ought to consider such applications.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.00937"><code>ungroup</code></a> is, “an
R package for efficient estimation of smooth distributions from coarsely
binned data.” As such, this package is an example of regression-based
software for which the input data are (effectively) categorical. The
package is primarily intended to implement a particular method for
“unbinning” the data, and so represents a particular class of
interpolation methods.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.00557"><code>registr</code></a> is
a package for “registration for exponential family functional data,” where
registration in this context is effectively an interpolation method
applied within a functional data analysis context.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.00772"><code>ggeffects</code></a> for “tidy
data frames of marginal effects from regression models.” This package aims
to make statistics quantifying marginal effects readily understandable, and
so implements a standard (tidyverse-based) methodology for representing and
visualising statistics relating to marginal effects.</li>
</ol>
<p>Click on the following link to view a demonstration <a href="https://hackmd.io/VZ-wgQtZRV2pb-wFZNDM5g">Application of Regression
and Supervised Learning Standards</a>.</p>
<p>The following standards are divided among several sub-categories, with each
standard prefixed with “RE”.</p>
<div id="input-data-structures-and-validation-1" class="section level3">
<h3>
<span class="header-section-number">6.5.1</span> Input data structures and validation<a class="anchor" aria-label="anchor" href="#input-data-structures-and-validation-1"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="RE1_0"><strong>RE1.0</strong></span> <em>Regression Software should enable models to be specified
via a formula interface, unless reasons for not doing so are explicitly
documented.</em>
</li>
<li>
<span id="RE1_1"><strong>RE1.1</strong></span> <em>Regression Software should document how formula
interfaces are converted to matrix representations of input data.</em>
</li>
</ul>
<p>See Max Kuhn’s <a href="https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/">RStudio blog
post</a>
for examples of how to implement and describe such conversions.</p>
<ul>
<li>
<span id="RE1_2"><strong>RE1.2</strong></span> <em>Regression Software should document expected format
(types or classes) for inputting predictor variables, including descriptions
of types or classes which are not accepted.</em>
</li>
</ul>
<p>Examples documentation addressing this standard include clarifying that
software accepts only numeric inputs in <code>vector</code> or <code>matrix</code> form, or that all
inputs must be in <code>data.frame</code> form with both column and row names.</p>
<ul>
<li>
<span id="RE1_3"><strong>RE1.3</strong></span> <em>Regression Software which passes or otherwise transforms
aspects of input data onto output structures should ensure that those output
structures retain all relevant aspects of input data, notably including row
and column names, and potentially information from other <code><a href="https://rdrr.io/r/base/attributes.html">attributes()</a></code>.</em>
<ul>
<li>
<span id="RE1_3a"><strong>RE1.3a</strong></span> <em>Where otherwise relevant information is not
transferred, this should be explicitly documented.</em>
</li>
</ul>
</li>
</ul>
<p>This standard reflects the common process in regression software of
transforming a rectangular input structure into a modified version which
includes additional columns of model fits or predictions. Software which
constructs such modified versions anew often copies numeric values from input
columns, and may implicitly drop additional information such as attributes.
This standard requires all such information to be retained.</p>
<ul>
<li>
<span id="RE1_4"><strong>RE1.4</strong></span> <em>Regression Software should document any assumptions made
with regard to input data; for example distributional assumptions, or
assumptions that predictor data have mean values of zero. Implications of
violations of these assumptions should be both documented and tested.</em>
</li>
</ul>
</div>
<div id="pre-processing-and-variable-transformation" class="section level3">
<h3>
<span class="header-section-number">6.5.2</span> Pre-processing and Variable Transformation<a class="anchor" aria-label="anchor" href="#pre-processing-and-variable-transformation"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="RE2_0"><strong>RE2.0</strong></span> <em>Regression Software should document any transformations
applied to input data, for example conversion of label-values to <code>factor</code>,
and should provide ways to explicitly avoid any default transformations (with
error or warning conditions where appropriate).</em>
</li>
<li>
<span id="RE2_1"><strong>RE2.1</strong></span> <em>Regression Software should implement explicit parameters
controlling the processing of missing values, ideally distinguishing <code>NA</code> or
<code>NaN</code> values from <code>Inf</code> values (for example, through use of <code><a href="https://rdrr.io/r/stats/na.fail.html">na.omit()</a></code> and
related functions from the <code>stats</code> package).</em>
</li>
</ul>
<p>Note that fulfilling this standard ensures compliance with all <em>General
Standard</em> for missing values (<strong>G2.13</strong>–<strong>G2.16</strong>).</p>
<ul>
<li>
<span id="RE2_2"><strong>RE2.2</strong></span> <em>Regression Software should provide different options for
processing missing values in predictor and response data. For example, it
should be possible to fit a model with no missing predictor data in order to
generate values for all associated response points, even where submitted
response values may be missing.</em>
</li>
<li>
<span id="RE2_3"><strong>RE2.3</strong></span> <em>Where applicable, Regression Software should enable data
to be centred (for example, through converting to zero-mean equivalent
values; or to z-scores) or offset (for example, to zero-intercept equivalent
values) via additional parameters, with the effects of any such parameters
clearly documented and tested.</em>
</li>
<li>
<span id="RE2_4"><strong>RE2.4</strong></span> <em>Regression Software should implement pre-processing
routines to identify whether aspects of input data are perfectly collinear,
notably including:</em>
<ul>
<li>
<span id="RE2_4a"><strong>RE2.4a</strong></span> <em>Perfect collinearity among predictor variables</em>
</li>
<li>
<span id="RE2_4b"><strong>RE2.4b</strong></span> <em>Perfect collinearity between independent and
dependent variables</em>
</li>
</ul>
</li>
</ul>
<p>These pre-processing routines should also be tested as described below.</p>
</div>
<div id="algorithms-1" class="section level3">
<h3>
<span class="header-section-number">6.5.3</span> Algorithms<a class="anchor" aria-label="anchor" href="#algorithms-1"><i class="fas fa-link"></i></a>
</h3>
<p>The following standards apply to the model fitting algorithms of Regression
Software which implement or rely on iterative algorithms which are expected
to converge to generate model statistics. Regression Software which implements
or relies on iterative convergence algorithms should:</p>
<ul>
<li>
<span id="RE3_0"><strong>RE3.0</strong></span> <em>Issue appropriate warnings or other diagnostic messages
for models which fail to converge.</em>
</li>
<li>
<span id="RE3_1"><strong>RE3.1</strong></span> <em>Enable such messages to be optionally suppressed, yet
should ensure that the resultant model object nevertheless includes
sufficient data to identify lack of convergence.</em>
</li>
<li>
<span id="RE3_2"><strong>RE3.2</strong></span> <em>Ensure that convergence thresholds have sensible default
values, demonstrated through explicit documentation.</em>
</li>
<li>
<span id="RE3_3"><strong>RE3.3</strong></span> <em>Allow explicit setting of convergence thresholds, unless
reasons against doing so are explicitly documented.</em>
</li>
</ul>
</div>
<div id="return-results" class="section level3">
<h3>
<span class="header-section-number">6.5.4</span> Return Results<a class="anchor" aria-label="anchor" href="#return-results"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="RE4_0"><strong>RE4.0</strong></span> <em>Regression Software should return some form of “model”
object, generally through using or modifying existing class structures for
model objects (such as <code>lm</code>, <code>glm</code>, or model objects from other packages), or
creating a new class of model objects.</em>
</li>
<li>
<span id="RE4_1"><strong>RE4.1</strong></span> <em>Regression Software may enable an ability to generate
a model object without actually fitting values. This may be useful for
controlling batch processing of computationally intensive fitting
algorithms.</em>
</li>
</ul>
<div id="accessor-methods" class="section level4">
<h4>
<span class="header-section-number">6.5.4.1</span> Accessor Methods<a class="anchor" aria-label="anchor" href="#accessor-methods"><i class="fas fa-link"></i></a>
</h4>
<p>Regression Software should provide functions to access or extract as much of
the following kinds of model data as possible or practicable. Access should
ideally rely on class-specific methods which extend, or implement otherwise
equivalent versions of, the methods from the <code>stats</code> package which are named in
parentheses in each of the following standards.</p>
<p>Model objects should include, or otherwise enable effectively immediate access
to the following descriptors. It is acknowledged that not all regression models
can sensibly provide access to these descriptors, yet should include access
provisions to all those that are applicable.</p>
<ul>
<li>
<span id="RE4_2"><strong>RE4.2</strong></span> <em>Model coefficients (via <code>coeff()</code> / <code><a href="https://rdrr.io/r/stats/coef.html">coefficients()</a></code>)</em>
</li>
<li>
<span id="RE4_3"><strong>RE4.3</strong></span> <em>Confidence intervals on those coefficients (via
<code><a href="https://rdrr.io/r/stats/confint.html">confint()</a></code>)</em>
</li>
<li>
<span id="RE4_4"><strong>RE4.4</strong></span> <em>The specification of the model, generally as a formula
(via <code><a href="https://rdrr.io/r/stats/formula.html">formula()</a></code>)</em>
</li>
<li>
<span id="RE4_5"><strong>RE4.5</strong></span> <em>Numbers of observations submitted to model (via
<code><a href="https://rdrr.io/r/stats/nobs.html">nobs()</a></code>)</em>
</li>
<li>
<span id="RE4_6"><strong>RE4.6</strong></span> <em>The variance-covariance matrix of the model parameters
(via <code><a href="https://rdrr.io/r/stats/vcov.html">vcov()</a></code>)</em>
</li>
<li>
<span id="RE4_7"><strong>RE4.7</strong></span> <em>Where appropriate, convergence statistics</em>
</li>
</ul>
<p>Note that compliance with <strong>RE4.6</strong> should also heed <em>General Standard</em>
<strong>G3.1</strong> in offering user control over covariance algorithms. Regression
Software should further provide simple and direct methods to return or
otherwise access the following form of data and metadata, where the latter
includes information on any transformations which may have been applied to the
data prior to submission to modelling routines.</p>
<ul>
<li>
<span id="RE4_8"><strong>RE4.8</strong></span> <em>Response variables, and associated “metadata” where
applicable.</em>
</li>
<li>
<span id="RE4_9"><strong>RE4.9</strong></span> <em>Modelled values of response variables.</em>
</li>
<li>
<span id="RE4_10"><strong>RE4.10</strong></span> <em>Model Residuals, including sufficient documentation to
enable interpretation of residuals, and to enable users to submit residuals
to their own tests.</em>
</li>
<li>
<span id="RE4_11"><strong>RE4.11</strong></span> <em>Goodness-of-fit and other statistics associated such
as effect sizes with model coefficients.</em>
</li>
<li>
<span id="RE4_12"><strong>RE4.12</strong></span> <em>Where appropriate, functions used to transform input
data, and associated inverse transform functions.</em>
</li>
</ul>
<p>Regression software may additionally opt to provide simple and direct methods
to return or otherwise access the following:</p>
<ul>
<li>
<span id="RE4_13"><strong>RE4.13</strong></span> <em>Predictor variables, and associated “metadata” where
applicable.</em>
</li>
</ul>
</div>
<div id="prediction-extrapolation-and-forecasting" class="section level4">
<h4>
<span class="header-section-number">6.5.4.2</span> Prediction, Extrapolation, and Forecasting<a class="anchor" aria-label="anchor" href="#prediction-extrapolation-and-forecasting"><i class="fas fa-link"></i></a>
</h4>
<p>Not all regression software is intended to, or can, provide distinct abilities
to extrapolate or forecast. Moreover, identifying cases in which a regression
model is used to extrapolate or forecast may often be a non-trivial exercise.
It may nevertheless be possible, for example when input data used to construct
a model are unidimensional, and data on which a prediction is to be based
extend beyond the range used to construct the model. Where reasonably
unambiguous identification of extrapolation or forecasting using a model is
possible, the following standards apply:</p>
<ul>
<li>
<span id="RE4_14"><strong>RE4.14</strong></span> <em>Where possible, values should also be provided for
extrapolation or forecast </em>errors<em>.</em>
</li>
<li>
<span id="RE4_15"><strong>RE4.15</strong></span> <em>Sufficient documentation and/or testing should be
provided to demonstrate that forecast errors, confidence intervals, or
equivalent values increase with forecast horizons.</em>
</li>
</ul>
<p>Distinct from extrapolation or forecasting abilities, the following standard
applies to regression software which relies on, or otherwise provides abilities
to process, categorical grouping variables:</p>
<ul>
<li>
<span id="RE4_16"><strong>RE4.16</strong></span> <em>Regression Software which models distinct responses
for different categorical groups should include the ability to submit new
groups to <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> methods.</em>
</li>
</ul>
</div>
<div id="reporting-return-results" class="section level4">
<h4>
<span class="header-section-number">6.5.4.3</span> Reporting Return Results<a class="anchor" aria-label="anchor" href="#reporting-return-results"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="RE4_17"><strong>RE4.17</strong></span> <em>Model objects returned by Regression Software should
implement or appropriately extend a default <code>print</code> method which provides an
on-screen summary of model (input) parameters and (output) coefficients.</em>
</li>
<li>
<span id="RE4_18"><strong>RE4.18</strong></span> <em>Regression Software may also implement <code>summary</code>
methods for model objects, and in particular should implement distinct
<code>summary</code> methods for any cases in which calculation of summary statistics is
computationally non-trivial (for example, for bootstrapped estimates of
confidence intervals).</em>
</li>
</ul>
</div>
</div>
<div id="documentation-2" class="section level3">
<h3>
<span class="header-section-number">6.5.5</span> Documentation<a class="anchor" aria-label="anchor" href="#documentation-2"><i class="fas fa-link"></i></a>
</h3>
<p>Beyond the <a href="standards.html#general-standards"><em>General Standards</em></a> for documentation,
Regression Software should explicitly describe the following aspects, and
ideally provide extended documentation including summary graphical reports of:</p>
<ul>
<li>
<span id="RE5_0"><strong>RE5.0</strong></span> <em>Scaling relationships between sizes of input data
(numbers of observations, with potential extension to numbers of
variables/columns) and speed of algorithm.</em>
</li>
</ul>
</div>
<div id="visualization" class="section level3">
<h3>
<span class="header-section-number">6.5.6</span> Visualization<a class="anchor" aria-label="anchor" href="#visualization"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="RE6_0"><strong>RE6.0</strong></span> <em>Model objects returned by Regression Software (see</em>
<strong>RE4</strong><em>) should have default <code>plot</code> methods, either through explicit
implementation, extension of methods for existing model objects, or through
ensuring default methods work appropriately.</em>
</li>
<li>
<span id="RE6_1"><strong>RE6.1</strong></span> <em>Where the default <code>plot</code> method is <strong>NOT</strong> a generic
<code>plot</code> method dispatched on the class of return objects (that is, through an
S3-type <code>plot.&lt;myclass&gt;</code> function or equivalent), that method dispatch (or
equivalent) should nevertheless exist in order to explicitly direct users to
the appropriate function.</em>
</li>
<li>
<span id="RE6_2"><strong>RE6.2</strong></span> <em>The default <code>plot</code> method should produce a plot of the
<code>fitted</code> values of the model, with optional visualisation of confidence
intervals or equivalent.</em>
</li>
</ul>
<p>The following standard applies only to software fulfilling RE4.14-4.15, and the
conditions described prior to those standards.</p>
<ul>
<li>
<span id="RE6_3"><strong>RE6.3</strong></span> <em>Where a model object is used to generate a forecast (for
example, through a <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> method), the default <code>plot</code> method should
provide clear visual distinction between modelled (interpolated) and forecast
(extrapolated) values.</em>
</li>
</ul>
</div>
<div id="testing-3" class="section level3">
<h3>
<span class="header-section-number">6.5.7</span> Testing<a class="anchor" aria-label="anchor" href="#testing-3"><i class="fas fa-link"></i></a>
</h3>
<div id="input-data-3" class="section level4">
<h4>
<span class="header-section-number">6.5.7.1</span> Input Data<a class="anchor" aria-label="anchor" href="#input-data-3"><i class="fas fa-link"></i></a>
</h4>
<p>Tests for Regression Software should include the following conditions and cases:</p>
<ul>
<li>
<span id="RE7_0"><strong>RE7.0</strong></span> <em>Tests with noiseless, exact relationships between
predictor (independent) data.</em>
<ul>
<li>
<span id="RE7_0a"><strong>RE7.0a</strong></span> In particular, these tests should confirm ability
to reject perfectly noiseless input data.</li>
</ul>
</li>
<li>
<span id="RE7_1"><strong>RE7.1</strong></span> <em>Tests with noiseless, exact relationships between
predictor (independent) and response (dependent) data.</em>
<ul>
<li>
<span id="RE7_1a"><strong>RE7.1a</strong></span> <em>In particular, these tests should confirm that
model fitting is at least as fast or (preferably) faster than testing
with equivalent noisy data (see RE2.4b).</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="return-results-1" class="section level4">
<h4>
<span class="header-section-number">6.5.7.2</span> Return Results<a class="anchor" aria-label="anchor" href="#return-results-1"><i class="fas fa-link"></i></a>
</h4>
<p>Tests for Regression Software should</p>
<ul>
<li>
<span id="RE7_2"><strong>RE7.2</strong></span> Demonstrate that output objects retain aspects of input
data such as row or case names (see <strong>RE1.3</strong>).</li>
<li>
<span id="RE7_3"><strong>RE7.3</strong></span> Demonstrate and test expected behaviour when objects
returned from regression software are submitted to the accessor methods of
<strong>RE4.2</strong>–<strong>RE4.7</strong>.</li>
<li>
<span id="RE7_4"><strong>RE7.4</strong></span> Extending directly from <strong>RE4.15</strong>, where appropriate,
tests should demonstrate and confirm that forecast errors, confidence
intervals, or equivalent values increase with forecast horizons.</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="spatial-software" class="section level2">
<h2>
<span class="header-section-number">6.6</span> Spatial Software<a class="anchor" aria-label="anchor" href="#spatial-software"><i class="fas fa-link"></i></a>
</h2>
<p>Standards for spatial software begin with a consideration and standardisation
of domains of applicability. Following that we proceed to standards according
to which spatial software is presumed to perform one or more of the following
steps:</p>
<ol style="list-style-type: decimal">
<li>Accept and validate input data</li>
<li>Apply one or more analytic algorithms</li>
<li>Return the result of that algorithmic application</li>
<li>Offer additional functionality such as printing or summarising return results</li>
<li>Testing</li>
</ol>
<p>Each standard for spatial software is prefixed with “<strong>SP</strong>”.</p>
<div id="spatial-domains" class="section level3">
<h3>
<span class="header-section-number">6.6.1</span> Spatial Domains<a class="anchor" aria-label="anchor" href="#spatial-domains"><i class="fas fa-link"></i></a>
</h3>
<p>Many developers of spatial software in R, including many of those those
featured on the CRAN Task view on <a href="https://cran.r-project.org/web/views/Spatial.html">“Analysis of Spatial
Data”</a>, have been primarily
focussed on geographic data; that is, data quantifying positions, structures,
and relationships on the Earth and other planets. Spatial analyses are
nevertheless both broader and more general than geography alone. In particular,
spatial software may be <em>geometric</em> – that is, concerned with
positions, structures, and relationships in space in any general or specific
sense, not necessarily confined to geographic systems alone.</p>
<p>It is important to distinguish these two domains because many algorithms and
procedures devised in one of these two domains are not necessarily (directly)
applicable in the other, most commonly because geometric algorithms presume
space to be rectilinear or Cartesian, while geographic algorithms (generally)
presume it be have a specific curvilinear form (commonly spherical or
elliptical). Algorithms designed for Cartesian space may not be directly
applicable in curvilinear space, and vice-versa.</p>
<p>Moreover, spatial software and algorithms might be intended to apply in spaces
of arbitrary dimensionality. The phrase “Cartesian” refers to any space of
arbitrary dimensionality in which all dimensions are orthogonal and described
by straight lines; dimensions in a curvilinear space or arbitrary
dimensionality are described by curved lines. A planar geometry is
a two-dimensional Cartesian space; a spherical geometry is a two- (or maybe
three-)dimensional curvilinear space.</p>
<p>One of the earliest and still most widely used R spatial packages,
<a href="https://cran.r-project.org/web/packages/spatstat/"><code>spatstat</code></a> (first released
2002), describes itself as, “[f]ocused mainly on two-dimensional point
patterns, including multitype/marked points, in any spatial region.” Routines
from this package are thus generally applicable to two-dimensional Cartesian
data only, even through the final phrase might be interpreted to indicate
a comprehensive generality. <code>spatstat</code> routines may not necessarily give
accurate results when applied in curvilinear space.</p>
<p>These considerations motivate the first standard for spatial software:</p>
<ul>
<li>
<span id="SP1_0"><strong>SP1.0</strong></span> <em>Spatial software should explicitly indicate its domain
of applicability, and in particular distinguish whether the software may be
applied in Cartesian/rectilinear/geometric domains, curvilinear/geographic
domains, or both.</em>
</li>
</ul>
<p>We encourage the use of clear and unambiguous phrases such as “planar”,
“spherical”, “Cartesian”, “rectilinear” or “curvilinear”, along with clear
indications of dimensionality such as “two-” or “three-dimensional.” Concepts
of dimensionality should be interpreted to refer explicitly to the
dimensionality of independent spatial coordinates. Elevation is a third spatial
dimension, and time may also be considered an additional dimension. Beyond
those two, other attributes measured at spatial locations do not represent
additional dimensions.</p>
<ul>
<li>
<span id="SP1_1"><strong>SP1.1</strong></span> <em>Spatial software should explicitly indicate its
dimensional domain of applicability, in particular through identifying
whether it is applicable to two or three dimensions only, or whether there
are any other restrictions on dimensionality.</em>
</li>
</ul>
<p>These considerations of domains of applicability permeate much of the ensuring
standards, which distinguish “geometric software” from “geographic software”,
where these phrases are to be interpreted as shorthand references to software
intended for use in the respective domains.</p>
</div>
<div id="input-data-structures-and-validation-2" class="section level3">
<h3>
<span class="header-section-number">6.6.2</span> Input data structures and validation<a class="anchor" aria-label="anchor" href="#input-data-structures-and-validation-2"><i class="fas fa-link"></i></a>
</h3>
<p>Input validation is an important software task, and an important part of our
standards. While there are many ways to approach validation, the class systems
of R offer a particularly convenient and effective means. For Spatial
Software in particular, a range of class systems have been developed, for which
we refer to the CRAN Task view on <a href="https://cran.r-project.org/web/views/Spatial.html">“Analysis of Spatial
Data”</a>. Software which uses
and relies on defined classes can often validate input through affirming
appropriate class(es). Software which does not use or rely on class systems
will generally need specific routines to validate input data structures.</p>
<p>As for our standards for <a href="https://ropenscilabs.github.io/statistical-software-review-book/standards.html#time-series-software">Time-Series
Software</a>,
these standards for Spatial Software also suggest that software should use
explicit class systems designed and intended for spatial data. New packages may
implement new class systems for spatial data, and these may even be as simple
as appending a class attribute to a matrix of coordinates. The primary
motivation of the following standard is nevertheless to encourage and enhance
inter-operability with the rich system of classes for spatial data in R.</p>
<ul>
<li>
<span id="SP2_0"><strong>SP2.0</strong></span> <em>Spatial software should only accept input data of one or
more classes explicitly developed to represent such data.</em>
<ul>
<li>
<span id="SP2_0a"><strong>SP2.0a</strong></span> <em>Where new classes are implemented, conversion to
other common classes for spatial data in R should be documented.</em>
</li>
<li>
<span id="SP2_0b"><strong>SP2.0b</strong></span> <em>Class systems should ensure that functions error
appropriately, rather than merely warning, in response to data from
inappropriate spatial domains.</em>
</li>
</ul>
</li>
</ul>
<p><strong>Spatial Workflows, Packages, and Classes</strong></p>
<p>Spatial software encompasses an enormous diversity, yet workflows implemented
by spatial software often share much in common. In particular, coordinate
reference systems used to precisely relate pairs of coordinates to precise
locations in a curvilinear space, and in particular to the Earth’s ellipsoid,
need to be able to be compared and transformed regardless of the specificities
of individual software. This ubiquitous need has fostered the development of
the <a href="https://proj.org/"><code>PROJ</code> library</a> for representing and transforming
spatial coordinates. Several other libraries have been built on top or or
alongside that, notably including the <a href="https://gdal.org"><code>GDAL</code> (“Geospatial Data Abstraction
Library”)</a> and <a href="https://trac.osgeo.org/geos/"><code>GEOS</code> (“Geometry Engine, Open
Source”)</a> libraries. These libraries are used by,
and integrated within, most geographical spatial software commonly used today,
and will likely continue to be used.</p>
<p>While not a standard in itself, it is expected that spatial software should
not, absent very convincing and explicit justification, attempt to reconstruct
aspects of these generic libraries. Given that, the following standards aim to
ensure that spatial software remains as compatible as possible with workflows
established by preceding packages which have aimed to expose and integrate as
much of the functionality of these generic libraries as possible. The use of
specific class systems for spatial data, and the workflows encapsulated in
associated packages, ensures maximal ongoing compatibility with these
libraries and with spatial workflows in general.</p>
<p>Notable class systems and associated packages in R include
<a href="https://cran.r-project.org/package=sp"><code>sp</code></a>,
<a href="https://cran.r-project.org/package=sf"><code>sf</code></a>, and
<a href="https://rspatial.org/raster/"><code>raster</code></a>, and more recent extensions such as
<a href="https://cran.r-project.org/package=stars"><code>stars</code></a>,
<a href="https://rspatial.org/terra"><code>terra</code></a>, and
<a href="https://r-spatial.github.io/s2/"><code>s2</code></a>. With regard to these packages, the
following single standard applies, because the maintainer of sp has made it
clear that new software <a href="https://github.com/r-spatial/discuss/issues/48#issuecomment-798543339">should build upon sf, not
sp</a>.</p>
<ul>
<li>
<span id="SP2_1"><strong>SP2.1</strong></span> <em>Spatial Software should not use the <a href="https://cran.r-project.org/package=sp"><code>sp</code>
package</a>, rather should use
<a href="https://cran.r-project.org/package=sf"><code>sf</code></a>.</em>
</li>
</ul>
<p>More generally,</p>
<ul>
<li>
<span id="SP2_2"><strong>SP2.2</strong></span> <em>Geographical Spatial Software should ensure maximal
compatibility with established packages and workflows, minimally through:</em>
<ul>
<li>
<span id="SP2_2a"><strong>SP2.2a</strong></span> <em>Clear and extensive documentation demonstrating
how routines from that software may be embedded within, or otherwise
adapted to, workflows which rely on these established packages; and</em>
</li>
<li>
<span id="SP2_2b"><strong>SP2.2b</strong></span> <em>Tests which clearly demonstrate that routines from
that software may be successfully translated into forms and workflows
which rely on these established packages.</em>
</li>
</ul>
</li>
</ul>
<p>This standard is further refined in a number of subsequent standards concerning
documentation and testing.</p>
<ul>
<li>
<span id="SP2_3"><strong>SP2.3</strong></span> <em>Software which accepts spatial input data in any
standard format established in other R packages (such as any of the formats
able to be read by <a href="https://gdal.org"><code>GDAL</code></a>, and therefore by the <a href="https://cran.r-project.org/package=sf"><code>sf</code>
package</a>) should include example and
test code which load those data in spatial formats, rather than R-specific
binary formats such as <code>.Rds</code>.</em>
</li>
</ul>
<p>See the <code>sf</code> vignette on <a href="https://cran.r-project.org/web/packages/sf/vignettes/sf2.html">“<em>Reading, Writing and Converting Simple
Features</em>”</a> for
useful examples.</p>
<p><strong>Coordinate Reference Systems</strong></p>
<p>As described above, one of the primary reasons for the development of classes
in Spatial Software is to represent the coordinate reference systems in which
data are represented, and to ensure compatibility with the <a href="https://proj.org/"><code>PROJ</code>
system</a> and other generic spatial libraries. The
<a href="https://proj.org/"><code>PROJ</code></a> standards and associated software library have been
recently (2020) updated (to version number 7) with “breaking changes” that are
not backwards-compatible with previous versions, and in particular with the
long-standing version 4. The details and implications of these changes within
the context of spatial software in R can be examined in <a href="https://www.r-spatial.org//r/2020/03/17/wkt.html">this blog
entry</a> on
<a href="https://r-spatial.org"><code>r-spatial.org</code></a>, and in <a href="https://cran.r-project.org/web/packages/rgdal/vignettes/PROJ6_GDAL3.html">this
vignette</a>
for the <a href="https://cran.r-project.org/web/packages/rgdal/"><code>rgdal</code> package</a>. The
“breaking” nature of these updates partly reflects analogous “breaking changes”
associated with updates in the <a href="http://docs.opengeospatial.org/is/12-063r5/12-063r5.html">“Well-Known Text”
(WKT)</a> system for
representing coordinate reference systems.</p>
<p>The following standard applies to software which directly or indirectly relies
on geographic data which uses or relies upon coordinate reference systems.</p>
<ul>
<li>
<span id="SP2_4"><strong>SP2.4</strong></span> <em>Geographical Spatial Software should be compliant with
version 6 or larger of</em> <a href="https://proj.org/"><code>PROJ</code></a>, <em>and with</em> <code>WKT2</code>
<em>representations. The primary implication, described in detail in the
articles linked to above, is that:</em>
<ul>
<li>
<span id="SP2_4a"><strong>SP2.4a</strong></span> <em>Software should not permit coordinate reference
systems to be represented merely by so-called “PROJ4-strings”, but should
use at least WKT2.</em>
</li>
</ul>
</li>
</ul>
<p><strong>General Input Structures</strong></p>
<p>New spatial software may nevertheless eschew these prior packages and classes
in favour of implementing new classes. Whether or not prior classes are used or
expected, geographic software should accord as much as possible with the
principles of these prior systems by according with the following standards:</p>
<ul>
<li>
<span id="SP2_5"><strong>SP2.5</strong></span> <em>Class systems for input data must contain meta data on
associated coordinate reference systems.</em>
<ul>
<li>
<span id="SP2_5a"><strong>SP2.5a</strong></span> <em>Software which implements new classes to input
spatial data (or the spatial components of more general data) should
provide an ability to convert such input objects into alternative spatial
classes such as those listed above.</em>
</li>
</ul>
</li>
<li>
<span id="SP2_6"><strong>SP2.6</strong></span> <em>Spatial Software should explicitly document the types
and classes of input data able to be passed to each function.</em>
</li>
<li>
<span id="SP2_7"><strong>SP2.7</strong></span> <em>Spatial Software should implement validation routines to
confirm that inputs are of acceptable classes (or represented in otherwise
appropriate ways for software which does not use class systems).</em>
</li>
<li>
<span id="SP2_8"><strong>SP2.8</strong></span> <em>Spatial Software should implement a single
pre-processing routine to validate input data, and to appropriately transform
it to a single uniform type to be passed to all subsequent data-processing
functions.</em>
</li>
<li>
<span id="SP2_9"><strong>SP2.9</strong></span> <em>The pre-processing function described above should
maintain those metadata attributes of input data which are relevant or
important to core algorithms or return values.</em>
</li>
</ul>
</div>
<div id="algorithms-2" class="section level3">
<h3>
<span class="header-section-number">6.6.3</span> Algorithms<a class="anchor" aria-label="anchor" href="#algorithms-2"><i class="fas fa-link"></i></a>
</h3>
<p>The following standards will be conditionally applicable to some but not all
spatial software. Procedures for standards deemed not applicable to
a particular piece of software are described in the <a href="https://github.com/ropenscilabs/srr"><code>srr</code>
package</a>.</p>
<ul>
<li>
<span id="SP3_0"><strong>SP3.0</strong></span> <em>Spatial software which considers spatial neighbours
should enable user control over neighbourhood forms and sizes. In
particular:</em>
<ul>
<li>
<span id="SP3_0a"><strong>SP3.0a</strong></span> <em>Neighbours (able to be expressed) on regular grids
should be able to be considered in both rectangular only, or rectangular
and diagonal (respectively “rook” and “queen” by analogy to chess).</em>
</li>
<li>
<span id="SP3_0b"><strong>SP3.0b</strong></span> <em>Neighbourhoods in irregular spaces should be
minimally able to be controlled via an integer number of neighbours, an
area (or equivalent distance defining an area) in which to include
neighbours, or otherwise equivalent user-controlled value.</em>
</li>
</ul>
</li>
<li>
<span id="SP3_1"><strong>SP3.1</strong></span> <em>Spatial software which considers spatial neighbours
should wherever possible enable neighbour contributions to be weighted by
distance (or other continuous weighting variable), and not rely exclusively
on a uniform-weight rectangular cut-off.</em>
</li>
<li>
<span id="SP3_2"><strong>SP3.2</strong></span> <em>Spatial software which relies on sampling from input
data (even if only of spatial coordinates) should enable sampling procedures
to be based on local spatial densities of those input data.</em>
</li>
</ul>
<p>An example of software which would <em>not</em> adhere to <strong>SP3.2</strong> would be where
input data were a simple matrix of spatial coordinates, and sampling were
implemented using the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/sample.html"><code>sample()</code>
function</a>
to randomly select elements of those input data
(like <code>sample(nrow(xy), n)</code>). In the context of an example based on the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/sample.html"><code>sample()</code>
function</a>,
adhering to the standard would require including an additional <code>prob</code> vector
where each point was weighted by the local density of surrounding points. Doing
so would lead to higher probabilities of samples being taken from central
clusters of higher densities than from outlying extreme points. Note that the
standard merely suggests that software should <em>enable</em> such density-based
samples to be taken, not that it must, or even necessarily should by default.</p>
<p>Algorithms for spatial software are often related to other categories of
statistical software, and it is anticipated that spatial software will commonly
also be subject to standards from these other categories. Nevertheless, because
spatial analyses frequently face unique challenges, some of these
category-specific standards also have extension standards when applied to
spatial software. The following standards will be applicable for any spatial
software which also fits any of the other listed categories of statistical
software.</p>
<p><strong>Regression Software</strong></p>
<ul>
<li>
<span id="SP3_3"><strong>SP3.3</strong></span> <em>Spatial regression software should explicitly quantify
and distinguish autocovariant or autoregressive processes from those
covariant or regressive processes not directly related to spatial structure
alone.</em>
</li>
</ul>
<p><strong>Unsupervised Learning Software</strong></p>
<p>The following standard applies to any spatial unsupervised learning software
which uses clustering algorithms.</p>
<ul>
<li>
<span id="SP3_4"><strong>SP3.4</strong></span> <em>Where possible, spatial clustering software should avoid
using standard non-spatial clustering algorithms in which spatial proximity
is merely represented by an additional weighting factor in favour of
explicitly spatial algorithms.</em>
</li>
</ul>
<p><strong>Machine Learning Software</strong></p>
<p>One common application in which machine learning algorithms are applied to
spatial software is in analyses of raster images. The first of the following
standards applies because the individual cells or pixels of these raster images
represent fixed spatial coordinates. (This standard also renders <strong>ML2.1</strong>
inapplicable).</p>
<ul>
<li>
<span id="SP3_5"><strong>SP3.5</strong></span> <em>Spatial machine learning software should ensure that
broadcasting procedures for reconciling inputs of different dimensions are
<strong>not</strong> applied</em>.</li>
</ul>
<p>A definition of broadcasting is given at the end of the introduction to
corresponding <a href="standards.html#ml-standards">Machine Learning Standards</a>, just
above <em>Input Data Specification</em>.</p>
<ul>
<li>
<span id="SP3_6"><strong>SP3.6</strong></span> <em>Spatial machine learning software should document (and,
where possible, test) the potential effects of different sampling procedures</em>
</li>
</ul>
<p>A simple example might be to provide examples or extended documentation which
compares the effects of sampling both test and training data from the same
spatial region versus sampling them from distinct regions. Although there are
no comparable <em>General Standard</em> for <a href="standards.html#ml-standards"><em>Machine Learning
Software</em></a>, procedures for sampling spatial data may have
particularly pronounced effects on results, and this standard attempts to
foster a “best practice” of documenting how such effects may arise with a given
piece of software.</p>
<p>A more concrete example may be to demonstrate a particular technique for
generating distinct test and training data such as spatial partitioning
<span class="citation">(Muenchow <a href="appendices.html#ref-muenchow_chapter_nodate" role="doc-biblioref">2019</a>; Brenning <a href="appendices.html#ref-brenning_spatial_2012" role="doc-biblioref">2012</a>; Schratz et al. <a href="appendices.html#ref-schratz_hyperparameter_2019" role="doc-biblioref">2019</a>; Valavi et al. <a href="appendices.html#ref-valavi_blockcv_2019" role="doc-biblioref">2019</a>)</span>.
There may nevertheless be cases in which such sampling from a common
spatial region is appropriate, for example for software intended to analyse or
model temporally-structured spatial data for which a more appropriate
distinction might be temporal rather than spatial. Adherence to this standard
merely requires that the potential for any such confounding effects be
explicitly documented (and possibly tested as well).</p>
</div>
<div id="return-results-2" class="section level3">
<h3>
<span class="header-section-number">6.6.4</span> Return Results<a class="anchor" aria-label="anchor" href="#return-results-2"><i class="fas fa-link"></i></a>
</h3>
<p>For (functions within) Spatial Software which return spatial data:</p>
<ul>
<li>
<span id="SP4_0"><strong>SP4.0</strong></span> <em>Return values should either:</em>
<ul>
<li>
<span id="SP4_0a"><strong>SP4.0a</strong></span> <em>Be in same class as input data, or</em>
</li>
<li>
<span id="SP4_0b"><strong>SP4.0b</strong></span> <em>Be in a unique, preferably class-defined, format.</em>
</li>
</ul>
</li>
<li>
<span id="SP4_1"><strong>SP4.1</strong></span> <em>Any aspects of input data which are included in output
data (either directly, or in some transformed form) and which contain units
should ensure those same units are maintained in return values.</em>
</li>
<li>
<span id="SP4_2"><strong>SP4.2</strong></span> <em>The type and class of all return values should be
explicitly documented.</em>
</li>
</ul>
</div>
<div id="visualization-1" class="section level3">
<h3>
<span class="header-section-number">6.6.5</span> Visualization<a class="anchor" aria-label="anchor" href="#visualization-1"><i class="fas fa-link"></i></a>
</h3>
<p>Spatial Software which returns objects in a custom class structure explicitly
designed to represent or include spatial data should:</p>
<ul>
<li>
<span id="SP5_0"><strong>SP5.0</strong></span> <em>Implement default <code>plot</code> methods for any implemented
class system.</em>
</li>
<li>
<span id="SP5_1"><strong>SP5.1</strong></span> <em>Implement appropriate placement of variables along x-
and y-axes.</em>
</li>
<li>
<span id="SP5_2"><strong>SP5.2</strong></span> <em>Ensure that axis labels include appropriate units.</em>
</li>
</ul>
<p>An example of <strong>SP5.1</strong> might be ensuring that longitude is placed on the
x-axis, latitude on the y, although standard orientations may depend on
coordinate reference systems and other aspects of data and software design.
The preceding three standards will generally not apply to software which
returns objects in a custom class structure yet which is not inherently
spatial.</p>
<p>Spatial Software which returns objects with geographical coordinates should:</p>
<ul>
<li>
<span id="SP5_3"><strong>SP5.3</strong></span> <em>Offer an ability to generate interactive (generally
<code>html</code>-based) visualisations of results.</em>
</li>
</ul>
</div>
<div id="testing-4" class="section level3">
<h3>
<span class="header-section-number">6.6.6</span> Testing<a class="anchor" aria-label="anchor" href="#testing-4"><i class="fas fa-link"></i></a>
</h3>
<p>The following standards apply to all Spatial Software which is intended or able
to be applied to data represented in curvilinear systems, notably including all
geographical data. The only Spatial Software to which the following standards
do not (necessarily) apply would be software explicitly intended to be applied
exclusively to Cartesian spatial data, and which ensured appropriate rejection
of curvilinear data according to <strong>SP2.0b</strong>.</p>
<p><strong>Round-Trip Tests</strong></p>
<ul>
<li>
<span id="SP6_0"><strong>SP6.0</strong></span> <em>Software which implements routines for transforming
coordinates of input data should include tests which demonstrate ability to
recover the original coordinates.</em>
</li>
</ul>
<p>This standard is applicable to any software which implements any routines for
coordinate transformations, even if those routines are implemented via
<a href="https://proj.org"><code>PROJ</code></a>. Conversely, software which has no routines for
coordinate transformations need not adhere to <strong>SP6.0</strong>, even if that software
relies on <code>PROJ</code> for other purposes.</p>
<ul>
<li>
<span id="SP6_1"><strong>SP6.1</strong></span> <em>All functions which can be applied to both Cartesian and
curvilinear data should be tested through application to both.</em>
<ul>
<li>
<span id="SP6_1a"><strong>SP6.1a</strong></span> <em>Functions which may yield inaccurate results when
applied to data in one or the other forms (such as the preceding examples
of centroids and buffers from ellipsoidal data) should test that results
from inappropriate application of those functions are indeed less
accurate.</em>
</li>
<li>
<span id="SP6_1b"><strong>SP6.1b</strong></span> <em>Functions which yield accurate results regardless
of whether input data are rectilinear or curvilinear should demonstrate
equivalent accuracy in both cases, and should also demonstrate how
equivalent results may be obtained through first explicitly transforming
input data.</em>
</li>
</ul>
</li>
</ul>
<p><strong>Extreme Geographical Coordinates</strong></p>
<ul>
<li>
<span id="SP6_2"><strong>SP6.2</strong></span> <em>Geographical Software should include tests with extreme
geographical coordinates, minimally including extension to polar extremes of
+/-90 degrees.</em>
</li>
</ul>
<p>While such tests should generally confirm that software generates reliable
results to such extreme coordinates, software which is unable to generate
reliable results to such inputs should nevertheless include tests to indicate
both approximate bounds of reliability, and the expected characteristics of
unreliable results.</p>
<p>The remaining standards for testing Spatial Software extend directly from the
preceding Algorithmic Standards (<strong>SP3</strong>), with the same sub-section headings
used here.</p>
<ul>
<li>
<span id="SP6_3"><strong>SP6.3</strong></span> <em>Spatial Software which considers spatial neighbours
should explicitly test all possible ways of defining them, and should
explicitly compare quantitative effects of different ways of defining
neighbours.</em>
</li>
<li>
<span id="SP6_4"><strong>SP6.4</strong></span> <em>Spatial Software which considers spatial neighbours
should explicitly test effects of different schemes to weight neighbours by
spatial proximity.</em>
</li>
</ul>
<p><strong>Unsupervised Learning Software</strong></p>
<ul>
<li>
<span id="SP6_5"><strong>SP6.5</strong></span> <em>Spatial Unsupervised Learning Software which uses
clustering algorithms should implement tests which explicitly compare results
with equivalent results obtained with a non-spatial clustering algorithm.</em>
</li>
</ul>
<p><strong>Machine Learning Software</strong></p>
<ul>
<li>
<span id="SP6_6"><strong>SP6.6</strong></span> *Spatial Machine Learning Software should implement tests
which explicitly demonstrate the detrimental consequences of sampling test
and training data from the same spatial region, rather than from spatially
distinct regions.</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
<div id="standards-time-series" class="section level2">
<h2>
<span class="header-section-number">6.7</span> Time Series Software<a class="anchor" aria-label="anchor" href="#standards-time-series"><i class="fas fa-link"></i></a>
</h2>
<p>The category of Time Series software is arguably easier to define that the
preceding categories, and represents any software the primary input of which is
intended to be temporally structured data. Importantly, while “<em>temporally
structured</em>” may often imply temporally ordered, this need not necessarily be
the case. The primary definition of temporally structured data is that they
possess some kind of index which can be used to extract temporal relationships.</p>
<p>Time series software is presumed to perform one or more of the following steps:</p>
<ol style="list-style-type: decimal">
<li>Accept and validate input data</li>
<li>Apply data transformation and pre-processing steps</li>
<li>Apply one or more analytic algorithms</li>
<li>Return the result of that algorithmic application</li>
<li>Offer additional functionality such as printing or summarising return results</li>
</ol>
<p>This document details standards for each of these steps, each prefixed with “TS”.</p>
<div id="input-data-structures-and-validation-3" class="section level3">
<h3>
<span class="header-section-number">6.7.1</span> Input data structures and validation<a class="anchor" aria-label="anchor" href="#input-data-structures-and-validation-3"><i class="fas fa-link"></i></a>
</h3>
<p>Input validation is an important software task, and an important part of our
standards. While there are many ways to approach validation, the class systems
of R offer a particularly convenient and effective means. For Time Series
Software in particular, a range of class systems have been developed, for which
we refer to the section “Time Series Classes” in the CRAN Task view on <a href="https://cran.r-project.org/web/views/TimeSeries.html">Time
Series Analysis"</a>, and
the class-conversion package <a href="https://www.tsbox.help/"><code>tsbox</code></a>. Software which
uses and relies on defined classes can often validate input through affirming
appropriate class(es). Software which does not use or rely on class systems
will generally need specific routines to validate input data structures. In
particular, because of the long history of time series software in R, and the
variety of class systems for representing time series data, new time series
packages should accept as many different classes of input as possible by
according with the following standards:</p>
<ul>
<li>
<span id="TS1_0"><strong>TS1.0</strong></span> <em>Time Series Software should use and rely on explicit
class systems developed for representing time series data, and should not
permit generic, non-time-series input</em>
</li>
</ul>
<p>The core algorithms of time-series software are often ultimately applied to
simple vector objects, and some time series software accepts simple vector
inputs, assuming these to represent temporally sequential data. Permitting such
generic inputs nevertheless prevents any such assumptions from being asserted
or tested. Missing values pose particular problems in this regard. A simple
<code><a href="https://rdrr.io/r/stats/na.fail.html">na.omit()</a></code> call or similar will shorten the length of the vector by removing
any <code>NA</code> values, and will change the explicit temporal relationship between
elements. The use of explicit classes for time series generally ensures an
ability to explicitly assert properties such as strict temporal regularity, and
to control for any deviation from expected properties.</p>
<ul>
<li>
<span id="TS1_1"><strong>TS1.1</strong></span> <em>Time Series Software should explicitly document the
types and classes of input data able to be passed to each function.</em>
</li>
</ul>
<p>Such documentation should include a demonstration of how to input data in at
least one commonly used class for time-series such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html"><code>ts</code></a>.</p>
<ul>
<li>
<span id="TS1_2"><strong>TS1.2</strong></span> <em>Time Series Software should implement validation
routines to confirm that inputs are of acceptable classes (or represented in
otherwise appropriate ways for software which does not use class systems).</em>
</li>
<li>
<span id="TS1_3"><strong>TS1.3</strong></span> <em>Time Series Software should implement a single
pre-processing routine to validate input data, and to appropriately transform
it to a single uniform type to be passed to all subsequent data-processing
functions (the <a href="https://www.tsbox.help/"><code>tsbox</code> package</a> provides one
convenient approach for this).</em>
</li>
<li>
<span id="TS1_4"><strong>TS1.4</strong></span> <em>The pre-processing function described above should
maintain all time- or date-based components or attributes of input data.</em>
</li>
</ul>
<p>For Time Series Software which relies on or implements custom classes or types
for representing time-series data, the following standards should be adhered
to:</p>
<ul>
<li>
<span id="TS1_5"><strong>TS1.5</strong></span> <em>The software should ensure strict ordering of the time,
frequency, or equivalent ordering index variable.</em>
</li>
<li>
<span id="TS1_6"><strong>TS1.6</strong></span> <em>Any violations of ordering should be caught in the
pre-processing stages of all functions.</em>
</li>
</ul>
<div id="time-intervals-and-relative-time" class="section level4">
<h4>
<span class="header-section-number">6.7.1.1</span> Time Intervals and Relative Time<a class="anchor" aria-label="anchor" href="#time-intervals-and-relative-time"><i class="fas fa-link"></i></a>
</h4>
<p>While most common packages and classes for time series data assume <em>absolute</em>
temporal scales such as those represented in <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/as.POSIXlt.html"><code>POSIX</code>
classes</a>
for dates or times, time series may also be quantified on <em>relative</em> scales
where the temporal index variable quantifies intervals rather than absolute
times or dates. Many analytic routines which accept time series inputs in
absolute form are also appropriately applied to analogous data in relative
form, and thus many packages should accept time series inputs both in absolute
and relative forms. Software which can or should accept times series inputs in
relative form should:</p>
<ul>
<li>
<span id="TS1_7"><strong>TS1.7</strong></span> <em>Accept inputs defined via the <a href="https://github.com/r-quantities/units/"><code>units</code>
package</a> for attributing SI units to
R vectors.</em>
</li>
<li>
<span id="TS1_8"><strong>TS1.8</strong></span> <em>Where time intervals or periods may be days or months,
be explicit about the system used to represent such, particularly regarding
whether a calendar system is used, or whether a year is presumed to have 365
days, 365.2422 days, or some other value.</em>
</li>
</ul>
</div>
</div>
<div id="pre-processing-and-variable-transformation-1" class="section level3">
<h3>
<span class="header-section-number">6.7.2</span> Pre-processing and Variable Transformation<a class="anchor" aria-label="anchor" href="#pre-processing-and-variable-transformation-1"><i class="fas fa-link"></i></a>
</h3>
<div id="missing-data" class="section level4">
<h4>
<span class="header-section-number">6.7.2.1</span> Missing Data<a class="anchor" aria-label="anchor" href="#missing-data"><i class="fas fa-link"></i></a>
</h4>
<p>One critical pre-processing step for Time Series Software is the appropriate
handling of missing data. It is convenient to distinguish between <em>implicit</em>
and <em>explicit</em> missing data. For regular time series, explicit missing data may
be represented by <code>NA</code> values, while for irregular time series, implicit
missing data may be represented by missing rows. The difference is demonstrated
in the following table.</p>
<div class="inline-table"><table class="table table-sm">
<caption>
Missing Values
</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">
Time
</td>
<td style="text-align: left;">
value
</td>
</tr>
<tr class="even">
<td style="text-align: left;">
08:43
</td>
<td style="text-align: left;">
0.71
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:44
</td>
<td style="text-align: left;">
NA
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:45
</td>
<td style="text-align: left;">
0.28
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:47
</td>
<td style="text-align: left;">
0.34
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:48
</td>
<td style="text-align: left;">
0.07
</td>
</tr>
</tbody>
</table></div>
<p>The value for 08:46 is <em>implicitly missing</em>, while the value for 08:44 is
<em>explicitly missing</em>. These two forms of missingness may connote different
things, and may require different forms of pre-processing. With this in mind,
and beyond the <a href="standards.html#general-standards"><em>General Standards</em></a> for missing data
(<strong>G2.13</strong>–<strong>G2.16</strong>), the following standards apply:</p>
<ul>
<li>
<span id="TS2_0"><strong>TS2.0</strong></span> <em>Time Series Software which presumes or requires regular
data should only allow <strong>explicit</strong> missing values, and should issue
appropriate diagnostic messages, potentially including errors, in response to
any <strong>implicit</strong> missing values.</em>
</li>
<li>
<span id="TS2_1"><strong>TS2.1</strong></span> <em>Where possible, all functions should provide options for
users to specify how to handle missing data, with options minimally
including:</em>
<ul>
<li>
<span id="TS2_1a"><strong>TS2.1a</strong></span> *error on missing data; or.</li>
<li>
<span id="TS2_1b"><strong>TS2.1b</strong></span> <em>warn or ignore missing data, and proceed to analyse
irregular data, ensuring that results from function calls with regular yet
missing data return identical values to submitting equivalent irregular
data with no missing values; or</em>
</li>
<li>
<span id="TS2_1c"><strong>TS2.1c</strong></span> <em>replace missing data with appropriately imputed
values.</em>
</li>
</ul>
</li>
</ul>
<p>This latter standard is a modified version of <em>General Standard</em> <strong>G2.14</strong>,
with additional requirements via <strong>TS2.1b</strong>.</p>
</div>
<div id="stationarity" class="section level4">
<h4>
<span class="header-section-number">6.7.2.2</span> Stationarity<a class="anchor" aria-label="anchor" href="#stationarity"><i class="fas fa-link"></i></a>
</h4>
<p>Time Series Software should explicitly document assumptions or requirements
made with respect to the stationarity or otherwise of all input data. In
particular, any (sub-)functions which assume or rely on stationarity should:</p>
<ul>
<li>
<span id="TS2_2"><strong>TS2.2</strong></span> *Consider stationarity of all relevant moments
<ul>
<li>typically first (mean) and second (variance) order, or otherwise document
why such consideration may be restricted to lower orders only.*</li>
</ul>
</li>
<li>
<span id="TS2_3"><strong>TS2.3</strong></span> <em>Explicitly document all assumptions and/or requirements
of stationarity</em>
</li>
<li>
<span id="TS2_4"><strong>TS2.4</strong></span> <em>Implement appropriate checks for all relevant forms of
stationarity, and either:</em>
<ul>
<li>
<span id="TS2_4a"><strong>TS2.4a</strong></span> <em>issue diagnostic messages or warnings; or</em>
</li>
<li>
<span id="TS2_4b"><strong>TS2.4b</strong></span> <em>enable or advise on appropriate transformations to
ensure stationarity.</em>
</li>
</ul>
</li>
</ul>
<p>The two options in the last point (TS2.4b) respectively translate to <em>enabling</em>
transformations to ensure stationarity by providing appropriate routines,
generally triggered by some function parameter, or <em>advising</em> on appropriate
transformations, for example by directing users to additional functions able to
implement appropriate transformations.</p>
</div>
<div id="covariance-matrices" class="section level4">
<h4>
<span class="header-section-number">6.7.2.3</span> Covariance Matrices<a class="anchor" aria-label="anchor" href="#covariance-matrices"><i class="fas fa-link"></i></a>
</h4>
<p>Where covariance matrices are constructed or otherwise used within or as input
to functions, they should:</p>
<ul>
<li>
<span id="TS2_5"><strong>TS2.5</strong></span> <em>Incorporate a system to ensure that both row and column
orders follow the same ordering as the underlying time series data. This may,
for example, be done by including the <code>index</code> attribute of the time series
data as an attribute of the covariance matrix.</em>
</li>
<li>
<span id="TS2_6"><strong>TS2.6</strong></span> <em>Where applicable, covariance matrices should also
include specification of appropriate units.</em>
</li>
</ul>
<p><em>General Standard</em> <strong>G3.1</strong> also applies to all Time Series Software which
constructs or uses covariance matrices.</p>
</div>
</div>
<div id="analytic-algorithms-2" class="section level3">
<h3>
<span class="header-section-number">6.7.3</span> Analytic Algorithms<a class="anchor" aria-label="anchor" href="#analytic-algorithms-2"><i class="fas fa-link"></i></a>
</h3>
<p>Analytic algorithms are considered here to reflect the core analytic components
of Time Series Software. These may be many and varied, and we explicitly
consider only a small subset here.</p>
<div id="forecasting" class="section level4">
<h4>
<span class="header-section-number">6.7.3.1</span> Forecasting<a class="anchor" aria-label="anchor" href="#forecasting"><i class="fas fa-link"></i></a>
</h4>
<p>Statistical software which implements forecasting routines should:</p>
<ul>
<li>
<span id="TS3_0"><strong>TS3.0</strong></span> <em>Provide tests to demonstrate at least one case in which
errors widen appropriately with forecast horizon.</em>
</li>
<li>
<span id="TS3_1"><strong>TS3.1</strong></span> <em>If possible, provide at least one test which violates
TS3.0</em>
</li>
<li>
<span id="TS3_2"><strong>TS3.2</strong></span> <em>Document the general drivers of forecast errors or
horizons, as demonstrated via the particular cases of TS3.0 and TS3.1</em>
</li>
<li>
<span id="TS3_3"><strong>TS3.3</strong></span> <em>Either:</em>
<ul>
<li>
<span id="TS3_3a"><strong>TS3.3a</strong></span> <em>Document, preferable via an example, how to trim
forecast values based on a specified error margin or equivalent; or</em>
</li>
<li>
<span id="TS3_3b"><strong>TS3.3b</strong></span> <em>Provide an explicit mechanism to trim forecast
values to a specified error margin, either via an explicit
post-processing function, or via an input parameter to a primary analytic
function.</em>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="return-results-3" class="section level3">
<h3>
<span class="header-section-number">6.7.4</span> Return Results<a class="anchor" aria-label="anchor" href="#return-results-3"><i class="fas fa-link"></i></a>
</h3>
<p>For (functions within) Time Series Software which return time series data:</p>
<ul>
<li>
<span id="TS4_0"><strong>TS4.0</strong></span> <em>Return values should either:</em>
<ul>
<li>
<span id="TS4_0a"><strong>TS4.0a</strong></span> <em>Be in same class as input data, for example by
using the <a href="https://www.tsbox.help/"><code>tsbox</code> package</a> to re-convert from
standard internal format (see 1.4, above); or</em>
</li>
<li>
<span id="TS4_0b"><strong>TS4.0b</strong></span> <em>Be in a unique, preferably class-defined, format.</em>
</li>
</ul>
</li>
<li>
<span id="TS4_1"><strong>TS4.1</strong></span> <em>Any units included as attributes of input data should
also be included within return values.</em>
</li>
<li>
<span id="TS4_2"><strong>TS4.2</strong></span> <em>The type and class of all return values should be
explicitly documented.</em>
</li>
</ul>
<p>For (functions within) Time Series Software which return data other than direct
series:</p>
<ul>
<li>
<span id="TS4_3"><strong>TS4.3</strong></span> <em>Return values should explicitly include all appropriate
units and/or time scales</em>
</li>
</ul>
<div id="data-transformation" class="section level4">
<h4>
<span class="header-section-number">6.7.4.1</span> Data Transformation<a class="anchor" aria-label="anchor" href="#data-transformation"><i class="fas fa-link"></i></a>
</h4>
<p>Time Series Software which internally implements routines for transforming data
to achieve stationarity and which returns forecast values should:</p>
<ul>
<li>
<span id="TS4_4"><strong>TS4.4</strong></span> <em>Document the effect of any such transformations on
forecast data, including potential effects on both first- and second-order
estimates.</em>
</li>
<li>
<span id="TS4_5"><strong>TS4.5</strong></span> <em>In decreasing order of preference, either:</em>
<ul>
<li>
<span id="TS4_5a"><strong>TS4.5a</strong></span> <em>Provide explicit routines or options to
back-transform data commensurate with original, non-stationary input
data</em>
</li>
<li>
<span id="TS4_5b"><strong>TS4.5b</strong></span> <em>Demonstrate how data may be back-transformed to
a form commensurate with original, non-stationary input data.</em>
</li>
<li>
<span id="TS4_5c"><strong>TS4.5c</strong></span> <em>Document associated limitations on forecast
values</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="forecasting-1" class="section level4">
<h4>
<span class="header-section-number">6.7.4.2</span> Forecasting<a class="anchor" aria-label="anchor" href="#forecasting-1"><i class="fas fa-link"></i></a>
</h4>
<p>Where Time Series Software implements or otherwise enables forecasting
abilities, it should return one of the following three kinds of information.
These are presented in decreasing order of preference, such that software
should strive to return the first kind of object, failing that the second, and
only the third as a last resort.</p>
<ul>
<li>
<span id="TS4_6"><strong>TS4.6</strong></span> <em>Time Series Software which implements or otherwise
enables forecasting should return either:</em>
<ul>
<li>
<span id="TS4_6a"><strong>TS4.6a</strong></span> <em>A distribution object, for example via one of the
many packages described in the CRAN Task View on <a href="https://cran.r-project.org/web/views/Distributions.html">Probability
Distributions</a>
(or the new <a href="https://pkg.mitchelloharawild.com/distributional/"><code>distributional</code>
package</a> as used in
the <a href="https://fable.tidyverts.org"><code>fable</code> package</a> for time-series
forecasting).</em>
</li>
<li>
<span id="TS4_6b"><strong>TS4.6b</strong></span> <em>For each variable to be forecast, predicted values
equivalent to first- and second-order moments (for example, mean and
standard error values).</em>
</li>
<li>
<span id="TS4_6c"><strong>TS4.6c</strong></span> <em>Some more general indication of error associated
with forecast estimates.</em>
</li>
</ul>
</li>
</ul>
<p>Beyond these particular standards for return objects, Time Series Software
which implements or otherwise enables forecasting should:</p>
<ul>
<li>
<span id="TS4_7"><strong>TS4.7</strong></span> <em>Ensure that forecast (modelled) values are clearly
distinguished from observed (model or input) values, either (in this case in
no order of preference) by</em>
<ul>
<li>
<span id="TS4_7a"><strong>TS4.7a</strong></span> <em>Returning forecast values alone</em>
</li>
<li>
<span id="TS4_7b"><strong>TS4.7b</strong></span> <em>Returning distinct list items for model and
forecast values</em>
</li>
<li>
<span id="TS4_7c"><strong>TS4.7c</strong></span> <em>Combining model and forecast values into a single
return object with an appropriate additional column clearly
distinguishing the two kinds of data.</em>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="visualization-2" class="section level3">
<h3>
<span class="header-section-number">6.7.5</span> Visualization<a class="anchor" aria-label="anchor" href="#visualization-2"><i class="fas fa-link"></i></a>
</h3>
<p>Time Series Software should:</p>
<ul>
<li>
<span id="TS5_0"><strong>TS5.0</strong></span> <em>Implement default <code>plot</code> methods for any implemented
class system.</em>
</li>
<li>
<span id="TS5_1"><strong>TS5.1</strong></span> <em>When representing results in temporal domain(s), ensure
that one axis is clearly labelled “time” (or equivalent), with continuous
units.</em>
</li>
<li>
<span id="TS5_2"><strong>TS5.2</strong></span> <em>Default to placing the “time” (or equivalent) variable
on the horizontal axis.</em>
</li>
<li>
<span id="TS5_3"><strong>TS5.3</strong></span> <em>Ensure that units of the time, frequency, or index
variable are printed by default on the axis.</em>
</li>
<li>
<span id="TS5_4"><strong>TS5.4</strong></span> <em>For frequency visualization, abscissa spanning
<span class="math inline">\([-\pi, \pi]\)</span> should be avoided in favour of positive units of <span class="math inline">\([0, 2\pi]\)</span> or
<span class="math inline">\([0, 0.5]\)</span>, in all cases with appropriate additional explanation of units.</em>
</li>
<li>
<span id="TS5_5"><strong>TS5.5</strong></span> <em>Provide options to determine whether plots of data with
missing values should generate continuous or broken lines.</em>
</li>
</ul>
<p>For the results of forecast operations, Time Series Software should</p>
<ul>
<li>
<span id="TS5_6"><strong>TS5.6</strong></span> <em>By default indicate distributional limits of forecast on
plot</em>
</li>
<li>
<span id="TS5_7"><strong>TS5.7</strong></span> <em>By default include model (input) values in plot, as well
as forecast (output) values</em>
</li>
<li>
<span id="TS5_8"><strong>TS5.8</strong></span> <em>By default provide clear visual distinction between
model (input) values and forecast (output) values.</em>
</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
<div id="dimensionality-reduction-clustering-and-unsupervised-learning" class="section level2">
<h2>
<span class="header-section-number">6.8</span> Dimensionality Reduction, Clustering, and Unsupervised Learning<a class="anchor" aria-label="anchor" href="#dimensionality-reduction-clustering-and-unsupervised-learning"><i class="fas fa-link"></i></a>
</h2>
<p>This sub-section details standards for Dimensionality Reduction, Clustering,
and Unsupervised Learning Software – referred to from here on for simplicity
as “Unsupervised Learning Software”. Software in this category is distinguished
from Regression Software though the latter aiming to construct or analyse one
or more mappings between two defined data sets (for example, a set of
“independent” data, <span class="math inline">\(X\)</span>, and a set of “dependent” data, “Y”), whereas
Unsupervised Learning Software aims to construct or analyse one or more
mappings between a defined set of input or independent data, and a second set
of “output” data which are not necessarily known or given prior to the
analysis. A key distinction in Unsupervised Learning Software and Algorithms is
between that for which output data represent (generally numerical)
transformations of the input data set, and that for which output data are
discrete labels applied to the input data. Examples of the former type include
dimensionality reduction and ordination software and algorithms, and examples
of the latter include clustering and discrete partitioning software and
algorithms.</p>
<p>Some examples of <em>Dimensionality Reduction, Clustering, and Unsupervised
Learning</em> software include:</p>
<ol style="list-style-type: decimal">
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01596"><code>ivis</code></a> implements
a dimensionality reduction technique using a "Siamese Neural Network
architecture.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01279"><code>tsfeaturex</code></a> is
a package to automate “time series feature extraction,” which also provides
an example of a package for which both input and output data are generally
incomparable with most other packages in this category.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.01077"><code>iRF</code></a> is another
example of a generally incomparable package within this category, here one
for which the features extracted are the most distinct predictive features
extracted from repeated iterations of random forest algorithms.</li>
<li>
<a href="https://joss.theoj.org/papers/10.21105/joss.00967"><code>compboost</code></a> is
a package for component-wise gradient boosting which may be sufficient
general to potentially allow general application to problems addressed by
several packages in this category.</li>
<li>The <a href="https://joss.theoj.org/papers/10.21105/joss.00786"><code>iml</code></a> package may
offer usable functionality for devising general assessments of software
within this category, through offering a “toolbox for making machine
learning models interpretable” in a “model agnostic” way.</li>
</ol>
<p>Click on the following link to view a demonstration <a href="https://hackmd.io/iOZD_oCpT86zoY5z4memaQ">Application of
Dimensionality Reduction, Clustering, and Unsupervised Learning
Standards</a>.</p>
<div id="input-data-structures-and-validation-4" class="section level3">
<h3>
<span class="header-section-number">6.8.1</span> Input Data Structures and Validation<a class="anchor" aria-label="anchor" href="#input-data-structures-and-validation-4"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="UL1_0"><strong>UL1.0</strong></span> <em>Unsupervised Learning Software should explicitly
document expected format (types or classes) for input data, including
descriptions of types or classes which are not accepted; for example,
specification that software accepts only numeric inputs in <code>vector</code> or
<code>matrix</code> form, or that all inputs must be in <code>data.frame</code> form with both
column and row names.</em>
</li>
<li>
<span id="UL1_1"><strong>UL1.1</strong></span> <em>Unsupervised Learning Software should provide distinct
sub-routines to assert that all input data is of the expected form, and issue
informative error messages when incompatible data are submitted.</em>
</li>
</ul>
<p>The following code demonstrates an example of a routine from the base <code>stats</code>
package which fails to meet this standard.</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">d</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span> <span class="co"># example from help file for 'hclust' function</span>
<span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="va">d</span><span class="op">)</span> <span class="co"># okay</span>
<span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">as.matrix</a></span> <span class="op">(</span><span class="va">d</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; Error in if (is.na(n) || n &gt; 65536L) stop("size cannot be NA nor exceed 65536"): missing value where TRUE/FALSE needed</span></code></pre></div>
<p>The latter fails, yet issues an uninformative error message that clearly
indicates a failure to provide sufficient checks on the class of input data.</p>
<ul>
<li>
<span id="UL1_2"><strong>UL1.2</strong></span> <em>Unsupervised learning which uses row or column names to
label output objects should assert that input data have non-default row or
column names, and issue an informative message when these are not provided.</em>
</li>
</ul>
<p>Such messages need not necessarily be provided by default, but should at least
be optionally available.</p>
<details><summary>
Click here for examples of checks for whether row and column names have generic default values.
</summary><p>
</p>
<p>The <code>data.frame</code> function inserts default row and column names where these are
not explicitly specified.</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, ncol <span class="op">=</span> <span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="va">x</span>
<span class="co">#&gt;   X1 X2</span>
<span class="co">#&gt; 1  1  6</span>
<span class="co">#&gt; 2  2  7</span>
<span class="co">#&gt; 3  3  8</span>
<span class="co">#&gt; 4  4  9</span>
<span class="co">#&gt; 5  5 10</span></code></pre></div>
<p>Generic row names are almost always simple integer sequences, which the
following condition confirms.</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
<p>Generic column names may come in a variety of formats. The following code uses
a <code>grep</code> expression to match any number of characters plus an optional leading
zero followed by a generic sequence of column numbers, appropriate for matching
column names produced by generic construction of <code>data.frame</code> objects.</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/all.html">all</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">vapply</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span>, <span class="kw">function</span> <span class="op">(</span><span class="va">i</span><span class="op">)</span>
             <span class="fu"><a href="https://rdrr.io/r/base/grep.html">grepl</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span> <span class="op">(</span><span class="st">"[[:alpha:]]0?"</span>, <span class="va">i</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/logical.html">logical</a></span> <span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] TRUE</span></code></pre></div>
<p>Messages should be issued in both of these cases.</p>

</details><p><br></p>
<p>The following code illustrates that the <code>hclust</code> function does not implement
any such checks or assertions, rather it silently returns an object with
default labels.</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">u</span> <span class="op">&lt;-</span> <span class="va">USArrests</span>
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">rownames</a></span> <span class="op">(</span><span class="va">u</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span> <span class="op">(</span><span class="va">u</span><span class="op">)</span><span class="op">)</span>
<span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span> <span class="op">(</span><span class="va">u</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span> <span class="op">(</span><span class="va">hc</span><span class="op">$</span><span class="va">labels</span><span class="op">)</span>
<span class="co">#&gt; [1] "1" "2" "3" "4" "5" "6"</span></code></pre></div>
<ul>
<li>
<span id="UL1_3"><strong>UL1.3</strong></span> <em>Unsupervised Learning Software should transfer all
relevant aspects of input data, notably including row and column names, and
potentially information from other <code><a href="https://rdrr.io/r/base/attributes.html">attributes()</a></code>, to corresponding aspects
of return objects.</em>
<ul>
<li>
<span id="UL1_3a"><strong>UL1.3a</strong></span> <em>Where otherwise relevant information is not
transferred, this should be explicitly documented.</em>
</li>
</ul>
</li>
</ul>
<p>An example of a function according with UL1.3 is
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>stats::cutree()</code></a></p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span> <span class="op">(</span><span class="va">hc</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;    Alabama     Alaska    Arizona   Arkansas California   Colorado </span>
<span class="co">#&gt;          1          2          3          4          5          4</span></code></pre></div>
<p>The row names of <code>USArrests</code> are transferred to the output object. In contrast,
some routines from the <a href="https://cran.r-project.org/package=cluster"><code>cluster</code>
package</a> do not comply with this standard:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span> <span class="op">(</span><span class="va"><a href="https://svn.r-project.org/R-packages/trunk/cluster/">cluster</a></span><span class="op">)</span>
<span class="va">ac</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/cluster/man/agnes.html">agnes</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span> <span class="co"># agglomerative nesting</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span> <span class="op">(</span><span class="va">ac</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 1 2 3 4 3 4</span></code></pre></div>
<p>The case labels are not appropriately carried through to the object returned by
<a href="https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/agnes.html"><code>agnes()</code></a>
to enable them to be transferred within
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>cutree()</code></a>.
(The labels are transferred to the object returned by <code>agnes</code>, just not in
a way that enables <code>cutree</code> to inherit them.)</p>
<ul>
<li>
<span id="UL1_4"><strong>UL1.4</strong></span> <em>Unsupervised Learning Software should document any
assumptions made with regard to input data; for example assumptions about
distributional forms or locations (such as that data are centred or on
approximately equivalent distributional scales). Implications of violations
of these assumptions should be both documented and tested, in particular:</em>
<ul>
<li>
<span id="UL1_4a"><strong>UL1.4a</strong></span> <em>Software which responds qualitatively differently
to input data which has components on markedly different scales should
explicitly document such differences, and implications of submitting such
data.</em>
</li>
<li>
<span id="UL1_4b"><strong>UL1.4b</strong></span> <em>Examples or other documentation should not use
<code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> or equivalent transformations without explaining why scale is
applied, and explicitly illustrating and contrasting the consequences of
not applying such transformations.</em>
</li>
</ul>
</li>
</ul>
</div>
<div id="pre-processing-and-variable-transformation-2" class="section level3">
<h3>
<span class="header-section-number">6.8.2</span> Pre-processing and Variable Transformation<a class="anchor" aria-label="anchor" href="#pre-processing-and-variable-transformation-2"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="UL2_0"><strong>UL2.0</strong></span> <em>Routines likely to give unreliable or irreproducible
results in response to violations of assumptions regarding input data (see
UL1.6) should implement pre-processing steps to diagnose potential
violations, and issue appropriately informative messages, and/or include
parameters to enable suitable transformations to be applied.</em>
</li>
</ul>
<p>Example of compliance with this standard are the documentation entries for the
<code>center</code> and <code>scale.</code> parameters of the
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html"><code>stats::prcomp()</code></a>
function.</p>
<ul>
<li>
<span id="UL2_1"><strong>UL2.1</strong></span> <em>Unsupervised Learning Software should document any
transformations applied to input data, for example conversion of label-values
to <code>factor</code>, and should provide ways to explicitly avoid any default
transformations (with error or warning conditions where appropriate).</em>
</li>
<li>
<span id="UL2_2"><strong>UL2.2</strong></span> <em>Unsupervised Learning Software which accepts missing
values in input data should implement explicit parameters controlling the
processing of missing values, ideally distinguishing <code>NA</code> or <code>NaN</code> values
from <code>Inf</code> values.</em>
</li>
</ul>
<p>This standard applies beyond <em>General Standards</em> <strong>G2.13</strong>–<strong>G2.16</strong>, through
the additional requirement of implementing explicit parameters.</p>
<ul>
<li>
<span id="UL2_3"><strong>UL2.3</strong></span> <em>Unsupervised Learning Software should implement
pre-processing routines to identify whether aspects of input data are
perfectly collinear.</em>
</li>
</ul>
</div>
<div id="algorithms-3" class="section level3">
<h3>
<span class="header-section-number">6.8.3</span> Algorithms<a class="anchor" aria-label="anchor" href="#algorithms-3"><i class="fas fa-link"></i></a>
</h3>
<div id="labelling" class="section level4">
<h4>
<span class="header-section-number">6.8.3.1</span> Labelling<a class="anchor" aria-label="anchor" href="#labelling"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="UL3_0"><strong>UL3.0</strong></span> <em>Algorithms which apply sequential labels to input data
(such as clustering or partitioning algorithms) should ensure that the
sequence follows decreasing group sizes (so labels of “1”, “a”, or “A”
describe the largest group, “2”, “b”, or “B” the second largest, and so on.)</em>
</li>
</ul>
<p>Note that the <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>stats::cutree()</code>
function</a>
does not accord with this standard:</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span> <span class="op">(</span><span class="va">hc</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;  1  2  3  4  5  6  7  8  9 10 </span>
<span class="co">#&gt;  3  3  3  6  5 10  2  5  5  8</span></code></pre></div>
<p>The <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>cutree()</code>
function</a>
applies arbitrary integer labels to the groups, yet the order of labels is not
related to the order of group sizes.</p>
<ul>
<li>
<span id="UL3_1"><strong>UL3.1</strong></span> <em>Dimensionality reduction or equivalent algorithms which
label dimensions should ensure that that sequences of labels follows
decreasing “importance” (for example, eigenvalues or variance
contributions).</em>
</li>
</ul>
<p>The
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html"><code>stats::prcomp</code></a>
function accords with this standard:</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">z</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span> <span class="op">(</span><span class="va">eurodist</span>, rank <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="co"># return maximum of 5 components</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span> <span class="op">(</span><span class="va">z</span><span class="op">)</span>
<span class="co">#&gt; Importance of first k=5 (out of 21) components:</span>
<span class="co">#&gt;                              PC1       PC2       PC3       PC4       PC5</span>
<span class="co">#&gt; Standard deviation     2529.6298 2157.3434 1459.4839 551.68183 369.10901</span>
<span class="co">#&gt; Proportion of Variance    0.4591    0.3339    0.1528   0.02184   0.00977</span>
<span class="co">#&gt; Cumulative Proportion     0.4591    0.7930    0.9458   0.96764   0.97741</span></code></pre></div>
<p>The proportion of variance explained by each component decreasing with
increasing numeric labelling of the components.</p>
<ul>
<li>
<span id="UL3_2"><strong>UL3.2</strong></span> <em>Unsupervised Learning Software for which input data does
not generally include labels (such as <code>array</code>-like data with no row names)
should provide an additional parameter to enable cases to be labelled.</em>
</li>
</ul>
</div>
<div id="prediction" class="section level4">
<h4>
<span class="header-section-number">6.8.3.2</span> Prediction<a class="anchor" aria-label="anchor" href="#prediction"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="UL3_3"><strong>UL3.3</strong></span> <em>Where applicable, Unsupervised Learning Software should
implement routines to predict the properties (such as numerical ordinates, or
cluster memberships) of additional new data without re-running the entire
algorithm.</em>
</li>
</ul>
<p>While many algorithms such as Hierarchical clustering can not (readily) be used
to predict memberships of new data, other algorithms can nevertheless be
applied to perform this task. The following demonstrates how the output of
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html"><code>stats::hclust</code></a>
can be used to predict membership of new data using the <a href="https://stat.ethz.ch/R-manual/R-devel/library/class/html/knn.html"><code>class:knn()</code>
function</a>.
(This is intended to illustrate only one of many possible approaches.)</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span> <span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">class</a></span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Attaching package: 'class'</span>
<span class="co">#&gt; The following object is masked from 'package:igraph':</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;     knn</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span> <span class="op">(</span><span class="fl">1</span><span class="op">)</span>
<span class="va">hc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span> <span class="op">(</span><span class="va">iris</span> <span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span><span class="op">)</span>
<span class="va">groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span> <span class="op">(</span><span class="va">hc</span>, k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>
<span class="co"># function to randomly select part of a data.frame and # add some randomness</span>
<span class="va">sample_df</span> <span class="op">&lt;-</span> <span class="kw">function</span> <span class="op">(</span><span class="va">x</span>, <span class="va">n</span> <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">x</span> <span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span>, size <span class="op">=</span> <span class="va">n</span><span class="op">)</span>, <span class="op">]</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span> <span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">*</span> <span class="va">n</span><span class="op">)</span>
<span class="op">}</span>
<span class="va">iris_new</span> <span class="op">&lt;-</span> <span class="fu">sample_df</span> <span class="op">(</span><span class="va">iris</span> <span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span>, n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>
<span class="co"># use knn to predict membership of those new points:</span>
<span class="va">knnClust</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span> <span class="op">(</span>train <span class="op">=</span> <span class="va">iris</span> <span class="op">[</span>, <span class="op">-</span><span class="fl">5</span><span class="op">]</span>, test <span class="op">=</span> <span class="va">iris_new</span> , k <span class="op">=</span> <span class="fl">1</span>, cl <span class="op">=</span> <span class="va">groups</span><span class="op">)</span>
<span class="va">knnClust</span>
<span class="co">#&gt; [1] 2 2 1 1 2</span>
<span class="co">#&gt; Levels: 1 2 3</span></code></pre></div>
<p>The <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html"><code>stats::prcomp()</code>
function</a>
implements its own <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> method which conforms to this standard:</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span>
<span class="va">arrests_new</span> <span class="op">&lt;-</span> <span class="fu">sample_df</span> <span class="op">(</span><span class="va">USArrests</span>, n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span> <span class="op">(</span><span class="va">res</span>, newdata <span class="op">=</span> <span class="va">arrests_new</span><span class="op">)</span>
<span class="co">#&gt;                      PC1        PC2        PC3       PC4</span>
<span class="co">#&gt; North Carolina 165.17494 -30.693263 -11.682811  1.304563</span>
<span class="co">#&gt; Maryland       129.44401  -4.132644  -2.161693  1.258237</span>
<span class="co">#&gt; Ohio           -49.51994  12.748248   2.104966 -2.777463</span>
<span class="co">#&gt; Colorado        35.78896  14.023774  12.869816  1.233391</span>
<span class="co">#&gt; Georgia         41.28054  -7.203986   3.987152 -7.818416</span></code></pre></div>
</div>
<div id="group-distributions-and-associated-statistics" class="section level4">
<h4>
<span class="header-section-number">6.8.3.3</span> Group Distributions and Associated Statistics<a class="anchor" aria-label="anchor" href="#group-distributions-and-associated-statistics"><i class="fas fa-link"></i></a>
</h4>
<p>Many unsupervised learning algorithms serve to label, categorise, or partition
data. Software which performs any of these tasks will commonly output some kind
of labelling or grouping schemes. The above example of principal components
illustrates that the return object records the standard deviations associated
with each component:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span>
<span class="co">#&gt; Standard deviations (1, .., p=4):</span>
<span class="co">#&gt; [1] 83.732400 14.212402  6.489426  2.482790</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Rotation (n x k) = (4 x 4):</span>
<span class="co">#&gt;                 PC1         PC2         PC3         PC4</span>
<span class="co">#&gt; Murder   0.04170432 -0.04482166  0.07989066 -0.99492173</span>
<span class="co">#&gt; Assault  0.99522128 -0.05876003 -0.06756974  0.03893830</span>
<span class="co">#&gt; UrbanPop 0.04633575  0.97685748 -0.20054629 -0.05816914</span>
<span class="co">#&gt; Rape     0.07515550  0.20071807  0.97408059  0.07232502</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span> <span class="op">(</span><span class="va">res</span><span class="op">)</span>
<span class="co">#&gt; Importance of components:</span>
<span class="co">#&gt;                            PC1      PC2    PC3     PC4</span>
<span class="co">#&gt; Standard deviation     83.7324 14.21240 6.4894 2.48279</span>
<span class="co">#&gt; Proportion of Variance  0.9655  0.02782 0.0058 0.00085</span>
<span class="co">#&gt; Cumulative Proportion   0.9655  0.99335 0.9991 1.00000</span></code></pre></div>
<p>Such output accords with the following standard:</p>
<ul>
<li>
<span id="UL3_4"><strong>UL3.4</strong></span> <em>Objects returned from Unsupervised Learning Software
which labels, categorise, or partitions data into discrete groups should
include, or provide immediate access to, quantitative information on
intra-group variances or equivalent, as well as on inter-group relationships
where applicable.</em>
</li>
</ul>
<p>The above example of principal components is one where there are no inter-group
relationships, and so that standard is fulfilled by providing information on
intra-group variances alone. Discrete clustering algorithms, in contrast, yield
results for which inter-group relationships are meaningful, and such
relationships can generally be meaningfully provided. The <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html"><code>hclust()</code>
routine</a>,
like many clustering routines, simply returns a <em>scheme</em> for devising an
arbitrary number of clusters, and so
can not meaningfully provide variances or relationships between such. The
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cutree.html"><code>cutree()</code>
function</a>,
however, does yield defined numbers of clusters, yet devoid of any quantitative
information on variances or equivalent.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/hclust.html">hclust</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/dist.html">dist</a></span> <span class="op">(</span><span class="va">USArrests</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/cutree.html">cutree</a></span> <span class="op">(</span><span class="va">res</span>, k <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt;  Named int [1:50] 1 1 1 2 1 2 3 1 4 2 ...</span>
<span class="co">#&gt;  - attr(*, "names")= chr [1:50] "Alabama" "Alaska" "Arizona" "Arkansas" ...</span></code></pre></div>
<p>Compare that with the output of a largely equivalent routine, the <a href="https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clara.html"><code>clara()</code>
function</a>
from the <a href="https://cran.r-project.org/package=cluster"><code>cluster</code> package</a>.</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span> <span class="op">(</span><span class="va"><a href="https://svn.r-project.org/R-packages/trunk/cluster/">cluster</a></span><span class="op">)</span>
<span class="va">cl</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/cluster/man/clara.html">clara</a></span> <span class="op">(</span><span class="va">USArrests</span>, k <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="co"># direct clustering into specified number of clusters</span>
<span class="va">cl</span><span class="op">$</span><span class="va">clusinfo</span>
<span class="co">#&gt;       size  max_diss   av_diss isolation</span>
<span class="co">#&gt;  [1,]    4 24.708298 14.284874 1.4837745</span>
<span class="co">#&gt;  [2,]    6 28.857755 16.759943 1.7329563</span>
<span class="co">#&gt;  [3,]    6 44.640565 23.718040 0.9677229</span>
<span class="co">#&gt;  [4,]    6 28.005892 17.382196 0.8442061</span>
<span class="co">#&gt;  [5,]    6 15.901258  9.363471 1.1037219</span>
<span class="co">#&gt;  [6,]    7 29.407822 14.817031 0.9080598</span>
<span class="co">#&gt;  [7,]    4 11.764353  6.781659 0.8165753</span>
<span class="co">#&gt;  [8,]    3  8.766984  5.768183 0.3547323</span>
<span class="co">#&gt;  [9,]    3 18.848077 10.101505 0.7176276</span>
<span class="co">#&gt; [10,]    5 16.477257  8.468541 0.6273603</span></code></pre></div>
<p>That object contains information on dissimilarities between each observation
and cluster medoids, which in the context of UL3.4 is “information on
intra-group variances or equivalent”. Moreover, inter-group information is also
available as the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/silhouette.html">“silhouette”</a>
of the clustering scheme.</p>
</div>
</div>
<div id="return-results-4" class="section level3">
<h3>
<span class="header-section-number">6.8.4</span> Return Results<a class="anchor" aria-label="anchor" href="#return-results-4"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="UL4_0"><strong>UL4.0</strong></span> <em>Unsupervised Learning Software should return some form
of “model” object, generally through using or modifying existing class
structures for model objects, or creating a new class of model objects.</em>
</li>
<li>
<span id="UL4_1"><strong>UL4.1</strong></span> <em>Unsupervised Learning Software may enable an ability to
generate a model object without actually fitting values. This may be useful
for controlling batch processing of computationally intensive fitting
algorithms.</em>
</li>
<li>
<span id="UL4_2"><strong>UL4.2</strong></span> <em>The return object from Unsupervised Learning Software
should include, or otherwise enable immediate extraction of, all parameters
used to control the algorithm used.</em>
</li>
</ul>
<div id="reporting-return-results-1" class="section level4">
<h4>
<span class="header-section-number">6.8.4.1</span> Reporting Return Results<a class="anchor" aria-label="anchor" href="#reporting-return-results-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<span id="UL4_3"><strong>UL4.3</strong></span> <em>Model objects returned by Unsupervised Learning Software
should implement or appropriately extend a default <code>print</code> method which
provides an on-screen summary of model (input) parameters and methods used to
generate results. The <code>print</code> method may also summarise statistical aspects
of the output data or results.</em>
<ul>
<li>
<span id="UL4_3a"><strong>UL4.3a</strong></span> <em>The default <code>print</code> method should always ensure
only a restricted number of rows of any result matrices or equivalent are
printed to the screen.</em>
</li>
</ul>
</li>
</ul>
<p>The <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html"><code>prcomp</code>
objects</a>
returned from the function of the same name include potential large matrices of
component coordinates which are by default printed in their entirety to the
screen. This is because the default print behaviour for most tabular objects in
R (<code>matrix</code>, <code>data.frame</code>, and objects from the <code>Matrix</code> package, for example)
is to print objects in their entirety (limited only by such options as
<code>getOption("max.print")</code>, which determines maximal numbers of printed objects,
such as lines of <code>data.frame</code> objects). Such default behaviour ought be
avoided, particularly in Unsupervised Learning Software which commonly returns
objects containing large numbers of numeric entries.</p>
<ul>
<li>
<span id="UL4_4"><strong>UL4.4</strong></span> <em>Unsupervised Learning Software should also implement
<code>summary</code> methods for model objects which should summarise the primary
statistics used in generating the model (such as numbers of observations,
parameters of methods applied). The <code>summary</code> method may also provide summary
statistics from the resultant model.</em>
</li>
</ul>
</div>
</div>
<div id="documentation-3" class="section level3">
<h3>
<span class="header-section-number">6.8.5</span> Documentation<a class="anchor" aria-label="anchor" href="#documentation-3"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="visualization-3" class="section level3">
<h3>
<span class="header-section-number">6.8.6</span> Visualization<a class="anchor" aria-label="anchor" href="#visualization-3"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span id="UL6_0"><strong>UL6.0</strong></span> <em>Objects returned by Unsupervised Learning Software
should have default <code>plot</code> methods, either through explicit implementation,
extension of methods for existing model objects, through ensuring default
methods work appropriately, or through explicit reference to helper packages
such as <a href="https://github.com/kassambara/factoextra"><code>factoextra</code></a> and
associated functions.</em>
</li>
<li>
<span id="UL6_1"><strong>UL6.1</strong></span> <em>Where the default <code>plot</code> method is <strong>NOT</strong> a generic
<code>plot</code> method dispatched on the class of return objects (that is, through an
S3-type <code>plot.&lt;myclass&gt;</code> function or equivalent), that method dispatch (or
equivalent) should nevertheless exist in order to explicitly direct users to
the appropriate function.</em>
</li>
<li>
<span id="UL6_2"><strong>UL6.2</strong></span> <em>Where default plot methods include labelling components
of return objects (such as cluster labels), routines should ensure that
labels are automatically placed to ensure readability, and/or that
appropriate diagnostic messages are issued where readability is likely to be
compromised (for example, through attempting to place too many labels).</em>
</li>
</ul>
</div>
<div id="testing-5" class="section level3">
<h3>
<span class="header-section-number">6.8.7</span> Testing<a class="anchor" aria-label="anchor" href="#testing-5"><i class="fas fa-link"></i></a>
</h3>
<p>Unsupervised Learning Software should test the following properties and
behaviours:</p>
<ul>
<li>
<span id="UL7_0"><strong>UL7.0</strong></span> <em>Inappropriate types of input data are rejected with
expected error messages.</em>
</li>
</ul>
<div id="input-scaling" class="section level4">
<h4>
<span class="header-section-number">6.8.7.1</span> Input Scaling<a class="anchor" aria-label="anchor" href="#input-scaling"><i class="fas fa-link"></i></a>
</h4>
<p>The following tests should be implement for Unsupervised Learning Software for
which inputs are presumed or required to be scaled in any particular ways (such
as having mean values of zero).</p>
<ul>
<li>
<span id="UL7_1"><strong>UL7.1</strong></span> <em>Tests should demonstrate that violations of assumed
input properties yield unreliable or invalid outputs, and should clarify how
such unreliability or invalidity is manifest through the properties of
returned objects.</em>
</li>
</ul>
</div>
<div id="output-labelling" class="section level4">
<h4>
<span class="header-section-number">6.8.7.2</span> Output Labelling<a class="anchor" aria-label="anchor" href="#output-labelling"><i class="fas fa-link"></i></a>
</h4>
<p>With regard to labelling of output data, tests for Unsupervised Learning
Software should:</p>
<ul>
<li>
<span id="UL7_2"><strong>UL7.2</strong></span> <em>Demonstrate that labels placed on output data follow
decreasing group sizes (<strong>UL3.0</strong>)</em>
</li>
<li>
<span id="UL7_3"><strong>UL7.3</strong></span> *Demonstrate that labels on input data are propagated to,
or may be recovered from, output data.</li>
</ul>
</div>
<div id="prediction-1" class="section level4">
<h4>
<span class="header-section-number">6.8.7.3</span> Prediction<a class="anchor" aria-label="anchor" href="#prediction-1"><i class="fas fa-link"></i></a>
</h4>
<p>With regard to prediction, tests for Unsupervised Learning Software should:</p>
<ul>
<li>
<span id="UL7_4"><strong>UL7.4</strong></span> <em>Demonstrate that submission of new data to a previously
fitted model can generate results more efficiently than initial model
fitting.</em>
</li>
</ul>
</div>
<div id="batch-processing-1" class="section level4">
<h4>
<span class="header-section-number">6.8.7.4</span> Batch Processing<a class="anchor" aria-label="anchor" href="#batch-processing-1"><i class="fas fa-link"></i></a>
</h4>
<p>For Unsupervised Learning Software which implements batch processing routines:</p>
<ul>
<li>
<span id="UL7_5"><strong>UL7.5</strong></span> <em>Batch processing routines should be explicitly tested,
commonly via extended tests (see <strong>G4.10</strong>–<strong>G4.12</strong>).</em>
<ul>
<li>
<span id="UL7_5a"><strong>UL7.5a</strong></span> <em>Tests of batch processing routines should
demonstrate that equivalent results are obtained from direct (non-batch)
processing.</em>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="pkgreview.html"><span class="header-section-number">5</span> Guide for Reviewers</a></div>
<div class="next"><a href="appendices.html"><span class="header-section-number">A</span> Appendices</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#standards"><span class="header-section-number">6</span> Standards: Version 0.0.1</a></li>
<li>
<a class="nav-link" href="#general-standards"><span class="header-section-number">6.1</span> General Standards for Statistical Software</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#documentation"><span class="header-section-number">6.1.1</span> Documentation</a></li>
<li><a class="nav-link" href="#input-structures"><span class="header-section-number">6.1.2</span> Input Structures</a></li>
<li><a class="nav-link" href="#algorithms"><span class="header-section-number">6.1.3</span> Algorithms</a></li>
<li><a class="nav-link" href="#output-structures"><span class="header-section-number">6.1.4</span> Output Structures</a></li>
<li><a class="nav-link" href="#testing"><span class="header-section-number">6.1.5</span> Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#standards-bayesian"><span class="header-section-number">6.2</span> Bayesian and Monte Carlo Software</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#documentation-of-inputs"><span class="header-section-number">6.2.1</span> Documentation of Inputs</a></li>
<li><a class="nav-link" href="#input-data-structures-and-validation"><span class="header-section-number">6.2.2</span> Input Data Structures and Validation</a></li>
<li><a class="nav-link" href="#pre-processing-and-data-transformation"><span class="header-section-number">6.2.3</span> Pre-processing and Data Transformation</a></li>
<li><a class="nav-link" href="#analytic-algorithms"><span class="header-section-number">6.2.4</span> Analytic Algorithms</a></li>
<li><a class="nav-link" href="#return-values"><span class="header-section-number">6.2.5</span> Return Values</a></li>
<li><a class="nav-link" href="#additional-functionality"><span class="header-section-number">6.2.6</span> Additional Functionality</a></li>
<li><a class="nav-link" href="#tests"><span class="header-section-number">6.2.7</span> Tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#exploratory-data-analysis"><span class="header-section-number">6.3</span> Exploratory Data Analysis</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#documentation-standards"><span class="header-section-number">6.3.1</span> Documentation Standards</a></li>
<li><a class="nav-link" href="#input-data-1"><span class="header-section-number">6.3.2</span> Input Data</a></li>
<li><a class="nav-link" href="#analytic-algorithms-1"><span class="header-section-number">6.3.3</span> Analytic Algorithms</a></li>
<li><a class="nav-link" href="#return-results-output-data"><span class="header-section-number">6.3.4</span> Return Results / Output Data</a></li>
<li><a class="nav-link" href="#visualization-and-summary-output"><span class="header-section-number">6.3.5</span> Visualization and Summary Output</a></li>
<li><a class="nav-link" href="#testing-1"><span class="header-section-number">6.3.6</span> Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#ml-standards"><span class="header-section-number">6.4</span> Machine Learning Software</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#input-data-specification"><span class="header-section-number">6.4.1</span> Input Data Specification</a></li>
<li><a class="nav-link" href="#pre-processing"><span class="header-section-number">6.4.2</span> Pre-processing</a></li>
<li><a class="nav-link" href="#model-and-algorithm-specification"><span class="header-section-number">6.4.3</span> Model and Algorithm Specification</a></li>
<li><a class="nav-link" href="#model-training"><span class="header-section-number">6.4.4</span> Model Training</a></li>
<li><a class="nav-link" href="#model-output-and-performance"><span class="header-section-number">6.4.5</span> Model Output and Performance</a></li>
<li><a class="nav-link" href="#documentation-1"><span class="header-section-number">6.4.6</span> Documentation</a></li>
<li><a class="nav-link" href="#testing-2"><span class="header-section-number">6.4.7</span> Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#standards-regression"><span class="header-section-number">6.5</span> Regression and Supervised Learning</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#input-data-structures-and-validation-1"><span class="header-section-number">6.5.1</span> Input data structures and validation</a></li>
<li><a class="nav-link" href="#pre-processing-and-variable-transformation"><span class="header-section-number">6.5.2</span> Pre-processing and Variable Transformation</a></li>
<li><a class="nav-link" href="#algorithms-1"><span class="header-section-number">6.5.3</span> Algorithms</a></li>
<li><a class="nav-link" href="#return-results"><span class="header-section-number">6.5.4</span> Return Results</a></li>
<li><a class="nav-link" href="#documentation-2"><span class="header-section-number">6.5.5</span> Documentation</a></li>
<li><a class="nav-link" href="#visualization"><span class="header-section-number">6.5.6</span> Visualization</a></li>
<li><a class="nav-link" href="#testing-3"><span class="header-section-number">6.5.7</span> Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#spatial-software"><span class="header-section-number">6.6</span> Spatial Software</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#spatial-domains"><span class="header-section-number">6.6.1</span> Spatial Domains</a></li>
<li><a class="nav-link" href="#input-data-structures-and-validation-2"><span class="header-section-number">6.6.2</span> Input data structures and validation</a></li>
<li><a class="nav-link" href="#algorithms-2"><span class="header-section-number">6.6.3</span> Algorithms</a></li>
<li><a class="nav-link" href="#return-results-2"><span class="header-section-number">6.6.4</span> Return Results</a></li>
<li><a class="nav-link" href="#visualization-1"><span class="header-section-number">6.6.5</span> Visualization</a></li>
<li><a class="nav-link" href="#testing-4"><span class="header-section-number">6.6.6</span> Testing</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#standards-time-series"><span class="header-section-number">6.7</span> Time Series Software</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#input-data-structures-and-validation-3"><span class="header-section-number">6.7.1</span> Input data structures and validation</a></li>
<li><a class="nav-link" href="#pre-processing-and-variable-transformation-1"><span class="header-section-number">6.7.2</span> Pre-processing and Variable Transformation</a></li>
<li><a class="nav-link" href="#analytic-algorithms-2"><span class="header-section-number">6.7.3</span> Analytic Algorithms</a></li>
<li><a class="nav-link" href="#return-results-3"><span class="header-section-number">6.7.4</span> Return Results</a></li>
<li><a class="nav-link" href="#visualization-2"><span class="header-section-number">6.7.5</span> Visualization</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#dimensionality-reduction-clustering-and-unsupervised-learning"><span class="header-section-number">6.8</span> Dimensionality Reduction, Clustering, and Unsupervised Learning</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#input-data-structures-and-validation-4"><span class="header-section-number">6.8.1</span> Input Data Structures and Validation</a></li>
<li><a class="nav-link" href="#pre-processing-and-variable-transformation-2"><span class="header-section-number">6.8.2</span> Pre-processing and Variable Transformation</a></li>
<li><a class="nav-link" href="#algorithms-3"><span class="header-section-number">6.8.3</span> Algorithms</a></li>
<li><a class="nav-link" href="#return-results-4"><span class="header-section-number">6.8.4</span> Return Results</a></li>
<li><a class="nav-link" href="#documentation-3"><span class="header-section-number">6.8.5</span> Documentation</a></li>
<li><a class="nav-link" href="#visualization-3"><span class="header-section-number">6.8.6</span> Visualization</a></li>
<li><a class="nav-link" href="#testing-5"><span class="header-section-number">6.8.7</span> Testing</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>rOpenSci Statistical Software Peer Review</strong>" was written by Mark Padgham and Noam Ross. It was last built on 2021-06-23.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
